{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8392\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function # Use a function definition from future version (say 3.x from 2.7 interpreter)\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import time \n",
    "\n",
    "import cntk as C\n",
    "import cntk.tests.test_utils\n",
    "from cntk.layers import *\n",
    "from cntk.layers.typing import *\n",
    "import pickle\n",
    "import random\n",
    "from cntk import sequence\n",
    "from cntk import load_model\n",
    "from cntk.device import try_set_default_device, gpu,cpu\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "cntk.tests.test_utils.set_device_from_pytest_env() # (only needed for our build system)\n",
    "C.cntk_py.set_fixed_random_seed(1) # fix a random seed for CNTK components\n",
    "try_set_default_device(gpu(0))\n",
    "\n",
    "\n",
    "vocab_size = 80000\n",
    "num_labels = 19#19\n",
    "title_size = 52000\n",
    "body_size  = 210000\n",
    "input_dim  = vocab_size\n",
    "label_dim  = num_labels\n",
    "emb_dim    = 300\n",
    "hidden_dim = 200\n",
    "\n",
    "max_length_title = 30\n",
    "max_length_body  = 100\n",
    "\n",
    "\n",
    "\n",
    "#suffix = \"180days_all_shuffled\"\n",
    "suffix = \"linkedin_only\"\n",
    "prefix = \"/home/t-haohu/IndustryClassifier/Data/\"\n",
    "\n",
    "\n",
    "data_train_sample = \"{}/middle/train_{}.txt\".format(prefix,suffix)\n",
    "#data_train_sample = \"{}/middle/train_{}_with_linkedin_all.txt\".format(prefix,suffix)\n",
    "data_test_sample  = \"{}/middle/test_{}.txt\".format(prefix,suffix)\n",
    "#data_test_sample_editor  = \"{}/middle/test_{}_editor.txt\".format(prefix,suffix)\n",
    "\n",
    "\n",
    "data_title_sample    = \"{}/ready/title_{}.wl\".format(prefix,suffix)\n",
    "data_body_sample     = \"{}/ready/body_{}.wl\".format(prefix,suffix)\n",
    "suffix = \"180days_all_shuffled\"\n",
    "data_industry_sample = \"{}/ready/industry_{}.wl\".format(prefix,suffix)\n",
    "filter_num=200 \n",
    "dropout_rate = 0.5\n",
    "emb_dim =300\n",
    "suffix = \"linkedin_only\"\n",
    "def load_data_dynamic(input_file,title_dict,industry_dict):\n",
    "    data = open(input_file, encoding = \"utf-8\").readlines()\n",
    "    \n",
    "    data_title =[ [] for x in range(len(data))]#np.zeros((len(data),max_length_title),dtype = np.float32)\n",
    "    data_label = np.zeros((len(data),1),dtype = np.float32)\n",
    "    \n",
    "    \n",
    "    for index,line in enumerate(data):\n",
    "        row = line.strip(\"\\n\").split(\"\\t\")       \n",
    "        title    =  row[0]\n",
    "        industry =  row[1]\n",
    "        \n",
    "        for jndex,token in enumerate(title.split(\" \")):\n",
    "            if jndex>=max_length_title:\n",
    "                break\n",
    "            data_title[index].append(title_dict.get(token,len(title_dict)-1))\n",
    "        while len(data_title[index])<5:\n",
    "            data_title[index].append(len(title_dict)-1)\n",
    "        data_label[index] = industry_dict.get(industry,len(industry_dict))\n",
    "    data_title = [ np.array(x) for x in data_title]\n",
    "    return data_title,data_label\n",
    "\n",
    "def load_data_static(input_file,title_dict,industry_dict):\n",
    "    data = open(input_file, encoding = \"utf-8\").readlines()\n",
    "    \n",
    "    data_title =np.zeros((len(data),max_length_title),dtype = np.float32)\n",
    "    data_label = np.zeros((len(data),1),dtype = np.float32)\n",
    "    \n",
    "    \n",
    "    for index,line in enumerate(data):\n",
    "        row = line.strip(\"\\n\").split(\"\\t\")       \n",
    "        title    =  row[0]\n",
    "        industry =  row[1]\n",
    "        \n",
    "        for jndex,token in enumerate(title.split(\" \")):\n",
    "            if jndex>=max_length_title:\n",
    "                break\n",
    "            data_title[index,jndex]=title_dict.get(token,len(title_dict)-1)    \n",
    "        data_label[index] = industry_dict.get(industry,len(industry_dict))\n",
    "    return data_title,data_label\n",
    "\n",
    "def load_embedding(title_file,embedding_model_file):\n",
    "    model = Word2Vec.load(embedding_model_file)\n",
    "    title_list = [x.strip(\"\\n\") for x in open(title_file,encoding = 'utf-8').readlines()]\n",
    "    embedding = np.zeros((len(title_list),emb_dim))\n",
    "    count = 0\n",
    "    for i,w in enumerate(title_list):\n",
    "        try:\n",
    "            vec = model.wv[w]\n",
    "        except:\n",
    "            vec=model.wv[\"UNK\"]\n",
    "            count+=1\n",
    "        embedding[i] =vec\n",
    "    print(count)\n",
    "    return embedding\n",
    "\n",
    "def create_model_cnn(embed = False):\n",
    "    #version 2 : 1 dense layer version3: sigmoid activation in dense\n",
    "    if embed:\n",
    "        h1= C.layers.Embedding(weights=embedding,name='embed_1')(input_xt_one_hot)#\n",
    "    else:\n",
    "        h1= C.layers.Embedding(emb_dim,name='embed_2')(input_xt_one_hot)#init=embedding,\n",
    "\n",
    "    \n",
    "\n",
    "    h2_1=C.layers.Convolution((2,emb_dim),num_filters=filter_num,reduction_rank=0,activation=C.relu)(h1)\n",
    "    h2_2=C.layers.Convolution((3,emb_dim),num_filters=filter_num,reduction_rank=0,activation=C.relu)(h1)\n",
    "    h2_3=C.layers.Convolution((4,emb_dim),num_filters=filter_num,reduction_rank=0,activation=C.relu)(h1)\n",
    "    \n",
    "    h3_1=C.layers.MaxPooling((max_length_title-1,1),name='pooling_1')(h2_1)\n",
    "    h3_2=C.layers.MaxPooling((max_length_title-2,1),name='pooling_2')(h2_2)\n",
    "    h3_3=C.layers.MaxPooling((max_length_title-3,1),name='pooling_3')(h2_3)\n",
    "    #h2=BiRecurrence(C.layers.LSTM(hidden_dim), C.layers.LSTM(hidden_dim))(h1)\n",
    "    h3=C.splice(h3_2,h3_1,h3_3,axis=0)\n",
    "    drop1 = C.layers.Dropout(dropout_rate)(h3)\n",
    "    h4=C.layers.Dense(num_labels,name='hidden')(drop1)\n",
    "\n",
    "    return h4\n",
    "\n",
    "def create_model_cnn_dynamic(embed = False):\n",
    "    #version 2 : 1 dense layer version3: sigmoid activation in dense\n",
    "    if embed:\n",
    "        h1= C.layers.Embedding(weights=embedding,name='embed_1')(input_xt_one_hot)#\n",
    "    else:\n",
    "        h1= C.layers.Embedding(emb_dim,name='embed_2')(input_xt_one_hot)#init=embedding,\n",
    "\n",
    "    h1 = C.squeeze(h1)\n",
    "    print(h1)\n",
    "    h2_1=C.layers.Convolution((3,emb_dim),num_filters=filter_num,reduction_rank=0,activation=C.relu,sequential=True)(h1)\n",
    "    h2_2=C.layers.Convolution((4,emb_dim),num_filters=filter_num,reduction_rank=0,activation=C.relu,sequential=True)(h1)\n",
    "    h2_3=C.layers.Convolution((5,emb_dim),num_filters=filter_num,reduction_rank=0,activation=C.relu,sequential=True)(h1)\n",
    "    seq_MaxPooling = C.layers.Fold(C.element_max)\n",
    "    h3_1=seq_MaxPooling(h2_1)\n",
    "    h3_2=seq_MaxPooling(h2_2)\n",
    "    h3_3=seq_MaxPooling(h2_3)\n",
    "    #h2=BiRecurrence(C.layers.LSTM(hidden_dim), C.layers.LSTM(hidden_dim))(h1)\n",
    "    h3=C.splice(h3_2,h3_1,h3_3,axis=0)\n",
    "    drop1 = C.layers.Dropout(dropout_rate)(h3)\n",
    "    h4=C.layers.Dense(num_labels,name='hidden')(drop1)\n",
    "\n",
    "    return h4\n",
    "\n",
    "def create_model_cnn_2fold():\n",
    "    #version 2 : 1 dense layer version3: sigmoid activation in dense\n",
    "    #\n",
    "    with C.layers.default_options(initial_state=0.1):\n",
    "\n",
    "\n",
    "        h1_1= C.layers.Embedding(weights=embedding,name='embed_1')(input_xt_one_hot)#\n",
    "        h1_2= C.layers.Embedding(300,name='embed_2')(input_xt_one_hot)#init=embedding,\n",
    "        \n",
    "        \n",
    "        \n",
    "        h1_1_expand = C.expand_dims(h1_1,-3)\n",
    "        h1_2_expand = C.expand_dims(h1_2,-3)\n",
    "        \n",
    "        h1 = C.splice(h1_1_expand,h1_2_expand,axis = -3)\n",
    "        \n",
    "        #bn = C.layers.BatchNormalization(name='bn')(h1)\n",
    "        \n",
    "\n",
    "        #value,valid = to_static(h1)\n",
    "\n",
    "        filter_num=100\n",
    "\n",
    "        h2_1=C.layers.Convolution((3,emb_dim),num_filters=filter_num,reduction_rank=1,activation=C.relu)(h1)\n",
    "        h2_2=C.layers.Convolution((4,emb_dim),num_filters=filter_num,reduction_rank=1,activation=C.relu)(h1)\n",
    "        h2_3=C.layers.Convolution((5,emb_dim),num_filters=filter_num,reduction_rank=1,activation=C.relu)(h1)\n",
    "\n",
    "        h3_1=C.layers.MaxPooling((max_length_title-2,1),name='pooling_1')(h2_1)\n",
    "        h3_2=C.layers.MaxPooling((max_length_title-3,1),name='pooling_2')(h2_2)\n",
    "        h3_3=C.layers.MaxPooling((max_length_title-4,1),name='pooling_3')(h2_3)\n",
    "\n",
    "        h3=C.splice(h3_2,h3_1,h3_3,axis=0)\n",
    "        drop1 =C.layers.Dropout(0.5)(h3)\n",
    "        h4=C.layers.Dense(num_labels,name='hidden')(drop1)\n",
    "\n",
    "    return h4\n",
    "\n",
    "def batch_iter(data,batch_size, num_epochs, shuffle=True):\n",
    "    # Generates a batch iterator for a dataset.\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((data_size-1)/batch_size) + 1\n",
    "    print('data_size: ', data_size, 'batch_size: ', batch_size, 'num_batches_per_epoch: ', num_batches_per_epoch)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            random.shuffle(data)\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield data[start_index:end_index]\n",
    "            \n",
    "\n",
    "def fast_hist(a, b, n):\n",
    "    k = (a >= 0) & (a < n)\n",
    "    return np.bincount(n * a[k].astype(int) + b[k], minlength=n**2).reshape(n, n)\n",
    "\n",
    "title_dict =     { x:i for i,x in enumerate([x.strip(\"\\n\") for x in open(data_title_sample).readlines()])}\n",
    "industry_dict =  { x:i for i,x in enumerate([x.strip(\"\\n\") for x in open(data_industry_sample).readlines()])}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#input_xt = C.input_variable(shape=(max_length_title),  dtype=np.float32)\n",
    "input_xt = C.input_variable(**Sequence[Tensor[1]])\n",
    "input_y  = C.input_variable(shape=(1))\n",
    "\n",
    "input_xt_one_hot = C.one_hot(input_xt, num_classes=len(title_dict)   ,  sparse_output=True)\n",
    "input_y_one_hot = C.one_hot(input_y  , num_classes=len(industry_dict) ,  sparse_output=True)\n",
    "\n",
    "\n",
    "test_data  = load_data_dynamic(data_test_sample,title_dict,industry_dict)\n",
    "train_data = load_data_dynamic(data_train_sample,title_dict,industry_dict)\n",
    "#test_data_editor  = load_data(data_test_sample_editor,title_dict,industry_dict)\n",
    "embedding = load_embedding(data_title_sample,\"word2vec.model\")\n",
    "\n",
    "\n",
    "def test(batch_size,model,data):\n",
    "    scores = model(input_xt)\n",
    "    predict = C.argmax(scores,axis = 0)\n",
    "    confuse = np.zeros((num_labels,num_labels))\n",
    "\n",
    "    test_data_title,test_data_label = data\n",
    "    batches = batch_iter(list(zip(test_data_title,test_data_label)), batch_size, 1)\n",
    "    \n",
    "    for batch in batches:\n",
    "        batch_data_title,batch_data_label = zip(*batch) \n",
    "        batch_data_title = [[[x] for x in y] for y in list(batch_data_title)] \n",
    "        batch_data_label = [x.tolist() for x in batch_data_label]\n",
    "        output = np.array(predict.eval({input_xt: batch_data_title}),dtype=np.int)\n",
    "        gt = np.array(batch_data_label,dtype=np.int)\n",
    "        confuse+=fast_hist(output,gt,num_labels)\n",
    "        \n",
    "    precision=np.diag(confuse)/np.sum(confuse,axis=0)\n",
    "    recall = np.diag(confuse)/np.sum(confuse,axis=1)\n",
    "    accuarcy = np.diag(confuse).sum() / confuse.sum()\n",
    "    aver_precision=np.nanmean(precision)\n",
    "    aver_recall = np.nanmean(recall)\n",
    "   \n",
    "    print(\"Precision:{} Recall:{} Acc:{}\".format(aver_precision,aver_recall,accuarcy))\n",
    "    return accuarcy\n",
    "\n",
    "\n",
    "\n",
    "def train(train_data,num_epochs,learning_rate,batch_size,tag=\"CNN\",l2_weight=0):\n",
    "    global model\n",
    "    #learning_rate *= batch_size\n",
    "    model = create_model_cnn_dynamic()\n",
    "    print(C.logging.get_node_outputs(model))\n",
    "    scores = model(input_xt)\n",
    "    loss =C.reduce_mean(C.losses.cross_entropy_with_softmax(scores, input_y_one_hot))\n",
    "    \n",
    "    # Training\n",
    "    lr_schedule = C.learning_parameter_schedule(learning_rate)\n",
    "    #learner = C.adam(scores.parameters, lr=lr_schedule, momentum=0.9,l2_regularization_weight=0)\n",
    "    progress_printer = C.logging.ProgressPrinter(tag='Training', num_epochs=num_epochs)\n",
    "    momentums = C.momentum_schedule(0.99, minibatch_size=batch_size)\n",
    "    learner = C.adam(parameters=scores.parameters,#model.parameters,\n",
    "                     lr=lr_schedule,\n",
    "                     momentum=momentums,\n",
    "                     gradient_clipping_threshold_per_sample=15,\n",
    "                     gradient_clipping_with_truncation=True,\n",
    "                     l2_regularization_weight=l2_weight)\n",
    "    trainer = C.Trainer(scores, (loss), [learner], progress_printer)\n",
    "    \n",
    "    train_data_title,train_data_label = train_data\n",
    "    batches = batch_iter(list(zip(train_data_title,train_data_label)), batch_size, num_epochs)\n",
    "\n",
    "    # training loop\n",
    "    count = 0\n",
    "    t = time.time()\n",
    "    for batch in batches:\n",
    "        count += 1\n",
    "        batch_data_title,batch_data_label = zip(*batch)\n",
    "        batch_data_title = [[[x] for x in y] for y in list(batch_data_title)]\n",
    "        batch_data_label = [x.tolist() for x in batch_data_label]\n",
    "        #print(batch_data_label[0])\n",
    "        trainer.train_minibatch({input_xt: batch_data_title, input_y: batch_data_label})\n",
    "        if count%400== 0:\n",
    "            print(count,time.time()-t)\n",
    "            t=time.time()\n",
    "            acc1=test(batch_size,model,test_data)\n",
    "            #acc2=test(batch_size,model,test_data_editor)\n",
    "\n",
    "            model.save('./model/{}/{}_acc{:.3f}.dnn'.format(suffix,tag,acc1))\n",
    "            #model.save('./model/{}/{}_acc1{:.3f}_acc2{:.3f}.dnn'.format(suffix,tag,acc1,acc2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composite(Sequence[Tensor[1]]) -> Sequence[Tensor[300]]\n",
      "[Output('hidden', [#], [19]), Output('Block659405_Output_0', [#], [600 x 1]), Output('Splice659385_Output_0', [#], [600 x 1]), Output('Block659135_Output_0', [#], [200 x 1]), Output('Block659108_Output_0', [#, defaultDynamicAxis_times_1_minus_3], [200 x 1]), Output('PastValue659092_Output_0', [#, defaultDynamicAxis_times_1_minus_3], [200 x 1]), Output('Block658376_Output_0', [#, defaultDynamicAxis_times_1_minus_3], [200 x 1]), Output('Squeeze657835_Output_0', [#, *], [300]), Output('embed_2', [#, *], [1 x 300]), Output('OneHotOp657801_Output_0', [#, *], [1 x 82896]), Output('Block658991_Output_0', [#], [200 x 1]), Output('Block658964_Output_0', [#, defaultDynamicAxis_times_1_minus_2], [200 x 1]), Output('PastValue658948_Output_0', [#, defaultDynamicAxis_times_1_minus_2], [200 x 1]), Output('Block658052_Output_0', [#, defaultDynamicAxis_times_1_minus_2], [200 x 1]), Output('Block659281_Output_0', [#], [200 x 1]), Output('Block659254_Output_0', [#, defaultDynamicAxis_times_1_minus_4], [200 x 1]), Output('PastValue659238_Output_0', [#, defaultDynamicAxis_times_1_minus_4], [200 x 1]), Output('Block658711_Output_0', [#, defaultDynamicAxis_times_1_minus_4], [200 x 1])]\n",
      "data_size:  2076724 batch_size:  150 num_batches_per_epoch:  13845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t-haohu/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/core.py:411: UserWarning: you provided the minibatch data as a list, but your corresponding input variable (uid \"Input657800\") has only one dynamic axis (batch axis). To speed up graph execution, please convert the data beforehand into one NumPy array to speed up training.\n",
      "  'training.' % var.uid)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate per minibatch: 0.007500000000000001\n",
      "400 126.96919322013855\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t-haohu/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/ipykernel_launcher.py:252: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/t-haohu/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/ipykernel_launcher.py:253: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:0.058823529411764705 Recall:0.19374287239122495 Acc:0.19374287239122495\n",
      "800 366.05057764053345\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.09166930509316573 Recall:0.24550388284528243 Acc:0.273594561950507\n",
      "1200 329.0851881504059\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.13799558544303014 Recall:0.4033974996402781 Acc:0.3586191399118002\n",
      "1600 362.52061438560486\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.23759672671026233 Recall:0.5508571841705289 Acc:0.4796977612988399\n",
      "2000 333.03437781333923\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.30828731830635053 Recall:0.5921098622614606 Acc:0.5486250386225106\n",
      "2400 369.42725372314453\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.35032286381806793 Recall:0.615774037774515 Acc:0.5844408864919525\n",
      "2800 353.8938491344452\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.4104660151984387 Recall:0.6167840734010249 Acc:0.6292969298615207\n",
      "3200 366.63681983947754\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.4413774245241633 Recall:0.654686242734479 Acc:0.6510367686300946\n",
      "3600 328.0143473148346\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.4712232044841289 Recall:0.6680525666796105 Acc:0.668319429229516\n",
      "4000 370.5212550163269\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.4956779364175933 Recall:0.6886253336794058 Acc:0.6824044268419427\n",
      "4400 339.26078367233276\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.5169238940200633 Recall:0.7059582546844383 Acc:0.6929501980281453\n",
      "4800 354.79355788230896\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n"
     ]
    }
   ],
   "source": [
    "train(train_data,num_epochs=20,learning_rate=[5e-4*150]*2+[1e-4*150],batch_size = 150,tag = \"linkedin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cntk import load_model\n",
    "from data_processor import *\n",
    "process_setting(low=False,old = True,stop = False)\n",
    "def inference(model,val_doc_file,output_file,title_dict,industry_file):\n",
    "    \n",
    "    scores = model(input_xt)\n",
    "    predict = C.argmax(scores,axis = 0)\n",
    "    probability = C.reduce_max(C.softmax(scores),axis = 0)\n",
    "    \n",
    "    industry = [x.strip(\"\\n\") for x in open(industry_file,encoding =\"utf-8\").readlines()]\n",
    "    val_doc = open(val_doc_file,encoding = \"utf-8\")\n",
    "    output = open(output_file,\"w\",encoding = \"utf-8\")\n",
    "    val_doc = [tokenize(x.strip(\"\\n\")) for x in val_doc.readlines()]\n",
    "    \n",
    "    data_title = np.zeros((len(val_doc),max_length_title),dtype = np.float32)\n",
    "    \n",
    "    for index,title in enumerate(val_doc):       \n",
    "        for jndex,token in enumerate(title.split(\" \")):\n",
    "            if jndex>=max_length_title:\n",
    "                break\n",
    "            data_title[index,jndex]=title_dict.get(token,len(title_dict)-1)\n",
    "            \n",
    "    batches = batch_iter(data_title, batch_size, 1)\n",
    "    for batch in batches:\n",
    "        batch_data_title = batch\n",
    "        pred = np.array(predict.eval({input_xt: np.array(batch_data_title)}),dtype=np.int)\n",
    "        prob = np.array(probability.eval({input_xt: np.array(batch_data_title)}),dtype=np.float32)\n",
    "        #gt = np.array(batch_data_label,dtype=np.int)\n",
    "        #confuse+=fast_hist(output,gt,num_labels)\n",
    "        for pre,pro in list(zip(pred,prob)):\n",
    "            output.write(\"\\t\".joint([str(industry[int(pre)]),str(pro)])+\"\\n\")\n",
    "    output.close()\n",
    "suffix_list =[\"180days_all_shuffled\",\"180days_editor_shuffled\"]\n",
    "model_list = [\"180days_all_shuffled\"]\n",
    "industry_list = [\"\"]\n",
    "prefix = \"/home/t-haohu/IndustryClassifier/Data/\"\n",
    "for suffix,model_name,indsutry in list(zip(suffix_list,model_list,industry_list)):\n",
    "    #model_file = \"{}/{}\".format(prefix,industry)\n",
    "    model = load_model(model_name)\n",
    "    data_industry_sample = \"{}/ready/industry_{}.wl\".format(prefix,industry)\n",
    "    data_title_sample    = \"{}/ready/title_{}.wl\".format(prefix,suffix)\n",
    "    data_body_sample     = \"{}/ready/body_{}.wl\".format(prefix,suffix)\n",
    "    title_dict =     { x:i for i,x in enumerate([x.strip(\"\\n\") for x in open(data_title_sample).readlines()])}\n",
    "    inference(model,\"middle/1day_measure_sample_valid.txt\",\"val/cnn_1day_measure_{}.txt\".format(suffix),title_dict,data_industry_sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([1]), list([1, 2])], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
