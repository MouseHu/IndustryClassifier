{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13201\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function # Use a function definition from future version (say 3.x from 2.7 interpreter)\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import time \n",
    "\n",
    "import cntk as C\n",
    "import cntk.tests.test_utils\n",
    "from cntk.layers import *\n",
    "from cntk.layers.typing import *\n",
    "import pickle\n",
    "import random\n",
    "from cntk import sequence\n",
    "from cntk import load_model\n",
    "from cntk.device import try_set_default_device, gpu,cpu\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "cntk.tests.test_utils.set_device_from_pytest_env() # (only needed for our build system)\n",
    "C.cntk_py.set_fixed_random_seed(1) # fix a random seed for CNTK components\n",
    "try_set_default_device(gpu(0))\n",
    "\n",
    "\n",
    "vocab_size = 80000\n",
    "num_labels = 19#19\n",
    "title_size = 52000\n",
    "body_size  = 210000\n",
    "input_dim  = vocab_size\n",
    "label_dim  = num_labels\n",
    "emb_dim    = 300\n",
    "hidden_dim = 200\n",
    "\n",
    "max_length_title = 30\n",
    "max_length_body  = 100\n",
    "\n",
    "\n",
    "\n",
    "suffix = \"180days_all_shuffled_with_linkedin_all\"\n",
    "#suffix = \"linkedin_only\"\n",
    "prefix = \"/home/t-haohu/IndustryClassifier/Data/\"\n",
    "\n",
    "\n",
    "data_train_sample = \"{}/middle/train_{}.txt\".format(prefix,suffix)\n",
    "#data_train_sample = \"{}/middle/train_{}_with_linkedin_all.txt\".format(prefix,suffix)\n",
    "suffix = \"180days_all_shuffled\"\n",
    "data_test_sample  = \"{}/middle/test_{}.txt\".format(prefix,suffix)\n",
    "#data_test_sample_editor  = \"{}/middle/test_{}_editor.txt\".format(prefix,suffix)\n",
    "\n",
    "suffix = \"180days_all_shuffled_with_linkedin_all_all\"\n",
    "data_title_sample    = \"{}/ready/title_{}.wl\".format(prefix,suffix)\n",
    "data_body_sample     = \"{}/ready/body_{}.wl\".format(prefix,suffix)\n",
    "suffix = \"180days_all_shuffled\"\n",
    "data_industry_sample = \"{}/ready/industry_{}.wl\".format(prefix,suffix)\n",
    "suffix = \"180days_all_shuffled_with_linkedin_all\"\n",
    "filter_num=200 \n",
    "dropout_rate = 0.5\n",
    "emb_dim =300\n",
    "suffix = \"180days_all_shuffled\"\n",
    "def load_data_dynamic(input_file,title_dict,industry_dict):\n",
    "    data = open(input_file, encoding = \"utf-8\").readlines()\n",
    "    \n",
    "    data_title =[ [] for x in range(len(data))]#np.zeros((len(data),max_length_title),dtype = np.float32)\n",
    "    data_label = np.zeros((len(data),1),dtype = np.float32)\n",
    "    \n",
    "    \n",
    "    for index,line in enumerate(data):\n",
    "        row = line.strip(\"\\n\").split(\"\\t\")       \n",
    "        title    =  row[0]\n",
    "        industry =  row[1]\n",
    "        \n",
    "        for jndex,token in enumerate(title.split(\" \")):\n",
    "            if jndex>=max_length_title:\n",
    "                break\n",
    "            data_title[index].append(title_dict.get(token,len(title_dict)-1))\n",
    "        while len(data_title[index])<5:\n",
    "            data_title[index].append(len(title_dict)-1)\n",
    "        data_label[index] = industry_dict.get(industry,len(industry_dict))\n",
    "    data_title = [ np.array(x) for x in data_title]\n",
    "    return data_title,data_label\n",
    "\n",
    "def load_data_static(input_file,title_dict,industry_dict):\n",
    "    data = open(input_file, encoding = \"utf-8\").readlines()\n",
    "    \n",
    "    data_title =np.zeros((len(data),max_length_title),dtype = np.float32)\n",
    "    data_label = np.zeros((len(data),1),dtype = np.float32)\n",
    "    \n",
    "    \n",
    "    for index,line in enumerate(data):\n",
    "        row = line.strip(\"\\n\").split(\"\\t\")       \n",
    "        title    =  row[0]\n",
    "        industry =  row[1]\n",
    "        \n",
    "        for jndex,token in enumerate(title.split(\" \")):\n",
    "            if jndex>=max_length_title:\n",
    "                break\n",
    "            data_title[index,jndex]=title_dict.get(token,len(title_dict)-1)    \n",
    "        data_label[index] = industry_dict.get(industry,len(industry_dict))\n",
    "    return data_title,data_label\n",
    "\n",
    "def load_embedding(title_file,embedding_model_file):\n",
    "    model = Word2Vec.load(embedding_model_file)\n",
    "    title_list = [x.strip(\"\\n\") for x in open(title_file,encoding = 'utf-8').readlines()]\n",
    "    embedding = np.zeros((len(title_list),emb_dim))\n",
    "    count = 0\n",
    "    for i,w in enumerate(title_list):\n",
    "        try:\n",
    "            vec = model.wv[w]\n",
    "        except:\n",
    "            vec=model.wv[\"UNK\"]\n",
    "            count+=1\n",
    "        embedding[i] =vec\n",
    "    print(count)\n",
    "    return embedding\n",
    "\n",
    "def create_model_cnn(embed = False):\n",
    "    #version 2 : 1 dense layer version3: sigmoid activation in dense\n",
    "    if embed:\n",
    "        h1= C.layers.Embedding(weights=embedding,name='embed_1')(input_xt_one_hot)#\n",
    "    else:\n",
    "        h1= C.layers.Embedding(emb_dim,name='embed_2')(input_xt_one_hot)#init=embedding,\n",
    "\n",
    "    \n",
    "\n",
    "    h2_1=C.layers.Convolution((1,emb_dim),num_filters=filter_num,reduction_rank=0,activation=C.relu)(h1)\n",
    "    h2_2=C.layers.Convolution((2,emb_dim),num_filters=filter_num,reduction_rank=0,activation=C.relu)(h1)\n",
    "    h2_3=C.layers.Convolution((3,emb_dim),num_filters=filter_num,reduction_rank=0,activation=C.relu)(h1)\n",
    "    \n",
    "    h3_1=C.layers.MaxPooling((max_length_title-2,1),name='pooling_1')(h2_1)\n",
    "    h3_2=C.layers.MaxPooling((max_length_title-3,1),name='pooling_2')(h2_2)\n",
    "    h3_3=C.layers.MaxPooling((max_length_title-4,1),name='pooling_3')(h2_3)\n",
    "    #h2=BiRecurrence(C.layers.LSTM(hidden_dim), C.layers.LSTM(hidden_dim))(h1)\n",
    "    h3=C.splice(h3_2,h3_1,h3_3,axis=0)\n",
    "    drop1 = C.layers.Dropout(dropout_rate)(h3)\n",
    "    h4=C.layers.Dense(num_labels,name='hidden')(drop1)\n",
    "\n",
    "    return h4\n",
    "\n",
    "def create_model_cnn_dynamic(embed = False):\n",
    "    #version 2 : 1 dense layer version3: sigmoid activation in dense\n",
    "    if embed:\n",
    "        h1= C.layers.Embedding(weights=embedding,name='embed_1')(input_xt_one_hot)#\n",
    "    else:\n",
    "        h1= C.layers.Embedding(emb_dim,name='embed_2')(input_xt_one_hot)#init=embedding,\n",
    "\n",
    "    h1 = C.squeeze(h1)\n",
    "    print(h1)\n",
    "    h2_1=C.layers.Convolution((3,emb_dim),num_filters=filter_num,reduction_rank=0,activation=C.relu,sequential=True)(h1)\n",
    "    h2_2=C.layers.Convolution((4,emb_dim),num_filters=filter_num,reduction_rank=0,activation=C.relu,sequential=True)(h1)\n",
    "    h2_3=C.layers.Convolution((5,emb_dim),num_filters=filter_num,reduction_rank=0,activation=C.relu,sequential=True)(h1)\n",
    "    seq_MaxPooling = C.layers.Fold(C.element_max)\n",
    "    h3_1=seq_MaxPooling(h2_1)\n",
    "    h3_2=seq_MaxPooling(h2_2)\n",
    "    h3_3=seq_MaxPooling(h2_3)\n",
    "    #h2=BiRecurrence(C.layers.LSTM(hidden_dim), C.layers.LSTM(hidden_dim))(h1)\n",
    "    h3=C.splice(h3_2,h3_1,h3_3,axis=0)\n",
    "    drop1 = C.layers.Dropout(dropout_rate)(h3)\n",
    "    h4=C.layers.Dense(num_labels,name='hidden')(drop1)\n",
    "\n",
    "    return h4\n",
    "\n",
    "def create_model_cnn_2fold(dynamic = False):\n",
    "    #version 2 : 1 dense layer version3: sigmoid activation in dense\n",
    "    #\n",
    "    with C.layers.default_options(initial_state=0.1):\n",
    "\n",
    "\n",
    "        h1_1= C.layers.Embedding(weights=embedding,name='embed_1')(input_xt_one_hot)#\n",
    "        h1_2= C.layers.Embedding(300,name='embed_2')(input_xt_one_hot)#init=embedding,\n",
    "        \n",
    "        \n",
    "        \n",
    "        h1_1_expand = C.expand_dims(h1_1,-3)\n",
    "        h1_2_expand = C.expand_dims(h1_2,-3)\n",
    "        \n",
    "        h1 = C.splice(h1_1_expand,h1_2_expand,axis = -3)\n",
    "        \n",
    "        #bn = C.layers.BatchNormalization(name='bn')(h1)\n",
    "        \n",
    "\n",
    "        #value,valid = to_static(h1)\n",
    "\n",
    "        filter_num=100\n",
    "\n",
    "        h2_1=C.layers.Convolution((3,emb_dim),num_filters=filter_num,reduction_rank=1,activation=C.relu)(h1)\n",
    "        h2_2=C.layers.Convolution((4,emb_dim),num_filters=filter_num,reduction_rank=1,activation=C.relu)(h1)\n",
    "        h2_3=C.layers.Convolution((5,emb_dim),num_filters=filter_num,reduction_rank=1,activation=C.relu)(h1)\n",
    "        if dynamic:\n",
    "            seq_MaxPooling = C.layers.Fold(C.element_max)\n",
    "            h3_1=seq_MaxPooling(h2_1)\n",
    "            h3_2=seq_MaxPooling(h2_2)\n",
    "            h3_3=seq_MaxPooling(h2_3)\n",
    "        else:\n",
    "            h3_1=C.layers.MaxPooling((max_length_title-2,1),name='pooling_1')(h2_1)\n",
    "            h3_2=C.layers.MaxPooling((max_length_title-3,1),name='pooling_2')(h2_2)\n",
    "            h3_3=C.layers.MaxPooling((max_length_title-4,1),name='pooling_3')(h2_3)\n",
    "        \n",
    "        h3=C.splice(h3_2,h3_1,h3_3,axis=0)\n",
    "        drop1 =C.layers.Dropout(0.5)(h3)\n",
    "        h4=C.layers.Dense(num_labels,name='hidden')(drop1)\n",
    "\n",
    "    return h4\n",
    "\n",
    "def batch_iter(data,batch_size, num_epochs, shuffle=True):\n",
    "    # Generates a batch iterator for a dataset.\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((data_size-1)/batch_size) + 1\n",
    "    print('data_size: ', data_size, 'batch_size: ', batch_size, 'num_batches_per_epoch: ', num_batches_per_epoch)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            random.shuffle(data)\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield data[start_index:end_index]\n",
    "            \n",
    "\n",
    "def fast_hist(a, b, n):\n",
    "    k = (a >= 0) & (a < n)\n",
    "    return np.bincount(n * a[k].astype(int) + b[k], minlength=n**2).reshape(n, n)\n",
    "\n",
    "title_dict =     { x:i for i,x in enumerate([x.strip(\"\\n\") for x in open(data_title_sample).readlines()])}\n",
    "industry_dict =  { x:i for i,x in enumerate([x.strip(\"\\n\") for x in open(data_industry_sample).readlines()])}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#input_xt = C.input_variable(shape=(max_length_title),  dtype=np.float32)\n",
    "input_xt = C.input_variable(**Sequence[Tensor[1]])\n",
    "input_y  = C.input_variable(shape=(1))\n",
    "\n",
    "input_xt_one_hot = C.one_hot(input_xt, num_classes=len(title_dict)   ,  sparse_output=True)\n",
    "input_y_one_hot = C.one_hot(input_y  , num_classes=len(industry_dict) ,  sparse_output=True)\n",
    "\n",
    "\n",
    "test_data  = load_data_dynamic(data_test_sample,title_dict,industry_dict)\n",
    "train_data = load_data_dynamic(data_train_sample,title_dict,industry_dict)\n",
    "#test_data_editor  = load_data(data_test_sample_editor,title_dict,industry_dict)\n",
    "embedding = load_embedding(data_title_sample,\"word2vec.model\")\n",
    "\n",
    "\n",
    "def test(batch_size,model,data):\n",
    "    scores = model(input_xt)\n",
    "    predict = C.argmax(scores,axis = 0)\n",
    "    confuse = np.zeros((num_labels,num_labels))\n",
    "\n",
    "    test_data_title,test_data_label = data\n",
    "    batches = batch_iter(list(zip(test_data_title,test_data_label)), batch_size, 1)\n",
    "    \n",
    "    for batch in batches:\n",
    "        batch_data_title,batch_data_label = zip(*batch) \n",
    "        batch_data_title = [[[x] for x in y] for y in list(batch_data_title)] \n",
    "        batch_data_label = [x.tolist() for x in batch_data_label]\n",
    "        output = np.array(predict.eval({input_xt: batch_data_title}),dtype=np.int)\n",
    "        gt = np.array(batch_data_label,dtype=np.int)\n",
    "        confuse+=fast_hist(output,gt,num_labels)\n",
    "        \n",
    "    precision=np.diag(confuse)/np.sum(confuse,axis=0)\n",
    "    recall = np.diag(confuse)/np.sum(confuse,axis=1)\n",
    "    accuarcy = np.diag(confuse).sum() / confuse.sum()\n",
    "    aver_precision=np.nanmean(precision)\n",
    "    aver_recall = np.nanmean(recall)\n",
    "   \n",
    "    print(\"Precision:{} Recall:{} Acc:{}\".format(aver_precision,aver_recall,accuarcy))\n",
    "    return accuarcy\n",
    "\n",
    "\n",
    "\n",
    "def train(train_data,num_epochs,learning_rate,batch_size,tag=\"CNN\",l2_weight=0):\n",
    "    global model\n",
    "    #learning_rate *= batch_size\n",
    "    model = create_model_cnn_dynamic()\n",
    "    print(C.logging.get_node_outputs(model))\n",
    "    scores = model(input_xt)\n",
    "    loss =C.reduce_mean(C.losses.cross_entropy_with_softmax(scores, input_y_one_hot))\n",
    "    \n",
    "    # Training\n",
    "    lr_schedule = C.learning_parameter_schedule(learning_rate)\n",
    "    #learner = C.adam(scores.parameters, lr=lr_schedule, momentum=0.9,l2_regularization_weight=0)\n",
    "    progress_printer = C.logging.ProgressPrinter(tag='Training', num_epochs=num_epochs)\n",
    "    momentums = C.momentum_schedule(0.99, minibatch_size=batch_size)\n",
    "    learner = C.adam(parameters=scores.parameters,#model.parameters,\n",
    "                     lr=lr_schedule,\n",
    "                     momentum=momentums,\n",
    "                     gradient_clipping_threshold_per_sample=15,\n",
    "                     gradient_clipping_with_truncation=True,\n",
    "                     l2_regularization_weight=l2_weight)\n",
    "    trainer = C.Trainer(scores, (loss), [learner], progress_printer)\n",
    "    \n",
    "    train_data_title,train_data_label = train_data\n",
    "    batches = batch_iter(list(zip(train_data_title,train_data_label)), batch_size, num_epochs)\n",
    "\n",
    "    # training loop\n",
    "    count = 0\n",
    "    t = time.time()\n",
    "    for batch in batches:\n",
    "        count += 1\n",
    "        batch_data_title,batch_data_label = zip(*batch)\n",
    "        batch_data_title = [[[x] for x in y] for y in list(batch_data_title)]\n",
    "        batch_data_label = [x.tolist() for x in batch_data_label]\n",
    "        #print(batch_data_label[0])\n",
    "        trainer.train_minibatch({input_xt: batch_data_title, input_y: batch_data_label})\n",
    "        if count%400== 0:\n",
    "            print(count,time.time()-t)\n",
    "            t=time.time()\n",
    "            acc1=test(batch_size,model,test_data)\n",
    "            #acc2=test(batch_size,model,test_data_editor)\n",
    "\n",
    "            model.save('./model/{}/{}_acc{:.3f}.dnn'.format(suffix,tag,acc1))\n",
    "            #model.save('./model/{}/{}_acc1{:.3f}_acc2{:.3f}.dnn'.format(suffix,tag,acc1,acc2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composite(Sequence[Tensor[1]]) -> Sequence[Tensor[300]]\n",
      "[Output('hidden', [#], [19]), Output('Block1609_Output_0', [#], [600 x 1]), Output('Splice1589_Output_0', [#], [600 x 1]), Output('Block1339_Output_0', [#], [200 x 1]), Output('Block1312_Output_0', [#, defaultDynamicAxis_times_1_minus_3], [200 x 1]), Output('PastValue1296_Output_0', [#, defaultDynamicAxis_times_1_minus_3], [200 x 1]), Output('Block580_Output_0', [#, defaultDynamicAxis_times_1_minus_3], [200 x 1]), Output('Squeeze39_Output_0', [#, *], [300]), Output('embed_2', [#, *], [1 x 300]), Output('OneHotOp5_Output_0', [#, *], [1 x 111056]), Output('Block1195_Output_0', [#], [200 x 1]), Output('Block1168_Output_0', [#, defaultDynamicAxis_times_1_minus_2], [200 x 1]), Output('PastValue1152_Output_0', [#, defaultDynamicAxis_times_1_minus_2], [200 x 1]), Output('Block256_Output_0', [#, defaultDynamicAxis_times_1_minus_2], [200 x 1]), Output('Block1485_Output_0', [#], [200 x 1]), Output('Block1458_Output_0', [#, defaultDynamicAxis_times_1_minus_4], [200 x 1]), Output('PastValue1442_Output_0', [#, defaultDynamicAxis_times_1_minus_4], [200 x 1]), Output('Block915_Output_0', [#, defaultDynamicAxis_times_1_minus_4], [200 x 1])]\n",
      "data_size:  3735254 batch_size:  150 num_batches_per_epoch:  24902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t-haohu/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/core.py:411: UserWarning: you provided the minibatch data as a list, but your corresponding input variable (uid \"Input4\") has only one dynamic axis (batch axis). To speed up graph execution, please convert the data beforehand into one NumPy array to speed up training.\n",
      "  'training.' % var.uid)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate per minibatch: 0.075\n",
      "400 267.9954340457916\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t-haohu/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/ipykernel_launcher.py:260: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:0.24669390358121232 Recall:0.4582156354272343 Acc:0.42406788923973765\n",
      "800 301.3657751083374\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.34870842862307283 Recall:0.5314587938446768 Acc:0.5248967694923488\n",
      "1200 301.0366325378418\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.41691832934051193 Recall:0.5972287741251298 Acc:0.5798882681564246\n",
      "1600 346.425817489624\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.4646358434756698 Recall:0.5951153872954037 Acc:0.613010080155453\n",
      "2000 307.9948151111603\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.4703778675719885 Recall:0.6195879512254169 Acc:0.6110153024046636\n",
      "2400 332.33660674095154\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.49601339631164487 Recall:0.6200687307658416 Acc:0.6330459072139908\n",
      "2800 349.01365852355957\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.5120263306721169 Recall:0.6269671902134076 Acc:0.6373360456643187\n",
      "3200 268.40716075897217\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.5083761747421631 Recall:0.6302198219007411 Acc:0.6362976682050037\n",
      "3600 323.7819182872772\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.523709018423254 Recall:0.6333867503830155 Acc:0.6485547728928832\n",
      "4000 347.4964301586151\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.5299137261185043 Recall:0.6428292714759168 Acc:0.6494140150595094\n",
      "4400 303.3579351902008\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.5361117214252413 Recall:0.6450888031571701 Acc:0.6571320136021375\n",
      "4800 306.23607540130615\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.5396465355523515 Recall:0.6515740925443255 Acc:0.6622267427738645\n",
      "5200 339.93869638442993\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.5451545294291938 Recall:0.6545636699158996 Acc:0.6587594121933447\n",
      "5600 348.7412269115448\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.5437088757568795 Recall:0.6547491584961638 Acc:0.6619565217391304\n",
      "6000 304.91297125816345\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.5540950651131256 Recall:0.6527462831716211 Acc:0.6706491377216419\n",
      "6400 339.83070158958435\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.5498698280913136 Recall:0.6580775325420124 Acc:0.6654359970852562\n",
      "6800 338.6050102710724\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.5497317501542064 Recall:0.6611150912947857 Acc:0.662229778965266\n",
      "7200 279.85389852523804\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.5527411007540236 Recall:0.6604961842598186 Acc:0.667564367257712\n",
      "7600 348.9942831993103\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.5582525154223195 Recall:0.6831980431596228 Acc:0.6744049064853048\n",
      "8000 355.75129532814026\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.5543521746437371 Recall:0.6887348518625808 Acc:0.670618775807627\n",
      "8400 300.9318027496338\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.5578714185169921 Recall:0.6818946025298519 Acc:0.675227714355113\n",
      "8800 326.3102729320526\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.5638914498559023 Recall:0.6827938147849179 Acc:0.6714112217634199\n",
      "9200 310.823992729187\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.5661994477080176 Recall:0.6796875344030919 Acc:0.6781303133349527\n",
      "9600 284.1071789264679\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.5640701853097783 Recall:0.679156047310595 Acc:0.6744140150595094\n",
      "10000 329.1659948825836\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.5618101757316207 Recall:0.6809684231104468 Acc:0.6762114403692009\n",
      "10400 342.1528272628784\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.5683449379962108 Recall:0.6838776590563226 Acc:0.6794996356570319\n",
      "10800 266.7222490310669\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.5747813210426422 Recall:0.6801775766994993 Acc:0.68217755647316\n",
      "11200 325.8254623413086\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.568581091801735 Recall:0.6837897075415776 Acc:0.6773075054651445\n",
      "11600 343.66496658325195\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.5663245767531518 Recall:0.6793236449979049 Acc:0.6775807626912801\n",
      "12000 296.6656405925751\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.5628218844273442 Recall:0.6833982338150396 Acc:0.6722401020160311\n",
      "12400 302.842839717865\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.5804821154669297 Recall:0.6792566875062314 Acc:0.6863583920330337\n",
      "12800 343.22676634788513\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.579935653918126 Recall:0.679427738714195 Acc:0.6866103959193588\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/logging/progress_print.py\u001b[0m in \u001b[0;36mon_training_update_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m___write_progress_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregate_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregate_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mon_training_update_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0;31m# Override for ProgressWriter.on_training_update_end.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m___generate_progress_heartbeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "SWIG director method error.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b648726443b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5e-4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cnn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-8d93fb686277>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_data, num_epochs, learning_rate, batch_size, tag, l2_weight)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0mbatch_data_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_data_label\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;31m#print(batch_data_label[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minput_xt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_data_title\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_y\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_data_label\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/train/trainer.py\u001b[0m in \u001b[0;36mtrain_minibatch\u001b[0;34m(self, arguments, outputs, device, is_sweep_end)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 updated = super(Trainer, self).train_minibatch(arguments, is_sweep_end,\n\u001b[0;32m--> 184\u001b[0;31m                     device)\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/cntk_py.py\u001b[0m in \u001b[0;36mtrain_minibatch\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3027\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_cntk_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer_train_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3029\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: SWIG director method error."
     ]
    }
   ],
   "source": [
    "train(train_data,num_epochs=20,learning_rate=[5e-4*150]*2+[1e-4*150],batch_size = 150,tag = \"cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model/180days_all_shuffled/cnn_acc0.850.dnn\n",
      "data_size:  361 batch_size:  20 num_batches_per_epoch:  19\n",
      "model/180days_editor_shuffled/\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "basic_filebuf::underflow error reading the file: iostream error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-71d072742a80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mdata_industry_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{}/ready/industry_{}.wl\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindustry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mdata_title_sample\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0;34m\"{}/ready/title_{}.wl\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/internal/swig_helper.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mmap_if_possible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/ops/functions.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(model, device, format)\u001b[0m\n\u001b[1;32m   1692\u001b[0m     \u001b[0mAlias\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mcntk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m     '''\n\u001b[0;32m-> 1694\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mUserFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/internal/swig_helper.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mmap_if_possible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/ops/functions.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(model, device, format)\u001b[0m\n\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1608\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcntk_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot load the model {} that is neither a file nor a byte buffer.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: basic_filebuf::underflow error reading the file: iostream error"
     ]
    }
   ],
   "source": [
    "from cntk import load_model\n",
    "from data_processor import *\n",
    "process_setting(low=False,old = True,stop = False)\n",
    "input_xt = C.input_variable(**Sequence[Tensor[1]])\n",
    "batch_size = 20\n",
    "def inference(model,val_doc_file,output_file,title_dict,industry_file):\n",
    "    \n",
    "    scores = model(input_xt)\n",
    "    predict = C.argmax(scores,axis = 0)\n",
    "    probability = C.reduce_max(C.softmax(scores),axis = 0)\n",
    "    \n",
    "    industry = [x.strip(\"\\n\") for x in open(industry_file,encoding =\"utf-8\").readlines()]\n",
    "    val_doc = open(val_doc_file,encoding = \"utf-8\")\n",
    "    output = open(output_file,\"w\",encoding = \"utf-8\")\n",
    "    val_doc = [tokenize(x.strip(\"\\n\")) for x in val_doc.readlines()]\n",
    "    \n",
    "    data_title = [[] for x  in range(len(val_doc))]\n",
    "    \n",
    "    for index,title in enumerate(val_doc):       \n",
    "            \n",
    "        for jndex,token in enumerate(title.split(\" \")):\n",
    "            if jndex>=max_length_title:\n",
    "                break\n",
    "            data_title[index].append(title_dict.get(token,len(title_dict)-1))\n",
    "        while len(data_title[index])<5:\n",
    "            data_title[index].append(len(title_dict)-1)        \n",
    "    batches = batch_iter(data_title, batch_size, 1)\n",
    "    for batch in batches:\n",
    "        batch_data_title = batch\n",
    "        pred = np.array(predict.eval({input_xt: batch_data_title}),dtype=np.int)\n",
    "        prob = np.array(probability.eval({input_xt: batch_data_title}),dtype=np.float32)\n",
    "        #gt = np.array(batch_data_label,dtype=np.int)\n",
    "        #confuse+=fast_hist(output,gt,num_labels)\n",
    "        for pre,pro in list(zip(pred,prob)):\n",
    "            output.write(\"\\t\".join([str(industry[int(pre)]),str(pro)])+\"\\n\")\n",
    "    output.close()\n",
    "    \n",
    "model_list =[\"model/180days_all_shuffled/cnn_acc0.850.dnn\",\"model/180days_editor_shuffled/\",\"model/linkedin_only/linkedin_acc0.770.dnn\"]\n",
    "suffix_list = [\"180days_all_shuffled\",\"180days_editor_shuffled\",\"linkedin_only\"]\n",
    "industry_list = [\"180days_all_shuffled\",\"180days_all_shuffled\",\"180days_all_shuffled\"]\n",
    "model = 0\n",
    "prefix = \"/home/t-haohu/IndustryClassifier/Data/\"\n",
    "for suffix,model_name,industry in list(zip(suffix_list,model_list,industry_list)):\n",
    "    #model_file = \"{}/{}\".format(prefix,industry)\n",
    "\n",
    "    if model\"model/180days_editor_shuffled/\",\n",
    "    model = load_model(model_name)\n",
    "    data_industry_sample = \"{}/ready/industry_{}.wl\".format(prefix,industry)\n",
    "    data_title_sample    = \"{}/ready/title_{}.wl\".format(prefix,suffix)\n",
    "    data_body_sample     = \"{}/ready/body_{}.wl\".format(prefix,suffix)\n",
    "    title_dict =     { x:i for i,x in enumerate([x.strip(\"\\n\") for x in open(data_title_sample).readlines()])}\n",
    "    inference(model,\"Data/middle/1day_measure_sample_valid.txt\",\"val/cnn_1day_measure_{}.txt\".format(suffix),title_dict,data_industry_sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([1]), list([1, 2])], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
