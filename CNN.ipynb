{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function # Use a function definition from future version (say 3.x from 2.7 interpreter)\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import time \n",
    "\n",
    "import cntk as C\n",
    "import cntk.tests.test_utils\n",
    "import pickle\n",
    "import random\n",
    "from cntk import sequence\n",
    "from cntk import load_model\n",
    "from cntk.device import try_set_default_device, gpu,cpu\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "cntk.tests.test_utils.set_device_from_pytest_env() # (only needed for our build system)\n",
    "C.cntk_py.set_fixed_random_seed(1) # fix a random seed for CNTK components\n",
    "try_set_default_device(gpu(0))\n",
    "\n",
    "\n",
    "vocab_size = 80000\n",
    "num_labels = 19\n",
    "title_size = 52000\n",
    "body_size  = 210000\n",
    "input_dim  = vocab_size\n",
    "label_dim  = num_labels\n",
    "emb_dim    = 300\n",
    "hidden_dim = 200\n",
    "\n",
    "max_length_title = 30\n",
    "max_length_body  = 100\n",
    "\n",
    "\n",
    "\n",
    "suffix = \"180days_sample\"\n",
    "prefix = \"/home/t-haohu/IndustryClassifier/Data/\"\n",
    "\n",
    "#data_token_body        = \"{}/middle/{}_token_body.txt\".format(prefix,suffix)\n",
    "data_train_sample = \"{}/middle/train_{}.txt\".format(prefix,suffix)\n",
    "data_test_sample  = \"{}/middle/test_{}.txt\".format(prefix,suffix)\n",
    "\n",
    "data_industry_sample = \"{}/ready/industry_{}.wl\".format(prefix,suffix)\n",
    "data_title_sample    = \"{}/ready/title_{}.wl\".format(prefix,suffix)\n",
    "data_body_sample     = \"{}/ready/body_{}.wl\".format(prefix,suffix)\n",
    "\n",
    " \n",
    "\n",
    "def load_data(input_file,title_dict,industry_dict):\n",
    "    data = open(input_file, encoding = \"utf-8\").readlines()\n",
    "    \n",
    "    data_title = np.zeros((len(data),max_length_title),dtype = np.float32)\n",
    "    data_label = np.zeros((len(data),1),dtype = np.float32)\n",
    "    \n",
    "    \n",
    "    for index,line in enumerate(data):\n",
    "        row = line.strip(\"\\n\").split(\"\\t\")\n",
    "        \n",
    "        title    =  row[0]\n",
    "        #body     =  row[1]\n",
    "        industry =  row[1]\n",
    "        \n",
    "        for jndex,token in enumerate(title.split(\" \")):\n",
    "            if jndex>=max_length_title:\n",
    "                break\n",
    "            data_title[index,jndex]=title_dict.get(token,len(title_dict)-1)    \n",
    "        data_label[index] = industry_dict.get(industry,len(industry_dict))\n",
    "    return data_title,data_label\n",
    "\n",
    "def create_model_cnn(embed = False):\n",
    "    #version 2 : 1 dense layer version3: sigmoid activation in dense\n",
    "    #\n",
    "    \n",
    "\n",
    "\n",
    "    if embed:\n",
    "        h1= C.layers.Embedding(weights=embedding_title,name='embed_1')(input_xt_one_hot)#\n",
    "    else:\n",
    "        h1= C.layers.Embedding(300,name='embed_2')(input_xt_one_hot)#init=embedding,\n",
    "\n",
    "    filter_num=100\n",
    "\n",
    "    h2_1=C.layers.Convolution((1,emb_dim),num_filters=filter_num,reduction_rank=0,activation=C.relu)(h1)\n",
    "    h2_2=C.layers.Convolution((2,emb_dim),num_filters=filter_num,reduction_rank=0,activation=C.relu)(h1)\n",
    "    h2_3=C.layers.Convolution((3,emb_dim),num_filters=filter_num,reduction_rank=0,activation=C.relu)(h1)\n",
    "\n",
    "    h3_1=C.layers.MaxPooling((max_length_title-0,1),name='pooling_1')(h2_1)\n",
    "    h3_2=C.layers.MaxPooling((max_length_title-1,1),name='pooling_2')(h2_2)\n",
    "    h3_3=C.layers.MaxPooling((max_length_title-2,1),name='pooling_3')(h2_3)\n",
    "    #h2=BiRecurrence(C.layers.LSTM(hidden_dim), C.layers.LSTM(hidden_dim))(h1)\n",
    "    h3=C.splice(h3_2,h3_1,h3_3,axis=0)\n",
    "    drop1 = C.layers.Dropout(0.1)(h3)\n",
    "    h4=C.layers.Dense(num_labels,name='hidden')(drop1)\n",
    "\n",
    "    return h4\n",
    "\n",
    "def batch_iter(data,batch_size, num_epochs, shuffle=True):\n",
    "    # Generates a batch iterator for a dataset.\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((data_size-1)/batch_size) + 1\n",
    "    print('data_size: ', data_size, 'batch_size: ', batch_size, 'num_batches_per_epoch: ', num_batches_per_epoch)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            random.shuffle(data)\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield data[start_index:end_index]\n",
    "            \n",
    "\n",
    "def fast_hist(a, b, n):\n",
    "    k = (a >= 0) & (a < n)\n",
    "    return np.bincount(n * a[k].astype(int) + b[k], minlength=n**2).reshape(n, n)\n",
    "\n",
    "title_dict =     { x:i for i,x in enumerate([x.strip(\"\\n\") for x in open(data_title_sample).readlines()])}\n",
    "industry_dict =  { x:i for i,x in enumerate([x.strip(\"\\n\") for x in open(data_industry_sample).readlines()])}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_xt = C.input_variable(shape=(max_length_title),  dtype=np.float32)\n",
    "input_y  = C.input_variable(shape=(1)               ,  dtype=np.int)\n",
    "\n",
    "input_xt_one_hot = C.one_hot(input_xt, num_classes=len(title_dict)   ,  sparse_output=True)\n",
    "input_y_one_hot = C.one_hot(input_y  , num_classes=len(industry_dict) ,  sparse_output=True)\n",
    "\n",
    "\n",
    "test_data  = load_data(data_test_sample,title_dict,industry_dict)\n",
    "train_data = load_data(data_train_sample,title_dict,industry_dict)\n",
    "\n",
    "def test(batch_size,model,data):\n",
    "    scores = model(input_xt)\n",
    "    predict = C.argmax(scores,axis = 0)\n",
    "    confuse = np.zeros((num_labels,num_labels))\n",
    "\n",
    "    test_data_title,test_data_label = data\n",
    "    batches = batch_iter(list(zip(test_data_title,test_data_label)), batch_size, 1)\n",
    "    \n",
    "    for batch in batches:\n",
    "        batch_data_title,batch_data_label = zip(*batch) \n",
    "        output = np.array(predict.eval({input_xt: np.array(batch_data_title)}),dtype=np.int)\n",
    "        gt = np.array(batch_data_label,dtype=np.int)\n",
    "        confuse+=fast_hist(output,gt,num_labels)\n",
    "        \n",
    "    precision=np.diag(confuse)/np.sum(confuse,axis=0)\n",
    "    recall = np.diag(confuse)/np.sum(confuse,axis=1)\n",
    "    accuarcy = np.diag(confuse).sum() / confuse.sum()\n",
    "    aver_precision=np.nanmean(precision)\n",
    "    aver_recall = np.nanmean(recall)\n",
    "   \n",
    "    print(\"Precision:{} Recall:{} Acc:{}\".format(aver_precision,aver_recall,accuarcy))\n",
    "    \n",
    "def train(train_data,num_epochs,learning_rate,batch_size,l2_weight=0):\n",
    "    \n",
    "    #learning_rate *= batch_size\n",
    "    model = create_model_cnn()\n",
    "    print(C.logging.get_node_outputs(model))\n",
    "    scores = model(input_xt)\n",
    "\n",
    "    loss =C.reduce_mean(C.losses.cross_entropy_with_softmax(scores, input_y_one_hot))\n",
    "    \n",
    "    # Training\n",
    "    lr_schedule = C.learning_parameter_schedule(learning_rate)\n",
    "    #learner = C.adam(scores.parameters, lr=lr_schedule, momentum=0.9,l2_regularization_weight=0)\n",
    "    progress_printer = C.logging.ProgressPrinter(tag='Training', num_epochs=num_epochs)\n",
    "    momentums = C.momentum_schedule(0.99, minibatch_size=batch_size)\n",
    "    learner = C.adam(parameters=scores.parameters,#model.parameters,\n",
    "                     lr=lr_schedule,\n",
    "                     momentum=momentums,\n",
    "                     gradient_clipping_threshold_per_sample=15,\n",
    "                     gradient_clipping_with_truncation=True,\n",
    "                     l2_regularization_weight=l2_weight)\n",
    "    trainer = C.Trainer(scores, (loss), [learner], progress_printer)\n",
    "    \n",
    "    train_data_title,train_data_label = train_data\n",
    "    batches = batch_iter(list(zip(train_data_title,train_data_label)), batch_size, num_epochs)\n",
    "\n",
    "    # training loop\n",
    "    count = 0\n",
    "    t = time.time()\n",
    "    for batch in batches:\n",
    "        count += 1\n",
    "        batch_data_title,batch_data_label = zip(*batch)\n",
    "        \n",
    "        trainer.train_minibatch({input_xt: np.array(batch_data_title), input_y: np.array(batch_data_label)})\n",
    "        if count%1398== 0:\n",
    "            print(count,time.time()-t)\n",
    "            t=time.time()\n",
    "            test(batch_size,model,test_data)\n",
    "\n",
    "    # save model\n",
    "    model.save('./model/IndustryClassifyCNN.dnn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Output('hidden', [#], [19]), Output('Block13273_Output_0', [#], [300 x 1 x 1]), Output('Splice13253_Output_0', [#], [300 x 1 x 1]), Output('pooling_2', [#], [100 x 1 x 1]), Output('Block13044_Output_0', [#], [100 x 29 x 1]), Output('embed_2', [#], [30 x 300]), Output('OneHotOp12907_Output_0', [#], [30 x 23184]), Output('pooling_1', [#], [100 x 1 x 1]), Output('Block12981_Output_0', [#], [100 x 30 x 1]), Output('pooling_3', [#], [100 x 1 x 1]), Output('Block13107_Output_0', [#], [100 x 28 x 1])]\n",
      "data_size:  41940 batch_size:  30 num_batches_per_epoch:  1398\n",
      "Learning rate per minibatch: 0.015\n",
      "1398 18.27876329421997\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t-haohu/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/ipykernel_launcher.py:147: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:0.5110806497883653 Recall:0.6923817686419086 Acc:0.6890681502086231\n",
      "2796 18.42057991027832\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.6082792499779451 Recall:0.7130376981205395 Acc:0.7360222531293463\n",
      "4194 17.433727502822876\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.6198075709116803 Recall:0.6513190393174622 Acc:0.7281223922114047\n",
      "5592 17.409441471099854\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.6129659700487289 Recall:0.6548085207460945 Acc:0.7156606397774687\n",
      "6990 17.552210807800293\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.6105506616896933 Recall:0.6341483941328568 Acc:0.7164951321279555\n",
      "8388 17.567949771881104\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.6027486388223229 Recall:0.6320560028956943 Acc:0.7081502086230876\n",
      "9786 17.82224178314209\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.6091101021185378 Recall:0.635615448132218 Acc:0.7076495132127956\n",
      "11184 17.545870304107666\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.6115821741252637 Recall:0.6362688847520123 Acc:0.71221140472879\n",
      "12582 17.47333073616028\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.6051028579237214 Recall:0.6392542649174329 Acc:0.7116550764951322\n",
      "13980 18.456584930419922\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.6025016415106282 Recall:0.6317579681931695 Acc:0.7063699582753825\n",
      "15378 17.68167996406555\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.6053899344671073 Recall:0.6312213800142707 Acc:0.7074269819193324\n",
      "16776 17.417667388916016\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.602322150715592 Recall:0.6280629256387111 Acc:0.7036995827538247\n",
      "18174 17.6495840549469\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.6032733423967922 Recall:0.6300017189392806 Acc:0.7064812239221141\n",
      "19572 17.559253931045532\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.6028568927156011 Recall:0.6263969484877826 Acc:0.704144645340751\n",
      "20970 17.74466586112976\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.6049619711296719 Recall:0.6307289867825819 Acc:0.7063143254520167\n",
      "22368 17.521804571151733\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.5998188208627173 Recall:0.6300356818818456 Acc:0.7060917941585535\n",
      "23766 17.456469774246216\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.5951237895823996 Recall:0.6272379447424201 Acc:0.7004728789986092\n",
      "25164 18.266122102737427\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.598506237845165 Recall:0.6260714674449724 Acc:0.7036995827538247\n",
      "26562 17.27428150177002\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.5934793764625472 Recall:0.616705523518914 Acc:0.6970792767732963\n",
      "27960 17.154330492019653\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.5859199336875188 Recall:0.6215933671725885 Acc:0.6912934631432546\n"
     ]
    }
   ],
   "source": [
    "train(train_data,num_epochs=20,learning_rate=[5e-4*30]*2+[1e-4*30],batch_size = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
