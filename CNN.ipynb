{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function # Use a function definition from future version (say 3.x from 2.7 interpreter)\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import cntk as C\n",
    "import cntk.tests.test_utils\n",
    "import pickle\n",
    "import random\n",
    "from cntk import sequence\n",
    "from cntk import load_model\n",
    "from cntk.device import try_set_default_device, gpu,cpu\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "cntk.tests.test_utils.set_device_from_pytest_env() # (only needed for our build system)\n",
    "C.cntk_py.set_fixed_random_seed(1) # fix a random seed for CNTK components\n",
    "try_set_default_device(gpu(0))\n",
    "\n",
    "\n",
    "vocab_size = 80000\n",
    "num_labels = 19\n",
    "title_size = 52000\n",
    "body_size  = 210000\n",
    "input_dim  = vocab_size\n",
    "label_dim  = num_labels\n",
    "emb_dim    = 300\n",
    "hidden_dim = 200\n",
    "\n",
    "max_length_title = 50\n",
    "max_length_body  = 200\n",
    "\n",
    "\n",
    "\n",
    "suffix = \"180days_sample\"\n",
    "prefix = \"C:\\\\Users\\\\t-haohu\\\\Documents\\\\Python\\\\IndustryClassifier\\\\Data\"\n",
    "\n",
    "data_token_body        = \"{}\\\\middle\\\\{}_token_body.txt\".format(prefix,suffix)\n",
    "data_train_sample_body = \"{}\\\\middle\\\\train_{}_body.txt\".format(prefix,suffix)\n",
    "data_test_sample_body  = \"{}\\\\middle\\\\test_{}_body.txt\".format(prefix,suffix)\n",
    "\n",
    "data_industry_sample = \"{}\\\\ready\\\\industry_{}.wl\".format(prefix,suffix)\n",
    "data_title_sample    = \"{}\\\\ready\\\\title_{}.wl\".format(prefix,suffix)\n",
    "data_body_sample     = \"{}\\\\ready\\\\body_{}.wl\".format(prefix,suffix)\n",
    "\n",
    "input_xt = C.input_variable(shape=(max_length_title),  dtype=np.int)\n",
    "input_xb = C.input_variable(shape=(max_length_body) ,  dtype=np.int)\n",
    "input_y  = C.input_variable(shape=(1)               ,  dtype=np.float32)\n",
    "\n",
    "input_xt_one_hot = C.one_hot(input_xt, num_classes=len(title_dict)   ,  sparse_output=True)\n",
    "input_xb_one_hot = C.one_hot(input_xb, num_classes=len(body_dict)    ,  sparse_output=True) \n",
    "input_y_one_hot = C.one_hot(input_y , num_classes=len(industry_dict) ,  sparse_output=True) \n",
    "\n",
    "def load_data_body(input_file,title_dict,body_dict,industry_dict):\n",
    "    data = open(input_file, encoding = \"utf-8\").readlines()\n",
    "    \n",
    "    data_title = np.zeros((len(data),max_length_title))\n",
    "    data_body  = np.zeros((len(data),max_length_body))\n",
    "    data_label = np.zeros((len(data),1))\n",
    "    \n",
    "    \n",
    "    for index,line in enumerate(data):\n",
    "        row = line.strip(\"\\n\").split(\"\\t\")\n",
    "        \n",
    "        title    =  row[0]\n",
    "        body     =  row[1]\n",
    "        industry =  row[2]\n",
    "        \n",
    "        for jndex,token in enumerate(title.split(\" \")):\n",
    "            if jndex>=max_length_title:\n",
    "                break\n",
    "            data_title[index,jndex]=title_dict.get(token,len(title_dict)-1)\n",
    "            \n",
    "        for jndex,token in enumerate(body.split(\" \")):\n",
    "            if jndex>=max_length_body:\n",
    "                break\n",
    "            data_body[index,jndex]=body_dict.get(token,len(title_dict)-1)\n",
    "            \n",
    "        data_label[index] = industry_dict.get(industry,len(industry_dict))\n",
    "    return data_title,data_body,data_label\n",
    "\n",
    "def create_model_cnn_with_body():\n",
    "    \n",
    "    h1t= C.layers.Embedding(300,name='embed')(input_xt_one_hot)#init=embedding,\n",
    "    h1b= C.layers.Embedding(300,name='embed')(input_xb_one_hot)#init=embedding,\n",
    "    \n",
    "    #bnb = C.layers.BatchNormalization(name='bn')(h1b)\n",
    "    #bnt = C.layers.BatchNormalization(name='bn')(h1t)\n",
    "\n",
    "\n",
    "\n",
    "    h2_1t=C.layers.Convolution((1,emb_dim),num_filters=100,reduction_rank=0,activation=C.relu)(h1t)\n",
    "    h2_2t=C.layers.Convolution((2,emb_dim),num_filters=100,reduction_rank=0,activation=C.relu)(h1t)\n",
    "    h2_3t=C.layers.Convolution((3,emb_dim),num_filters=100,reduction_rank=0,activation=C.relu)(h1t)\n",
    "\n",
    "    h2_1b=C.layers.Convolution((1,emb_dim),num_filters=100,reduction_rank=0,activation=C.relu)(h1b)\n",
    "    h2_2b=C.layers.Convolution((2,emb_dim),num_filters=100,reduction_rank=0,activation=C.relu)(h1b)\n",
    "    h2_3b=C.layers.Convolution((3,emb_dim),num_filters=100,reduction_rank=0,activation=C.relu)(h1b)\n",
    "\n",
    "    h3_2t=C.layers.MaxPooling((max_length_title-1,1),name='pooling_t_1')(h2_2t)\n",
    "    h3_1t=C.layers.MaxPooling((max_length_title-0,1),name='pooling_t_2')(h2_1t)\n",
    "    h3_3t=C.layers.MaxPooling((max_length_title-2,1),name='pooling_t_3')(h2_3t)\n",
    "\n",
    "    h3_2b=C.layers.MaxPooling((max_length_body-1,1),name='pooling_b_1')(h2_2b)\n",
    "    h3_1b=C.layers.MaxPooling((max_length_body-0,1),name='pooling_b_2')(h2_1b)\n",
    "    h3_3b=C.layers.MaxPooling((max_length_body-2,1),name='pooling_b_3')(h2_3b)\n",
    "\n",
    "    h3=C.splice(h3_2t,h3_1t,h3_3t,h3_2b,h3_1b,h3_3b,axis=0)\n",
    "\n",
    "    h4=C.layers.Dense(hidden_dim, activation=C.relu,name='hidden')(h3)\n",
    "    drop2 = C.layers.Dropout(keep_prob=0.5)(h4)\n",
    "\n",
    "    h5=C.layers.Dense(num_labels,name='classify')(drop2)\n",
    "\n",
    "    return h5\n",
    "\n",
    "def batch_iter(data,batch_size, num_epochs, shuffle=True):\n",
    "    # Generates a batch iterator for a dataset.\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((data_size-1)/batch_size) + 1\n",
    "    print('data_size: ', data_size, 'batch_size: ', batch_size, 'num_batches_per_epoch: ', num_batches_per_epoch)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            random.shuffle(data)\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield data[start_index:end_index]\n",
    "            \n",
    "def transfer(_tuple):\n",
    "    _list = list(_tuple)\n",
    "    return [item.tolist() for item in _list]\n",
    "\n",
    "title_dict =     { x:i for i,x in enumerate([x.strip(\"\\n\") for x in open(data_title_sample).readlines()])}\n",
    "body_dict  =     { x:i for i,x in enumerate([x.strip(\"\\n\") for x in open(data_body_sample ).readlines()])}\n",
    "industry_dict =  { x:i for i,x in enumerate([x.strip(\"\\n\") for x in open(data_industry_sample).readlines()])}\n",
    "\n",
    "\n",
    "test_data_title, test_data_body, test_data_label  = load_data_body(data_test_sample_body ,title_dict,body_dict,industry_dict)\n",
    "\n",
    "\n",
    "\n",
    "def train(num_epochs,learning_rate,batch_size):\n",
    "    \n",
    "    \n",
    "    model = create_model_cnn_with_body()\n",
    "    print(C.logging.get_node_outputs(model))\n",
    "    scores = model(input_xt,input_xb)\n",
    "\n",
    "    loss =C.reduce_mean(C.losses.cross_entropy_with_softmax(scores, input_y_one_hot))\n",
    "    \n",
    "    # Training\n",
    "    lr_schedule = C.learning_parameter_schedule(learning_rate)\n",
    "    learner = C.adam(scores.parameters, lr=lr_schedule, momentum=0.9,l2_regularization_weight=0.0001)\n",
    "    progress_printer = C.logging.ProgressPrinter(tag='Training', num_epochs=num_epochs)\n",
    "    \n",
    "    trainer = C.Trainer(scores, (loss), [learner], progress_printer)\n",
    "    \n",
    "    train_data_title,train_data_body,train_data_label = load_data_body(data_train_sample_body,title_dict,body_dict,industry_dict)\n",
    "    batches = batch_iter(list(zip(train_data_title,train_data_body,train_data_label)), batch_size, num_epochs)\n",
    "\n",
    "    # training loop\n",
    "    count = 0\n",
    "    for batch in batches:\n",
    "        count += 1\n",
    "        batch_data_title,batch_data_body,batch_data_label = zip(*batch)\n",
    "        #batch_data_title,batch_data_body,batch_data_label = transfer(batch_data_title),transfer(batch_data_body),trainsfer(batch_data_label)\n",
    "        trainer.train_minibatch({input_xb: np.array(batch_data_body),input_xt: np.array(batch_data_title), input_y: np.array(batch_data_label)})\n",
    "\n",
    "        \n",
    "\n",
    "    # save model\n",
    "    scores.save('./model/IndustryClassifyCNN_body.dnn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train(num_epochs=10,learning_rate=1e-4,batch_size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = [1,2,3]\n",
    "y = [4,5,6]\n",
    "zipped = zip(x,y)\n",
    "unzipped_x, unzipped_y = zip(*zipped)\n",
    "print(list(zipped),unzipped_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [cntk-py35]",
   "language": "python",
   "name": "Python [cntk-py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
