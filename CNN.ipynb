{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function # Use a function definition from future version (say 3.x from 2.7 interpreter)\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import time \n",
    "\n",
    "import cntk as C\n",
    "import cntk.tests.test_utils\n",
    "from cntk.layers import *\n",
    "from cntk.layers.typing import *\n",
    "import pickle\n",
    "import random\n",
    "from cntk import sequence\n",
    "from cntk import load_model\n",
    "from cntk.device import try_set_default_device, gpu,cpu\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "cntk.tests.test_utils.set_device_from_pytest_env() # (only needed for our build system)\n",
    "C.cntk_py.set_fixed_random_seed(1) # fix a random seed for CNTK components\n",
    "try_set_default_device(gpu(0))\n",
    "\n",
    "\n",
    "vocab_size = 80000\n",
    "num_labels = 19#19\n",
    "title_size = 52000\n",
    "body_size  = 210000\n",
    "input_dim  = vocab_size\n",
    "label_dim  = num_labels\n",
    "emb_dim    = 300\n",
    "hidden_dim = 200\n",
    "\n",
    "max_length_title = 30\n",
    "max_length_body  = 100\n",
    "\n",
    "\n",
    "\n",
    "suffix = \"180days_all_shuffled\"\n",
    "#suffix = \"linkedin_only\"\n",
    "prefix = \"/home/t-haohu/IndustryClassifier/Data/\"\n",
    "\n",
    "\n",
    "data_train_sample = \"{}/middle/train_{}.txt\".format(prefix,suffix)\n",
    "#data_train_sample = \"{}/middle/train_{}_with_linkedin_all.txt\".format(prefix,suffix)\n",
    "suffix = \"180days_all_shuffled\"\n",
    "data_test_sample  = \"{}/middle/test_{}.txt\".format(prefix,suffix)\n",
    "#data_test_sample_editor  = \"{}/middle/test_{}_editor.txt\".format(prefix,suffix)\n",
    "\n",
    "suffix = \"180days_all_shuffled\"\n",
    "data_title_sample    = \"{}/ready/title_{}.wl\".format(prefix,suffix)\n",
    "data_body_sample     = \"{}/ready/body_{}.wl\".format(prefix,suffix)\n",
    "suffix = \"180days_all_shuffled\"\n",
    "data_industry_sample = \"{}/ready/industry_{}.wl\".format(prefix,suffix)\n",
    "suffix = \"180days_all_shuffled\"\n",
    "filter_num=200 \n",
    "dropout_rate = 0.5\n",
    "emb_dim =300\n",
    "\n",
    "def load_data_dynamic(input_file,title_dict,industry_dict):\n",
    "    data = open(input_file, encoding = \"utf-8\").readlines()\n",
    "    \n",
    "    data_title =[ [] for x in range(len(data))]#np.zeros((len(data),max_length_title),dtype = np.float32)\n",
    "    data_label = np.zeros((len(data),1),dtype = np.float32)\n",
    "    \n",
    "    \n",
    "    for index,line in enumerate(data):\n",
    "        row = line.strip(\"\\n\").split(\"\\t\")       \n",
    "        title    =  row[0]\n",
    "        industry =  row[1]\n",
    "        \n",
    "        for jndex,token in enumerate(title.split(\" \")):\n",
    "            if jndex>=max_length_title:\n",
    "                break\n",
    "            data_title[index].append(title_dict.get(token,len(title_dict)-1))\n",
    "        while len(data_title[index])<5:\n",
    "            data_title[index].append(len(title_dict)-1)\n",
    "        data_label[index] = industry_dict.get(industry,len(industry_dict))\n",
    "    data_title = [ np.array(x) for x in data_title]\n",
    "    return data_title,data_label\n",
    "\n",
    "def load_data_static(input_file,title_dict,industry_dict):\n",
    "    data = open(input_file, encoding = \"utf-8\").readlines()\n",
    "    \n",
    "    data_title =np.zeros((len(data),max_length_title),dtype = np.float32)\n",
    "    data_label = np.zeros((len(data),1),dtype = np.float32)\n",
    "    \n",
    "    \n",
    "    for index,line in enumerate(data):\n",
    "        row = line.strip(\"\\n\").split(\"\\t\")       \n",
    "        title    =  row[0]\n",
    "        industry =  row[1]\n",
    "        \n",
    "        for jndex,token in enumerate(title.split(\" \")):\n",
    "            if jndex>=max_length_title:\n",
    "                break\n",
    "            data_title[index,jndex]=title_dict.get(token,len(title_dict)-1)    \n",
    "        data_label[index] = industry_dict.get(industry,len(industry_dict))\n",
    "    return data_title,data_label\n",
    "\n",
    "def load_embedding(title_file,embedding_model_file):\n",
    "    model = Word2Vec.load(embedding_model_file)\n",
    "    title_list = [x.strip(\"\\n\") for x in open(title_file,encoding = 'utf-8').readlines()]\n",
    "    embedding = np.zeros((len(title_list),emb_dim))\n",
    "    count = 0\n",
    "    for i,w in enumerate(title_list):\n",
    "        try:\n",
    "            vec = model.wv[w]\n",
    "        except:\n",
    "            vec=model.wv[\"UNK\"]\n",
    "            count+=1\n",
    "        embedding[i] =vec\n",
    "    print(count)\n",
    "    return embedding\n",
    "\n",
    "def create_model_cnn(embed = False):\n",
    "    #version 2 : 1 dense layer version3: sigmoid activation in dense\n",
    "    if embed:\n",
    "        h1= C.layers.Embedding(weights=embedding,name='embed_1')(input_xt_one_hot)#\n",
    "    else:\n",
    "        h1= C.layers.Embedding(emb_dim,name='embed_2')(input_xt_one_hot)#init=embedding,\n",
    "\n",
    "    \n",
    "\n",
    "    h2_1=C.layers.Convolution((1,emb_dim),num_filters=filter_num,reduction_rank=0,activation=C.relu)(h1)\n",
    "    h2_2=C.layers.Convolution((2,emb_dim),num_filters=filter_num,reduction_rank=0,activation=C.relu)(h1)\n",
    "    h2_3=C.layers.Convolution((3,emb_dim),num_filters=filter_num,reduction_rank=0,activation=C.relu)(h1)\n",
    "    \n",
    "    h3_1=C.layers.MaxPooling((max_length_title-0,1),name='pooling_1')(h2_1)\n",
    "    h3_2=C.layers.MaxPooling((max_length_title-1,1),name='pooling_2')(h2_2)\n",
    "    h3_3=C.layers.MaxPooling((max_length_title-2,1),name='pooling_3')(h2_3)\n",
    "    #h2=BiRecurrence(C.layers.LSTM(hidden_dim), C.layers.LSTM(hidden_dim))(h1)\n",
    "    h3=C.splice(h3_2,h3_1,h3_3,axis=0)\n",
    "    drop1 = C.layers.Dropout(dropout_rate)(h3)\n",
    "    h4=C.layers.Dense(num_labels,name='hidden')(drop1)\n",
    "\n",
    "    return h4\n",
    "\n",
    "def create_model_cnn_dynamic(embed = False):\n",
    "    #version 2 : 1 dense layer version3: sigmoid activation in dense\n",
    "    if embed:\n",
    "        h1= C.layers.Embedding(weights=embedding,name='embed_1')(input_xt_one_hot)#\n",
    "    else:\n",
    "        h1= C.layers.Embedding(emb_dim,name='embed_2')(input_xt_one_hot)#init=embedding,\n",
    "\n",
    "    h1 = C.squeeze(h1)\n",
    "    print(h1)\n",
    "    h2_1=C.layers.Convolution((1,emb_dim),num_filters=filter_num,reduction_rank=0,activation=C.relu,sequential=True)(h1)\n",
    "    h2_2=C.layers.Convolution((2,emb_dim),num_filters=filter_num,reduction_rank=0,activation=C.relu,sequential=True)(h1)\n",
    "    h2_3=C.layers.Convolution((3,emb_dim),num_filters=filter_num,reduction_rank=0,activation=C.relu,sequential=True)(h1)\n",
    "    seq_MaxPooling = C.layers.Fold(C.element_max)\n",
    "    h3_1=seq_MaxPooling(h2_1)\n",
    "    h3_2=seq_MaxPooling(h2_2)\n",
    "    h3_3=seq_MaxPooling(h2_3)\n",
    "    #h2=BiRecurrence(C.layers.LSTM(hidden_dim), C.layers.LSTM(hidden_dim))(h1)\n",
    "    h3=C.splice(h3_2,h3_1,h3_3,axis=0)\n",
    "    drop1 = C.layers.Dropout(dropout_rate)(h3)\n",
    "    h4=C.layers.Dense(num_labels,name='hidden')(drop1)\n",
    "\n",
    "    return h4\n",
    "\n",
    "def create_model_cnn_2fold(dynamic = False):\n",
    "    #version 2 : 1 dense layer version3: sigmoid activation in dense\n",
    "    #\n",
    "    with C.layers.default_options(initial_state=0.1):\n",
    "\n",
    "\n",
    "        h1_1= C.layers.Embedding(weights=embedding,name='embed_1')(input_xt_one_hot)#\n",
    "        h1_2= C.layers.Embedding(300,name='embed_2')(input_xt_one_hot)#init=embedding,\n",
    "        \n",
    "        \n",
    "        \n",
    "        h1_1_expand = C.expand_dims(h1_1,-3)\n",
    "        h1_2_expand = C.expand_dims(h1_2,-3)\n",
    "        \n",
    "        h1 = C.splice(h1_1_expand,h1_2_expand,axis = -3)\n",
    "        \n",
    "        #bn = C.layers.BatchNormalization(name='bn')(h1)\n",
    "        \n",
    "\n",
    "        #value,valid = to_static(h1)\n",
    "\n",
    "        filter_num=100\n",
    "\n",
    "        h2_1=C.layers.Convolution((3,emb_dim),num_filters=filter_num,reduction_rank=1,activation=C.relu)(h1)\n",
    "        h2_2=C.layers.Convolution((4,emb_dim),num_filters=filter_num,reduction_rank=1,activation=C.relu)(h1)\n",
    "        h2_3=C.layers.Convolution((5,emb_dim),num_filters=filter_num,reduction_rank=1,activation=C.relu)(h1)\n",
    "        if dynamic:\n",
    "            seq_MaxPooling = C.layers.Fold(C.element_max)\n",
    "            h3_1=seq_MaxPooling(h2_1)\n",
    "            h3_2=seq_MaxPooling(h2_2)\n",
    "            h3_3=seq_MaxPooling(h2_3)\n",
    "        else:\n",
    "            h3_1=C.layers.MaxPooling((max_length_title-2,1),name='pooling_1')(h2_1)\n",
    "            h3_2=C.layers.MaxPooling((max_length_title-3,1),name='pooling_2')(h2_2)\n",
    "            h3_3=C.layers.MaxPooling((max_length_title-4,1),name='pooling_3')(h2_3)\n",
    "        \n",
    "        h3=C.splice(h3_2,h3_1,h3_3,axis=0)\n",
    "        drop1 =C.layers.Dropout(0.5)(h3)\n",
    "        h4=C.layers.Dense(num_labels,name='hidden')(drop1)\n",
    "\n",
    "    return h4\n",
    "\n",
    "def batch_iter(data,batch_size, num_epochs, shuffle=True):\n",
    "    # Generates a batch iterator for a dataset.\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((data_size-1)/batch_size) + 1\n",
    "    print('data_size: ', data_size, 'batch_size: ', batch_size, 'num_batches_per_epoch: ', num_batches_per_epoch)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            random.shuffle(data)\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield data[start_index:end_index]\n",
    "            \n",
    "\n",
    "def fast_hist(a, b, n):\n",
    "    k = (a >= 0) & (a < n)\n",
    "    return np.bincount(n * a[k].astype(int) + b[k], minlength=n**2).reshape(n, n)\n",
    "\n",
    "title_dict =     { x:i for i,x in enumerate([x.strip(\"\\n\") for x in open(data_title_sample).readlines()])}\n",
    "industry_dict =  { x:i for i,x in enumerate([x.strip(\"\\n\") for x in open(data_industry_sample).readlines()])}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#input_xt = C.input_variable(shape=(max_length_title),  dtype=np.float32)\n",
    "input_xt = C.input_variable(**Sequence[Tensor[1]])\n",
    "input_y  = C.input_variable(shape=(1))\n",
    "\n",
    "input_xt_one_hot = C.one_hot(input_xt, num_classes=len(title_dict)   ,  sparse_output=True)\n",
    "input_y_one_hot = C.one_hot(input_y  , num_classes=len(industry_dict) ,  sparse_output=True)\n",
    "\n",
    "\n",
    "test_data  = load_data_dynamic(data_test_sample,title_dict,industry_dict)\n",
    "train_data = load_data_dynamic(data_train_sample,title_dict,industry_dict)\n",
    "#test_data_editor  = load_data(data_test_sample_editor,title_dict,industry_dict)\n",
    "embedding = load_embedding(data_title_sample,\"word2vec.model\")\n",
    "\n",
    "\n",
    "def test(batch_size,model,data):\n",
    "    scores = model(input_xt)\n",
    "    predict = C.argmax(scores,axis = 0)\n",
    "    confuse = np.zeros((num_labels,num_labels))\n",
    "\n",
    "    test_data_title,test_data_label = data\n",
    "    batches = batch_iter(list(zip(test_data_title,test_data_label)), batch_size, 1)\n",
    "    \n",
    "    for batch in batches:\n",
    "        batch_data_title,batch_data_label = zip(*batch) \n",
    "        batch_data_title = [[[x] for x in y] for y in list(batch_data_title)] \n",
    "        batch_data_label = [x.tolist() for x in batch_data_label]\n",
    "        output = np.array(predict.eval({input_xt: batch_data_title}),dtype=np.int)\n",
    "        gt = np.array(batch_data_label,dtype=np.int)\n",
    "        confuse+=fast_hist(output,gt,num_labels)\n",
    "        \n",
    "    precision=np.diag(confuse)/np.sum(confuse,axis=0)\n",
    "    recall = np.diag(confuse)/np.sum(confuse,axis=1)\n",
    "    accuarcy = np.diag(confuse).sum() / confuse.sum()\n",
    "    aver_precision=np.nanmean(precision)\n",
    "    aver_recall = np.nanmean(recall)\n",
    "   \n",
    "    print(\"Precision:{} Recall:{} Acc:{}\".format(aver_precision,aver_recall,accuarcy))\n",
    "    return accuarcy\n",
    "\n",
    "\n",
    "\n",
    "def train(train_data,num_epochs,learning_rate,batch_size,tag=\"CNN\",l2_weight=0):\n",
    "    global model\n",
    "    #learning_rate *= batch_size\n",
    "    model = create_model_cnn_dynamic()\n",
    "    print(C.logging.get_node_outputs(model))\n",
    "    scores = model(input_xt)\n",
    "    loss =C.reduce_mean(C.losses.cross_entropy_with_softmax(scores, input_y_one_hot))\n",
    "    \n",
    "    # Training\n",
    "    lr_schedule = C.learning_parameter_schedule(learning_rate)\n",
    "    #learner = C.adam(scores.parameters, lr=lr_schedule, momentum=0.9,l2_regularization_weight=0)\n",
    "    progress_printer = C.logging.ProgressPrinter(tag='Training', num_epochs=num_epochs)\n",
    "    momentums = C.momentum_schedule(0.99, minibatch_size=batch_size)\n",
    "    learner = C.adam(parameters=scores.parameters,#model.parameters,\n",
    "                     lr=lr_schedule,\n",
    "                     momentum=momentums,\n",
    "                     gradient_clipping_threshold_per_sample=15,\n",
    "                     gradient_clipping_with_truncation=True,\n",
    "                     l2_regularization_weight=l2_weight)\n",
    "    trainer = C.Trainer(scores, (loss), [learner], progress_printer)\n",
    "    \n",
    "    train_data_title,train_data_label = train_data\n",
    "    batches = batch_iter(list(zip(train_data_title,train_data_label)), batch_size, num_epochs)\n",
    "\n",
    "    # training loop\n",
    "    count = 0\n",
    "    t = time.time()\n",
    "    for batch in batches:\n",
    "        count += 1\n",
    "        batch_data_title,batch_data_label = zip(*batch)\n",
    "        batch_data_title = [[[x] for x in y] for y in list(batch_data_title)]\n",
    "        batch_data_label = [x.tolist() for x in batch_data_label]\n",
    "        #print(batch_data_label[0])\n",
    "        trainer.train_minibatch({input_xt: batch_data_title, input_y: batch_data_label})\n",
    "        if count%1000== 0:\n",
    "            print(count,time.time()-t)\n",
    "            t=time.time()\n",
    "            acc1=test(batch_size,model,test_data)\n",
    "            #acc2=test(batch_size,model,test_data_editor)\n",
    "\n",
    "            model.save('./model/{}/{}_acc{:.3f}.dnn'.format(suffix,tag,acc1))\n",
    "            #model.save('./model/{}/{}_acc1{:.3f}_acc2{:.3f}.dnn'.format(suffix,tag,acc1,acc2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composite(Sequence[Tensor[1]]) -> Sequence[Tensor[300]]\n",
      "[Output('hidden', [#], [19]), Output('Block135824_Output_0', [#], [600 x 1]), Output('Splice135804_Output_0', [#], [600 x 1]), Output('Block135578_Output_0', [#], [200 x 1]), Output('Block135551_Output_0', [#, defaultDynamicAxis_times_1_minus_1], [200 x 1]), Output('PastValue135535_Output_0', [#, defaultDynamicAxis_times_1_minus_1], [200 x 1]), Output('Block134921_Output_0', [#, defaultDynamicAxis_times_1_minus_1], [200 x 1]), Output('Squeeze134687_Output_0', [#, *], [300]), Output('embed_2', [#, *], [1 x 300]), Output('OneHotOp134653_Output_0', [#, *], [1 x 56178]), Output('Block135484_Output_0', [#], [200 x 1]), Output('Block135457_Output_0', [#, *], [200 x 1]), Output('PastValue135441_Output_0', [#, *], [200 x 1]), Output('Block134740_Output_0', [#, *], [200 x 1]), Output('Block135704_Output_0', [#], [200 x 1]), Output('Block135677_Output_0', [#, defaultDynamicAxis_times_1_minus_2], [200 x 1]), Output('PastValue135661_Output_0', [#, defaultDynamicAxis_times_1_minus_2], [200 x 1]), Output('Block135210_Output_0', [#, defaultDynamicAxis_times_1_minus_2], [200 x 1])]\n",
      "data_size:  768505 batch_size:  150 num_batches_per_epoch:  5124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t-haohu/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/core.py:411: UserWarning: you provided the minibatch data as a list, but your corresponding input variable (uid \"Input134652\") has only one dynamic axis (batch axis). To speed up graph execution, please convert the data beforehand into one NumPy array to speed up training.\n",
      "  'training.' % var.uid)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate per minibatch: 0.007500000000000001\n",
      "400 118.92575216293335\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t-haohu/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/ipykernel_launcher.py:260: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:0.05263157894736842 Recall:0.23429681807141123 Acc:0.23429681807141123\n",
      "800 181.83865976333618\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.05263334469296333 Recall:0.09477864998506957 Acc:0.23429985426281272\n",
      "1200 97.47118616104126\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.14510024707851676 Recall:0.5516306932906031 Acc:0.38345579305319405\n",
      "1600 84.60453581809998\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.2393946249345727 Recall:0.4499892850842112 Acc:0.5147164197230993\n",
      "2000 85.90180444717407\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.34386049245263245 Recall:0.55688253337123 Acc:0.5952787223706583\n",
      "2400 84.29878044128418\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.41526093992224594 Recall:0.6246055768129246 Acc:0.650731722127763\n",
      "2800 124.51237416267395\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.48574068082751864 Recall:0.6632951640854857 Acc:0.6975467573475832\n",
      "3200 174.0798318386078\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.5248719987404451 Recall:0.6849791631940535 Acc:0.7227896526597036\n",
      "3600 175.55839014053345\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.5505129001973511 Recall:0.6779876680102948 Acc:0.7395767549186301\n",
      "4000 173.1364724636078\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.5668436030040002 Recall:0.7116134512620284 Acc:0.7514452271071168\n",
      "4400 172.3503749370575\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.5869028563849426 Recall:0.7365027442692169 Acc:0.7633258440612096\n",
      "4800 174.4780752658844\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.6040991986840252 Recall:0.752135332496262 Acc:0.7728503764877338\n",
      "5200 173.70530462265015\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.6163983638635607 Recall:0.7603129533726568 Acc:0.7803922759290746\n",
      "5600 183.04090476036072\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.6273672416133841 Recall:0.7646888899999932 Acc:0.7864039349040564\n",
      "6000 176.00320172309875\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.6394397322548505 Recall:0.7702465977193155 Acc:0.792564367257712\n",
      "6400 171.34017968177795\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.6490177311357016 Recall:0.7745695131687884 Acc:0.7973585134806899\n",
      "6800 172.08656668663025\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.6602387673981315 Recall:0.7914297942377265 Acc:0.8025655817342725\n",
      "7200 170.32383346557617\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.6698468860216832 Recall:0.7807624462816241 Acc:0.8069832402234637\n",
      "7600 173.55293130874634\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.6772281666374942 Recall:0.7910207226970584 Acc:0.8104930774836046\n",
      "8000 175.61777114868164\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.683290570544614 Recall:0.7950559573643727 Acc:0.8138571775564731\n",
      "8400 171.8667140007019\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.6887731444187385 Recall:0.7962484043732352 Acc:0.8168903327665776\n",
      "8800 173.6866762638092\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.694274771135508 Recall:0.7969959411164437 Acc:0.819407335438426\n",
      "9200 176.32256817817688\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.6995031765420074 Recall:0.8008108035786299 Acc:0.8218514695166383\n",
      "9600 174.957022190094\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7085039521087448 Recall:0.8024896194476331 Acc:0.8247753218362885\n",
      "10000 176.83556842803955\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7150881621315571 Recall:0.8037790316141353 Acc:0.8269947777507894\n",
      "10400 178.5407145023346\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7183815723496101 Recall:0.8181294782148907 Acc:0.8294024775321837\n",
      "10800 174.35642671585083\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7218182061181859 Recall:0.8092019950942766 Acc:0.83082645129949\n",
      "11200 175.52148270606995\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7272465232840523 Recall:0.8185476793239874 Acc:0.8320348554772893\n",
      "11600 173.76734447479248\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7286206741450336 Recall:0.8111080830813759 Acc:0.833173427252854\n",
      "12000 174.79249811172485\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7330395985153113 Recall:0.8211877255885065 Acc:0.834703667719213\n",
      "12400 175.07925534248352\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7361159209596985 Recall:0.80418153425618 Acc:0.8359454700024289\n",
      "12800 175.19834208488464\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7391371532096107 Recall:0.8013061419705918 Acc:0.8370506436725771\n",
      "13200 92.03049778938293\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7396152184473506 Recall:0.8156549332423687 Acc:0.8378582705853777\n",
      "13600 85.15499186515808\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.741309375993394 Recall:0.8164995903383087 Acc:0.8387326937090114\n",
      "14000 85.3517255783081\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7434660710155269 Recall:0.8150930851584983 Acc:0.840056473160068\n",
      "14400 84.31349158287048\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.746408644977988 Recall:0.8102365814270406 Acc:0.8408853534126791\n",
      "14800 83.86549973487854\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7478848727426166 Recall:0.8058141878202664 Acc:0.8420117804226378\n",
      "15200 84.26716470718384\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7484691853533607 Recall:0.8093522963666127 Acc:0.8426615253825601\n",
      "15600 84.39295983314514\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7506398970503472 Recall:0.8094162227324615 Acc:0.8433082341510809\n",
      "16000 85.75350522994995\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.751856784429878 Recall:0.8137904536295214 Acc:0.8435207675491863\n",
      "16400 85.11479210853577\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7530105054821739 Recall:0.8125783430931974 Acc:0.8440763905756619\n",
      "16800 85.2445559501648\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7555656824883707 Recall:0.8122347665393201 Acc:0.8444498421180471\n",
      "17200 83.07121229171753\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7561874613260136 Recall:0.8156142128136287 Acc:0.8452422880738402\n",
      "17600 82.49006414413452\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7576026149185218 Recall:0.8155384762932233 Acc:0.8455944862764149\n",
      "18000 85.12636017799377\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7584673439894754 Recall:0.8176127962014739 Acc:0.8462533398105416\n",
      "18400 86.04154968261719\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7611404094316214 Recall:0.8166111737416358 Acc:0.8468484333252368\n",
      "18800 85.68593549728394\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7629767647856206 Recall:0.8169657925481558 Acc:0.8474890697109546\n",
      "19200 89.25043034553528\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7645847068084081 Recall:0.8161037693923742 Acc:0.847695530726257\n"
     ]
    }
   ],
   "source": [
    "train(train_data,num_epochs=20,learning_rate=[5e-4*150]*2+[1e-4*150],batch_size = 150,tag = \"cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pro Russian Rebel Leader Assassinated in East Ukraine\n",
      "data_size:  361 batch_size:  20 num_batches_per_epoch:  19\n",
      "Pro Russian Rebel Leader Assassinated in East Ukraine\n",
      "data_size:  361 batch_size:  20 num_batches_per_epoch:  19\n"
     ]
    }
   ],
   "source": [
    "from cntk import load_model\n",
    "from data_processor import *\n",
    "process_setting(low=False,old = True,stop = False)\n",
    "input_xt = C.input_variable(**Sequence[Tensor[1]])\n",
    "batch_size = 20\n",
    "def inference(model,val_doc_file,output_file,title_dict,industry_file):\n",
    "    \n",
    "    scores = model(input_xt)\n",
    "    predict = C.argmax(scores,axis = 0)\n",
    "    probability = C.reduce_max(C.softmax(scores),axis = 0)\n",
    "    \n",
    "    industry = [x.strip(\"\\n\") for x in open(industry_file,encoding =\"utf-8\").readlines()]\n",
    "    val_doc = open(val_doc_file,encoding = \"utf-8\")\n",
    "    output = open(output_file,\"w\",encoding = \"utf-8\")\n",
    "    val_doc = [tokenize(x.strip(\"\\n\").split(\"\\t\")[0]) for x in val_doc.readlines()]\n",
    "    print(val_doc[0:5])\n",
    "    data_title = [[] for x  in range(len(val_doc))]\n",
    "    \n",
    "    for index,title in enumerate(val_doc):       \n",
    "            \n",
    "        for jndex,token in enumerate(title.split(\" \")):\n",
    "            if jndex>=max_length_title:\n",
    "                break\n",
    "            data_title[index].append(title_dict.get(token,len(title_dict)-1))\n",
    "        while len(data_title[index])<5:\n",
    "            data_title[index].append(len(title_dict)-1)        \n",
    "    batches = batch_iter(data_title, batch_size, 1)\n",
    "    for batch in batches:\n",
    "        batch_data_title = batch\n",
    "        pred = np.array(predict.eval({input_xt: batch_data_title}),dtype=np.int)\n",
    "        prob = np.array(probability.eval({input_xt: batch_data_title}),dtype=np.float32)\n",
    "        #gt = np.array(batch_data_label,dtype=np.int)\n",
    "        #confuse+=fast_hist(output,gt,num_labels)\n",
    "        for pre,pro in list(zip(pred,prob)):\n",
    "            output.write(\"\\t\".join([str(industry[int(pre)]),str(pro[0])])+\"\\n\")\n",
    "    output.close()\n",
    "    \n",
    "model_list =[\"model/180days_all_shuffled/cnn_acc0.850.dnn\",\"model/180days_editor_shuffled/\",\"model/linkedin_only/linkedin_acc0.770.dnn\"]\n",
    "suffix_list = [\"180days_all_shuffled\",\"180days_editor_shuffled\",\"linkedin_only\"]\n",
    "industry_list = [\"180days_all_shuffled\",\"180days_all_shuffled\",\"180days_all_shuffled\"]\n",
    "model = 0\n",
    "prefix = \"/home/t-haohu/IndustryClassifier/Data/\"\n",
    "for suffix,model_name,industry in list(zip(suffix_list,model_list,industry_list)):\n",
    "    #model_file = \"{}/{}\".format(prefix,industry)\n",
    "\n",
    "    if model_name  ==\"model/180days_editor_shuffled/\":\n",
    "        continue\n",
    "    model = load_model(model_name)\n",
    "    data_industry_sample = \"{}/ready/industry_{}.wl\".format(prefix,industry)\n",
    "    data_title_sample    = \"{}/ready/title_{}.wl\".format(prefix,suffix)\n",
    "    data_body_sample     = \"{}/ready/body_{}.wl\".format(prefix,suffix)\n",
    "    title_dict =     { x:i for i,x in enumerate([x.strip(\"\\n\") for x in open(data_title_sample).readlines()])}\n",
    "    inference(model,\"Data/middle/1day_measure_sample_valid.txt\",\"val/cnn_1day_measure_{}.txt\".format(suffix),title_dict,data_industry_sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([1]), list([1, 2])], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
