{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function # Use a function definition from future version (say 3.x from 2.7 interpreter)\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import time \n",
    "\n",
    "import cntk as C\n",
    "import cntk.tests.test_utils\n",
    "import pickle\n",
    "import random\n",
    "from cntk import sequence\n",
    "from cntk import load_model\n",
    "from cntk.device import try_set_default_device, gpu,cpu\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "cntk.tests.test_utils.set_device_from_pytest_env() # (only needed for our build system)\n",
    "C.cntk_py.set_fixed_random_seed(1) # fix a random seed for CNTK components\n",
    "try_set_default_device(gpu(0))\n",
    "\n",
    "\n",
    "vocab_size = 80000\n",
    "num_labels = 19#19\n",
    "title_size = 52000\n",
    "body_size  = 210000\n",
    "input_dim  = vocab_size\n",
    "label_dim  = num_labels\n",
    "emb_dim    = 300\n",
    "hidden_dim = 200\n",
    "\n",
    "max_length_title = 30\n",
    "max_length_body  = 100\n",
    "\n",
    "\n",
    "\n",
    "suffix = \"180days_editor_shuffled\"\n",
    "#suffix = \"linkedin_only\"\n",
    "prefix = \"/home/t-haohu/IndustryClassifier/Data/\"\n",
    "\n",
    "#data_token_body        = \"{}/middle/{}_token_body.txt\".format(prefix,suffix)\n",
    "data_train_sample = \"{}/middle/train_{}.txt\".format(prefix,suffix)\n",
    "#data_train_sample = \"{}/middle/train_{}_with_linkedin_all.txt\".format(prefix,suffix)\n",
    "data_test_sample  = \"{}/middle/test_{}.txt\".format(prefix,suffix)\n",
    "#data_test_sample_editor  = \"{}/middle/test_{}_editor.txt\".format(prefix,suffix)\n",
    "\n",
    "\n",
    "data_title_sample    = \"{}/ready/title_{}.wl\".format(prefix,suffix)\n",
    "data_body_sample     = \"{}/ready/body_{}.wl\".format(prefix,suffix)\n",
    "suffix = \"180days_all_shuffled\"\n",
    "data_industry_sample = \"{}/ready/industry_{}.wl\".format(prefix,suffix)\n",
    "filter_num=200 \n",
    "dropout_rate = 0.5\n",
    "emb_dim =300\n",
    "def load_data(input_file,title_dict,industry_dict):\n",
    "    data = open(input_file, encoding = \"utf-8\").readlines()\n",
    "    \n",
    "    data_title = np.zeros((len(data),max_length_title),dtype = np.float32)\n",
    "    data_label = np.zeros((len(data),1),dtype = np.float32)\n",
    "    \n",
    "    \n",
    "    for index,line in enumerate(data):\n",
    "        row = line.strip(\"\\n\").split(\"\\t\")       \n",
    "        title    =  row[0]\n",
    "        industry =  row[1]\n",
    "        \n",
    "        for jndex,token in enumerate(title.split(\" \")):\n",
    "            if jndex>=max_length_title:\n",
    "                break\n",
    "            data_title[index,jndex]=title_dict.get(token,len(title_dict)-1)    \n",
    "        data_label[index] = industry_dict.get(industry,len(industry_dict))\n",
    "    return data_title,data_label\n",
    "def load_embedding(title_file,embedding_model_file):\n",
    "    model = Word2Vec.load(embedding_model_file)\n",
    "    title_list = [x.strip(\"\\n\") for x in open(title_file,encoding = 'utf-8').readlines()]\n",
    "    embedding = np.zeros((len(title_list),emb_dim))\n",
    "    count = 0\n",
    "    for i,w in enumerate(title_list):\n",
    "        try:\n",
    "            vec = model.wv[w]\n",
    "        except:\n",
    "            vec=model.wv[\"UNK\"]\n",
    "            count+=1\n",
    "        embedding[i] =vec\n",
    "    print(count)\n",
    "    return embedding\n",
    "def create_model_cnn(embed = False):\n",
    "    #version 2 : 1 dense layer version3: sigmoid activation in dense\n",
    "    if embed:\n",
    "        h1= C.layers.Embedding(weights=embedding,name='embed_1')(input_xt_one_hot)#\n",
    "    else:\n",
    "        h1= C.layers.Embedding(emb_dim,name='embed_2')(input_xt_one_hot)#init=embedding,\n",
    "\n",
    "    \n",
    "\n",
    "    h2_1=C.layers.Convolution((1,emb_dim),num_filters=filter_num,reduction_rank=0,activation=C.relu)(h1)\n",
    "    h2_2=C.layers.Convolution((2,emb_dim),num_filters=filter_num,reduction_rank=0,activation=C.relu)(h1)\n",
    "    h2_3=C.layers.Convolution((3,emb_dim),num_filters=filter_num,reduction_rank=0,activation=C.relu)(h1)\n",
    "\n",
    "    h3_1=C.layers.MaxPooling((max_length_title-0,1),name='pooling_1')(h2_1)\n",
    "    h3_2=C.layers.MaxPooling((max_length_title-1,1),name='pooling_2')(h2_2)\n",
    "    h3_3=C.layers.MaxPooling((max_length_title-2,1),name='pooling_3')(h2_3)\n",
    "    #h2=BiRecurrence(C.layers.LSTM(hidden_dim), C.layers.LSTM(hidden_dim))(h1)\n",
    "    h3=C.splice(h3_2,h3_1,h3_3,axis=0)\n",
    "    drop1 = C.layers.Dropout(dropout_rate)(h3)\n",
    "    h4=C.layers.Dense(num_labels,name='hidden')(drop1)\n",
    "\n",
    "    return h4\n",
    "def create_model_cnn_2fold():\n",
    "    #version 2 : 1 dense layer version3: sigmoid activation in dense\n",
    "    #\n",
    "    with C.layers.default_options(initial_state=0.1):\n",
    "\n",
    "\n",
    "        h1_1= C.layers.Embedding(weights=embedding,name='embed_1')(input_xt_one_hot)#\n",
    "        h1_2= C.layers.Embedding(300,name='embed_2')(input_xt_one_hot)#init=embedding,\n",
    "        \n",
    "        \n",
    "        \n",
    "        h1_1_expand = C.expand_dims(h1_1,-3)\n",
    "        h1_2_expand = C.expand_dims(h1_2,-3)\n",
    "        \n",
    "        h1 = C.splice(h1_1_expand,h1_2_expand,axis = -3)\n",
    "        \n",
    "        #bn = C.layers.BatchNormalization(name='bn')(h1)\n",
    "        \n",
    "\n",
    "        #value,valid = to_static(h1)\n",
    "\n",
    "        filter_num=100\n",
    "\n",
    "        h2_1=C.layers.Convolution((3,emb_dim),num_filters=filter_num,reduction_rank=1,activation=C.relu)(h1)\n",
    "        h2_2=C.layers.Convolution((4,emb_dim),num_filters=filter_num,reduction_rank=1,activation=C.relu)(h1)\n",
    "        h2_3=C.layers.Convolution((5,emb_dim),num_filters=filter_num,reduction_rank=1,activation=C.relu)(h1)\n",
    "\n",
    "        h3_1=C.layers.MaxPooling((max_length_title-2,1),name='pooling_1')(h2_1)\n",
    "        h3_2=C.layers.MaxPooling((max_length_title-3,1),name='pooling_2')(h2_2)\n",
    "        h3_3=C.layers.MaxPooling((max_length_title-4,1),name='pooling_3')(h2_3)\n",
    "\n",
    "        h3=C.splice(h3_2,h3_1,h3_3,axis=0)\n",
    "        drop1 =C.layers.Dropout(0.5)(h3)\n",
    "        h4=C.layers.Dense(num_labels,name='hidden')(drop1)\n",
    "\n",
    "    return h4\n",
    "def batch_iter(data,batch_size, num_epochs, shuffle=True):\n",
    "    # Generates a batch iterator for a dataset.\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((data_size-1)/batch_size) + 1\n",
    "    print('data_size: ', data_size, 'batch_size: ', batch_size, 'num_batches_per_epoch: ', num_batches_per_epoch)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            random.shuffle(data)\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield data[start_index:end_index]\n",
    "            \n",
    "\n",
    "def fast_hist(a, b, n):\n",
    "    k = (a >= 0) & (a < n)\n",
    "    return np.bincount(n * a[k].astype(int) + b[k], minlength=n**2).reshape(n, n)\n",
    "\n",
    "title_dict =     { x:i for i,x in enumerate([x.strip(\"\\n\") for x in open(data_title_sample).readlines()])}\n",
    "industry_dict =  { x:i for i,x in enumerate([x.strip(\"\\n\") for x in open(data_industry_sample).readlines()])}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_xt = C.input_variable(shape=(max_length_title),  dtype=np.float32)\n",
    "input_y  = C.input_variable(shape=(1)               ,  dtype=np.int)\n",
    "\n",
    "input_xt_one_hot = C.one_hot(input_xt, num_classes=len(title_dict)   ,  sparse_output=True)\n",
    "input_y_one_hot = C.one_hot(input_y  , num_classes=len(industry_dict) ,  sparse_output=True)\n",
    "\n",
    "\n",
    "test_data  = load_data(data_test_sample,title_dict,industry_dict)\n",
    "train_data = load_data(data_train_sample,title_dict,industry_dict)\n",
    "#test_data_editor  = load_data(data_test_sample_editor,title_dict,industry_dict)\n",
    "embedding = load_embedding(data_title_sample,\"word2vec.model\")\n",
    "def test(batch_size,model,data):\n",
    "    scores = model(input_xt)\n",
    "    predict = C.argmax(scores,axis = 0)\n",
    "    confuse = np.zeros((num_labels,num_labels))\n",
    "\n",
    "    test_data_title,test_data_label = data\n",
    "    batches = batch_iter(list(zip(test_data_title,test_data_label)), batch_size, 1)\n",
    "    \n",
    "    for batch in batches:\n",
    "        batch_data_title,batch_data_label = zip(*batch) \n",
    "        output = np.array(predict.eval({input_xt: np.array(batch_data_title)}),dtype=np.int)\n",
    "        gt = np.array(batch_data_label,dtype=np.int)\n",
    "        confuse+=fast_hist(output,gt,num_labels)\n",
    "        \n",
    "    precision=np.diag(confuse)/np.sum(confuse,axis=0)\n",
    "    recall = np.diag(confuse)/np.sum(confuse,axis=1)\n",
    "    accuarcy = np.diag(confuse).sum() / confuse.sum()\n",
    "    aver_precision=np.nanmean(precision)\n",
    "    aver_recall = np.nanmean(recall)\n",
    "   \n",
    "    print(\"Precision:{} Recall:{} Acc:{}\".format(aver_precision,aver_recall,accuarcy))\n",
    "    return accuarcy\n",
    "def train(train_data,num_epochs,learning_rate,batch_size,tag=\"CNN\",l2_weight=0):\n",
    "    \n",
    "    #learning_rate *= batch_size\n",
    "    model = create_model_cnn_2fold()\n",
    "    print(C.logging.get_node_outputs(model))\n",
    "    scores = model(input_xt)\n",
    "\n",
    "    loss =C.reduce_mean(C.losses.cross_entropy_with_softmax(scores, input_y_one_hot))\n",
    "    \n",
    "    # Training\n",
    "    lr_schedule = C.learning_parameter_schedule(learning_rate)\n",
    "    #learner = C.adam(scores.parameters, lr=lr_schedule, momentum=0.9,l2_regularization_weight=0)\n",
    "    progress_printer = C.logging.ProgressPrinter(tag='Training', num_epochs=num_epochs)\n",
    "    momentums = C.momentum_schedule(0.99, minibatch_size=batch_size)\n",
    "    learner = C.adam(parameters=scores.parameters,#model.parameters,\n",
    "                     lr=lr_schedule,\n",
    "                     momentum=momentums,\n",
    "                     gradient_clipping_threshold_per_sample=15,\n",
    "                     gradient_clipping_with_truncation=True,\n",
    "                     l2_regularization_weight=l2_weight)\n",
    "    trainer = C.Trainer(scores, (loss), [learner], progress_printer)\n",
    "    \n",
    "    train_data_title,train_data_label = train_data\n",
    "    batches = batch_iter(list(zip(train_data_title,train_data_label)), batch_size, num_epochs)\n",
    "\n",
    "    # training loop\n",
    "    count = 0\n",
    "    t = time.time()\n",
    "    for batch in batches:\n",
    "        count += 1\n",
    "        batch_data_title,batch_data_label = zip(*batch)\n",
    "        \n",
    "        trainer.train_minibatch({input_xt: np.array(batch_data_title), input_y: np.array(batch_data_label)})\n",
    "        if count%400== 0:\n",
    "            print(count,time.time()-t)\n",
    "            t=time.time()\n",
    "            acc1=test(batch_size,model,test_data)\n",
    "            acc2=test(batch_size,model,test_data_editor)\n",
    "            \n",
    "            # save model\n",
    "            model.save('./model/{}/{}_acc{:.3f}.dnn'.format(suffix,tag,acc1))\n",
    "            #model.save('./model/{}/{}_acc1{:.3f}_acc2{:.3f}.dnn'.format(suffix,tag,acc1,acc2))\n",
    "    \n",
    "\n",
    "    # save model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Output('hidden', [#], [19]), Output('Block164253_Output_0', [#], [300 x 1 x 1]), Output('Splice164233_Output_0', [#], [300 x 1 x 1]), Output('pooling_2', [#], [100 x 1 x 1]), Output('Block163978_Output_0', [#], [100 x 27 x 1]), Output('Splice163876_Output_0', [#], [2 x 30 x 300]), Output('Block163862_Output_0', [#], [1 x 30 x 300]), Output('embed_1', [#], [30 x 300]), Output('OneHotOp150253_Output_0', [#], [30 x 14273]), Output('Block163871_Output_0', [#], [1 x 30 x 300]), Output('embed_2', [#], [30 x 300]), Output('pooling_1', [#], [100 x 1 x 1]), Output('Block163912_Output_0', [#], [100 x 28 x 1]), Output('pooling_3', [#], [100 x 1 x 1]), Output('Block164044_Output_0', [#], [100 x 26 x 1])]\n",
      "data_size:  70043 batch_size:  150 num_batches_per_epoch:  467\n",
      "Learning rate per minibatch: 0.003\n",
      "400 25.786607265472412\n",
      "data_size:  30019 batch_size:  150 num_batches_per_epoch:  201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t-haohu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:195: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/t-haohu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:196: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:0.13336121616514124 Recall:0.403117775730817 Acc:0.4946533861887471\n",
      "data_size:  30238 batch_size:  150 num_batches_per_epoch:  202\n",
      "Precision:0.052621635952864225 Recall:0.10390915377301409 Acc:0.40561545075732525\n",
      "800 29.4045250415802\n",
      "data_size:  30019 batch_size:  150 num_batches_per_epoch:  201\n",
      "Precision:0.1893119258084402 Recall:0.3457874704513575 Acc:0.5288983643692328\n",
      "data_size:  30238 batch_size:  150 num_batches_per_epoch:  202\n",
      "Precision:0.05268166333423283 Recall:0.14357492067705765 Acc:0.405218599113698\n",
      "1200 29.403366804122925\n",
      "data_size:  30019 batch_size:  150 num_batches_per_epoch:  201\n",
      "Precision:0.222621058690706 Recall:0.3763084025365839 Acc:0.5487191445417902\n",
      "data_size:  30238 batch_size:  150 num_batches_per_epoch:  202\n",
      "Precision:0.0528145991686969 Recall:0.15087333493953398 Acc:0.40482174747007077\n",
      "1600 30.06884455680847\n",
      "data_size:  30019 batch_size:  150 num_batches_per_epoch:  201\n",
      "Precision:0.2429929839619751 Recall:0.3750766705796104 Acc:0.5602784902894833\n",
      "data_size:  30238 batch_size:  150 num_batches_per_epoch:  202\n",
      "Precision:0.05279861275337829 Recall:0.09273772929730428 Acc:0.4036973344797936\n",
      "2000 29.672728061676025\n",
      "data_size:  30019 batch_size:  150 num_batches_per_epoch:  201\n",
      "Precision:0.2673970327299831 Recall:0.38274988205799454 Acc:0.5708717812052366\n",
      "data_size:  30238 batch_size:  150 num_batches_per_epoch:  202\n",
      "Precision:0.052795928224427635 Recall:0.07854326277924321 Acc:0.40260599245981876\n",
      "2400 30.3204026222229\n",
      "data_size:  30019 batch_size:  150 num_batches_per_epoch:  201\n",
      "Precision:0.2738297653169278 Recall:0.389041806456939 Acc:0.5778340384423198\n",
      "data_size:  30238 batch_size:  150 num_batches_per_epoch:  202\n",
      "Precision:0.05283836706207795 Recall:0.07945088020800782 Acc:0.402142998875587\n",
      "2800 29.860276699066162\n",
      "data_size:  30019 batch_size:  150 num_batches_per_epoch:  201\n",
      "Precision:0.2922257327176951 Recall:0.38073137942088087 Acc:0.5883940171224891\n",
      "data_size:  30238 batch_size:  150 num_batches_per_epoch:  202\n",
      "Precision:0.052840152526745864 Recall:0.07745622733641382 Acc:0.40138236655863485\n",
      "3200 29.50983738899231\n",
      "data_size:  30019 batch_size:  150 num_batches_per_epoch:  201\n",
      "Precision:0.30569485151928405 Recall:0.3831442750698886 Acc:0.5983876877977281\n",
      "data_size:  30238 batch_size:  150 num_batches_per_epoch:  202\n",
      "Precision:0.052929081195181964 Recall:0.0728852339567002 Acc:0.39996031483563727\n",
      "3600 29.9009690284729\n",
      "data_size:  30019 batch_size:  150 num_batches_per_epoch:  201\n",
      "Precision:0.31258761121287615 Recall:0.3900392334512585 Acc:0.6045837636163763\n",
      "data_size:  30238 batch_size:  150 num_batches_per_epoch:  202\n",
      "Precision:0.05310857659031075 Recall:0.0786264187640999 Acc:0.3994642502811033\n",
      "4000 29.26236343383789\n",
      "data_size:  30019 batch_size:  150 num_batches_per_epoch:  201\n",
      "Precision:0.3241569408470356 Recall:0.38837643794457644 Acc:0.6115127086178753\n",
      "data_size:  30238 batch_size:  150 num_batches_per_epoch:  202\n",
      "Precision:0.05339329798303919 Recall:0.08663859172720506 Acc:0.39807526952840794\n",
      "4400 29.68135929107666\n",
      "data_size:  30019 batch_size:  150 num_batches_per_epoch:  201\n",
      "Precision:0.33695122321214593 Recall:0.38195123054979657 Acc:0.6186415270328792\n",
      "data_size:  30238 batch_size:  150 num_batches_per_epoch:  202\n",
      "Precision:0.05336996540556038 Recall:0.09328068246935076 Acc:0.3951650241418083\n",
      "4800 29.36755633354187\n",
      "data_size:  30019 batch_size:  150 num_batches_per_epoch:  201\n",
      "Precision:0.34179051679778877 Recall:0.3829837280455719 Acc:0.6204736999900063\n",
      "data_size:  30238 batch_size:  150 num_batches_per_epoch:  202\n",
      "Precision:0.05325279760788088 Recall:0.09494140650372618 Acc:0.39420596600304253\n",
      "5200 30.07457685470581\n",
      "data_size:  30019 batch_size:  150 num_batches_per_epoch:  201\n",
      "Precision:0.344041747395932 Recall:0.3882793916369804 Acc:0.623205303307905\n",
      "data_size:  30238 batch_size:  150 num_batches_per_epoch:  202\n",
      "Precision:0.053166907610980405 Recall:0.09370295181999723 Acc:0.39397446921092666\n",
      "5600 30.063138723373413\n",
      "data_size:  30019 batch_size:  150 num_batches_per_epoch:  201\n",
      "Precision:0.3450313147601752 Recall:0.3943665299537051 Acc:0.62550384756321\n",
      "data_size:  30238 batch_size:  150 num_batches_per_epoch:  202\n",
      "Precision:0.053231022002077466 Recall:0.0997195038413834 Acc:0.39314769495336993\n",
      "6000 29.203492879867554\n",
      "data_size:  30019 batch_size:  150 num_batches_per_epoch:  201\n",
      "Precision:0.3530886236419013 Recall:0.38829972234512994 Acc:0.6281688264099403\n",
      "data_size:  30238 batch_size:  150 num_batches_per_epoch:  202\n",
      "Precision:0.05310705234223038 Recall:0.08745791440057618 Acc:0.39261855942853363\n",
      "6400 30.004876375198364\n",
      "data_size:  30019 batch_size:  150 num_batches_per_epoch:  201\n",
      "Precision:0.3548727506295792 Recall:0.3898521520804728 Acc:0.6287018221792865\n",
      "data_size:  30238 batch_size:  150 num_batches_per_epoch:  202\n",
      "Precision:0.053055292080612076 Recall:0.08501875135629028 Acc:0.39175871420067465\n",
      "6800 29.71999740600586\n",
      "data_size:  30019 batch_size:  150 num_batches_per_epoch:  201\n",
      "Precision:0.35423113422342273 Recall:0.3931757157359074 Acc:0.6288683833572071\n",
      "data_size:  30238 batch_size:  150 num_batches_per_epoch:  202\n",
      "Precision:0.05303125426946827 Recall:0.08820284276655546 Acc:0.3918909980818837\n",
      "7200 29.56564426422119\n",
      "data_size:  30019 batch_size:  150 num_batches_per_epoch:  201\n",
      "Precision:0.3536319581393468 Recall:0.3961033008656087 Acc:0.630400746194077\n",
      "data_size:  30238 batch_size:  150 num_batches_per_epoch:  202\n",
      "Precision:0.053071958783213115 Recall:0.08932021081332199 Acc:0.3916925722600701\n",
      "7600 29.528396606445312\n",
      "data_size:  30019 batch_size:  150 num_batches_per_epoch:  201\n",
      "Precision:0.3528356380749058 Recall:0.3953286097665306 Acc:0.6297345014823945\n",
      "data_size:  30238 batch_size:  150 num_batches_per_epoch:  202\n",
      "Precision:0.052967443758699516 Recall:0.08557179267247002 Acc:0.3916264303194656\n",
      "8000 30.873370885849\n",
      "data_size:  30019 batch_size:  150 num_batches_per_epoch:  201\n",
      "Precision:0.35035366045160543 Recall:0.3969467783012383 Acc:0.6292015057130484\n",
      "data_size:  30238 batch_size:  150 num_batches_per_epoch:  202\n",
      "Precision:0.052909987782017096 Recall:0.08625775894349356 Acc:0.3913949335273497\n",
      "8400 29.84715175628662\n",
      "data_size:  30019 batch_size:  150 num_batches_per_epoch:  201\n",
      "Precision:0.34768136790531134 Recall:0.39230034745698755 Acc:0.6274026449915053\n",
      "data_size:  30238 batch_size:  150 num_batches_per_epoch:  202\n",
      "Precision:0.052929447238055906 Recall:0.08606049221265838 Acc:0.3912626496461406\n",
      "8800 29.99545168876648\n",
      "data_size:  30019 batch_size:  150 num_batches_per_epoch:  201\n",
      "Precision:0.34992565175865614 Recall:0.3948637491915804 Acc:0.6285019487657817\n",
      "data_size:  30238 batch_size:  150 num_batches_per_epoch:  202\n",
      "Precision:0.05275014337430329 Recall:0.08428453719442425 Acc:0.39079965606190886\n",
      "9200 30.92284393310547\n",
      "data_size:  30019 batch_size:  150 num_batches_per_epoch:  201\n",
      "Precision:0.34085364192046846 Recall:0.3960021921834057 Acc:0.6265032146307339\n",
      "data_size:  30238 batch_size:  150 num_batches_per_epoch:  202\n",
      "Precision:0.0528536985963534 Recall:0.08603595126838072 Acc:0.39103115285402473\n"
     ]
    }
   ],
   "source": [
    "train(train_data,num_epochs=20,learning_rate=[2e-5*150]*2+[1e-4*150],batch_size = 150,tag = \"editor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cntk import load_model\n",
    "from data_processor import *\n",
    "processor_setting(lower=False,old = True,stop = False)\n",
    "def inference(model,val_doc_file,output_file,title_dict,industry_file):\n",
    "    \n",
    "    scores = model(input_xt)\n",
    "    predict = C.argmax(scores,axis = 0)\n",
    "    probability = C.max(C.softmax(scores),axis = 0)\n",
    "    \n",
    "    industry = [x.strip(\"\\n\") for x in open(industry_file,encoding =\"utf-8\").readlines()]\n",
    "    val_doc = open(val_doc_file,encoding = \"utf-8\")\n",
    "    output = open(output_file,\"w\",encoding = \"utf-8\")\n",
    "    val_doc = [tokenize(x.strip(\"\\n\")) for x in val_doc.readlines()]\n",
    "    \n",
    "    data_title = np.zeros((len(val_doc),max_length_title),dtype = np.float32)\n",
    "    \n",
    "    for index,title in enumerate(val_doc):       \n",
    "        for jndex,token in enumerate(title.split(\" \")):\n",
    "            if jndex>=max_length_title:\n",
    "                break\n",
    "            data_title[index,jndex]=title_dict.get(token,len(title_dict)-1)\n",
    "            \n",
    "    batches = batch_iter(data_title, batch_size, 1)\n",
    "    for batch in batches:\n",
    "        batch_data_title = batch\n",
    "        pred = np.array(predict.eval({input_xt: np.array(batch_data_title)}),dtype=np.int)\n",
    "        prob = np.array(probability.eval({input_xt: np.array(batch_data_title)}),dtype=np.float32)\n",
    "        #gt = np.array(batch_data_label,dtype=np.int)\n",
    "        #confuse+=fast_hist(output,gt,num_labels)\n",
    "        for pre,pro in list(zip(pred,prob)):\n",
    "            output.write(\"\\t\".joint([industry[pre],pro])+\"\\n\")\n",
    "    output.close()\n",
    "suffix_list =[\"180days_all_shuffled\",\"180days_editor_shuffled\"]\n",
    "model_list = [\"180days_all_shuffled\"]\n",
    "industry_list = [\"\"]\n",
    "prefix = \"/home/t-haohu/IndustryClassifier/Data/\"\n",
    "for suffix,model_name,indsutry in list(zip(suffix_list,model_list,industry_list)):\n",
    "    #model_file = \"{}/{}\".format(prefix,industry)\n",
    "    model = load_model(model_name)\n",
    "    data_industry_sample = \"{}/ready/title_{}.wl\".format(prefix,industry)\n",
    "    data_title_sample    = \"{}/ready/title_{}.wl\".format(prefix,suffix)\n",
    "    data_body_sample     = \"{}/ready/body_{}.wl\".format(prefix,suffix)\n",
    "    title_dict =     { x:i for i,x in enumerate([x.strip(\"\\n\") for x in open(data_title_sample).readlines()])}\n",
    "    inference(\"middle/1day_measure_sample_valid.txt\",\"val/cnn_1day_measure_{}.txt\".format(suffix),title_dict,data_industry_sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
