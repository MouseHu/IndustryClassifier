{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8633\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function # Use a function definition from future version (say 3.x from 2.7 interpreter)\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import time \n",
    "\n",
    "import cntk as C\n",
    "import cntk.tests.test_utils\n",
    "from cntk.layers import *\n",
    "from cntk.layers.typing import *\n",
    "import pickle\n",
    "import random\n",
    "from cntk import sequence\n",
    "from cntk import load_model\n",
    "from cntk.device import try_set_default_device, gpu,cpu\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "cntk.tests.test_utils.set_device_from_pytest_env() # (only needed for our build system)\n",
    "C.cntk_py.set_fixed_random_seed(1) # fix a random seed for CNTK components\n",
    "try_set_default_device(gpu(0))\n",
    "\n",
    "\n",
    "vocab_size = 80000\n",
    "num_labels = 19#19\n",
    "title_size = 52000\n",
    "body_size  = 210000\n",
    "input_dim  = vocab_size\n",
    "label_dim  = num_labels\n",
    "emb_dim    = 300\n",
    "hidden_dim = 200\n",
    "\n",
    "max_length_title = 30\n",
    "max_length_body  = 100\n",
    "\n",
    "\n",
    "\n",
    "suffix = \"180days_all_shuffled_with_linkedin\"\n",
    "#suffix = \"linkedin_only\"\n",
    "prefix = \"/home/t-haohu/IndustryClassifier/Data/\"\n",
    "\n",
    "\n",
    "data_train_sample = \"{}/middle/train_{}.txt\".format(prefix,suffix)\n",
    "#data_train_sample = \"{}/middle/train_{}_with_linkedin_all.txt\".format(prefix,suffix)\n",
    "suffix = \"180days_all_shuffled_with_linkedin\"\n",
    "data_test_sample  = \"{}/middle/test_{}.txt\".format(prefix,suffix)\n",
    "#data_test_sample_editor  = \"{}/middle/test_{}_editor.txt\".format(prefix,suffix)\n",
    "\n",
    "suffix = \"180days_all_shuffled_with_linkedin\"\n",
    "data_title_sample    = \"{}/ready/title_{}.wl\".format(prefix,suffix)\n",
    "data_body_sample     = \"{}/ready/body_{}.wl\".format(prefix,suffix)\n",
    "suffix = \"180days_all_shuffled\"\n",
    "data_industry_sample = \"{}/ready/industry_{}.wl\".format(prefix,suffix)\n",
    "suffix = \"180days_all_shuffled_with_linkedin\"\n",
    "filter_num=200 \n",
    "dropout_rate = 0.5\n",
    "emb_dim =300\n",
    "\n",
    "def load_data_dynamic(input_file,title_dict,industry_dict):\n",
    "    data = open(input_file, encoding = \"utf-8\").readlines()\n",
    "    \n",
    "    data_title =[ [] for x in range(len(data))]#np.zeros((len(data),max_length_title),dtype = np.float32)\n",
    "    data_label = np.zeros((len(data),1),dtype = np.float32)\n",
    "    \n",
    "    \n",
    "    for index,line in enumerate(data):\n",
    "        row = line.strip(\"\\n\").split(\"\\t\")       \n",
    "        title    =  row[0]\n",
    "        industry =  row[1]\n",
    "        \n",
    "        for jndex,token in enumerate(title.split(\" \")):\n",
    "            if jndex>=max_length_title:\n",
    "                break\n",
    "            data_title[index].append(title_dict.get(token,len(title_dict)-1))\n",
    "        while len(data_title[index])<5:\n",
    "            data_title[index].append(len(title_dict)-1)\n",
    "        data_label[index] = industry_dict.get(industry,len(industry_dict))\n",
    "    data_title = [ np.array(x) for x in data_title]\n",
    "    return data_title,data_label\n",
    "\n",
    "def load_data_static(input_file,title_dict,industry_dict):\n",
    "    data = open(input_file, encoding = \"utf-8\").readlines()\n",
    "    \n",
    "    data_title =np.zeros((len(data),max_length_title),dtype = np.float32)\n",
    "    data_label = np.zeros((len(data),1),dtype = np.float32)\n",
    "    \n",
    "    \n",
    "    for index,line in enumerate(data):\n",
    "        row = line.strip(\"\\n\").split(\"\\t\")       \n",
    "        title    =  row[0]\n",
    "        industry =  row[1]\n",
    "        \n",
    "        for jndex,token in enumerate(title.split(\" \")):\n",
    "            if jndex>=max_length_title:\n",
    "                break\n",
    "            data_title[index,jndex]=title_dict.get(token,len(title_dict)-1)    \n",
    "        data_label[index] = industry_dict.get(industry,len(industry_dict))\n",
    "    return data_title,data_label\n",
    "\n",
    "def load_embedding(title_file,embedding_model_file):\n",
    "    model = Word2Vec.load(embedding_model_file)\n",
    "    title_list = [x.strip(\"\\n\") for x in open(title_file,encoding = 'utf-8').readlines()]\n",
    "    embedding = np.zeros((len(title_list),emb_dim))\n",
    "    count = 0\n",
    "    for i,w in enumerate(title_list):\n",
    "        try:\n",
    "            vec = model.wv[w]\n",
    "        except:\n",
    "            vec=model.wv[\"UNK\"]\n",
    "            count+=1\n",
    "        embedding[i] =vec\n",
    "    print(count)\n",
    "    return embedding\n",
    "\n",
    "def create_model_cnn(embed = False):\n",
    "    #version 2 : 1 dense layer version3: sigmoid activation in dense\n",
    "    if embed:\n",
    "        h1= C.layers.Embedding(weights=embedding,name='embed_1')(input_xt_one_hot)#\n",
    "    else:\n",
    "        h1= C.layers.Embedding(emb_dim,name='embed_2')(input_xt_one_hot)#init=embedding,\n",
    "\n",
    "    \n",
    "\n",
    "    h2_1=C.layers.Convolution((1,emb_dim),num_filters=filter_num,reduction_rank=0,activation=C.relu)(h1)\n",
    "    h2_2=C.layers.Convolution((2,emb_dim),num_filters=filter_num,reduction_rank=0,activation=C.relu)(h1)\n",
    "    h2_3=C.layers.Convolution((3,emb_dim),num_filters=filter_num,reduction_rank=0,activation=C.relu)(h1)\n",
    "    \n",
    "    h3_1=C.layers.MaxPooling((max_length_title-0,1),name='pooling_1')(h2_1)\n",
    "    h3_2=C.layers.MaxPooling((max_length_title-1,1),name='pooling_2')(h2_2)\n",
    "    h3_3=C.layers.MaxPooling((max_length_title-2,1),name='pooling_3')(h2_3)\n",
    "    #h2=BiRecurrence(C.layers.LSTM(hidden_dim), C.layers.LSTM(hidden_dim))(h1)\n",
    "    h3=C.splice(h3_2,h3_1,h3_3,axis=0)\n",
    "    drop1 = C.layers.Dropout(dropout_rate)(h3)\n",
    "    h4=C.layers.Dense(300,name='hidden')(drop1)\n",
    "    h5=C.layers.Dense(num_labels,name='hidden')(h4)\n",
    " \n",
    "    return h5\n",
    "\n",
    "def create_model_cnn_dynamic(embed = False):\n",
    "    #version 2 : 1 dense layer version3: sigmoid activation in dense\n",
    "    if embed:\n",
    "        h1= C.layers.Embedding(weights=embedding,name='embed_1')(input_xt_one_hot)#\n",
    "    else:\n",
    "        h1= C.layers.Embedding(emb_dim,name='embed_2')(input_xt_one_hot)#init=embedding,\n",
    "\n",
    "    h1 = C.squeeze(h1)\n",
    "    print(h1)\n",
    "    h2_1=C.layers.Convolution((1,emb_dim),num_filters=filter_num,reduction_rank=0,activation=C.relu,sequential=True)(h1)\n",
    "    h2_2=C.layers.Convolution((2,emb_dim),num_filters=filter_num,reduction_rank=0,activation=C.relu,sequential=True)(h1)\n",
    "    h2_3=C.layers.Convolution((3,emb_dim),num_filters=filter_num,reduction_rank=0,activation=C.relu,sequential=True)(h1)\n",
    "    seq_MaxPooling = C.layers.Fold(C.element_max)\n",
    "    h3_1=seq_MaxPooling(h2_1)\n",
    "    h3_2=seq_MaxPooling(h2_2)\n",
    "    h3_3=seq_MaxPooling(h2_3)\n",
    "    #h2=BiRecurrence(C.layers.LSTM(hidden_dim), C.layers.LSTM(hidden_dim))(h1)\n",
    "    h3=C.splice(h3_2,h3_1,h3_3,axis=0)\n",
    "    drop1 = C.layers.Dropout(dropout_rate)(h3)\n",
    "    h4=C.layers.Dense(num_labels,name='hidden')(drop1)\n",
    "\n",
    "    return h4\n",
    "\n",
    "def create_model_cnn_2fold(dynamic = False):\n",
    "    #version 2 : 1 dense layer version3: sigmoid activation in dense\n",
    "    #\n",
    "    with C.layers.default_options(initial_state=0.1):\n",
    "\n",
    "\n",
    "        h1_1= C.layers.Embedding(weights=embedding,name='embed_1')(input_xt_one_hot)#\n",
    "        h1_2= C.layers.Embedding(300,name='embed_2')(input_xt_one_hot)#init=embedding,\n",
    "        \n",
    "        \n",
    "        \n",
    "        h1_1_expand = C.expand_dims(h1_1,-3)\n",
    "        h1_2_expand = C.expand_dims(h1_2,-3)\n",
    "        \n",
    "        h1 = C.splice(h1_1_expand,h1_2_expand,axis = -3)\n",
    "        \n",
    "        #bn = C.layers.BatchNormalization(name='bn')(h1)\n",
    "        \n",
    "\n",
    "        #value,valid = to_static(h1)\n",
    "\n",
    "        filter_num=100\n",
    "\n",
    "        h2_1=C.layers.Convolution((3,emb_dim),num_filters=filter_num,reduction_rank=1,activation=C.relu)(h1)\n",
    "        h2_2=C.layers.Convolution((4,emb_dim),num_filters=filter_num,reduction_rank=1,activation=C.relu)(h1)\n",
    "        h2_3=C.layers.Convolution((5,emb_dim),num_filters=filter_num,reduction_rank=1,activation=C.relu)(h1)\n",
    "        if dynamic:\n",
    "            seq_MaxPooling = C.layers.Fold(C.element_max)\n",
    "            h3_1=seq_MaxPooling(h2_1)\n",
    "            h3_2=seq_MaxPooling(h2_2)\n",
    "            h3_3=seq_MaxPooling(h2_3)\n",
    "        else:\n",
    "            h3_1=C.layers.MaxPooling((max_length_title-2,1),name='pooling_1')(h2_1)\n",
    "            h3_2=C.layers.MaxPooling((max_length_title-3,1),name='pooling_2')(h2_2)\n",
    "            h3_3=C.layers.MaxPooling((max_length_title-4,1),name='pooling_3')(h2_3)\n",
    "        \n",
    "        h3=C.splice(h3_2,h3_1,h3_3,axis=0)\n",
    "        drop1 =C.layers.Dropout(0.5)(h3)\n",
    "        h4=C.layers.Dense(num_labels,name='hidden')(drop1)\n",
    "\n",
    "    return h4\n",
    "\n",
    "def batch_iter(data,batch_size, num_epochs, shuffle=True):\n",
    "    # Generates a batch iterator for a dataset.\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((data_size-1)/batch_size) + 1\n",
    "    print('data_size: ', data_size, 'batch_size: ', batch_size, 'num_batches_per_epoch: ', num_batches_per_epoch)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            random.shuffle(data)\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield data[start_index:end_index]\n",
    "            \n",
    "\n",
    "def fast_hist(a, b, n):\n",
    "    k = (a >= 0) & (a < n)\n",
    "    return np.bincount(n * a[k].astype(int) + b[k], minlength=n**2).reshape(n, n)\n",
    "\n",
    "title_dict =     { x:i for i,x in enumerate([x.strip(\"\\n\") for x in open(data_title_sample).readlines()])}\n",
    "industry_dict =  { x:i for i,x in enumerate([x.strip(\"\\n\") for x in open(data_industry_sample).readlines()])}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#input_xt = C.input_variable(shape=(max_length_title),  dtype=np.float32)\n",
    "input_xt = C.input_variable(**Sequence[Tensor[1]])\n",
    "input_y  = C.input_variable(shape=(1))\n",
    "\n",
    "input_xt_one_hot = C.one_hot(input_xt, num_classes=len(title_dict)   ,  sparse_output=True)\n",
    "input_y_one_hot = C.one_hot(input_y  , num_classes=len(industry_dict) ,  sparse_output=True)\n",
    "\n",
    "\n",
    "test_data  = load_data_dynamic(data_test_sample,title_dict,industry_dict)\n",
    "train_data = load_data_dynamic(data_train_sample,title_dict,industry_dict)\n",
    "#test_data_editor  = load_data(data_test_sample_editor,title_dict,industry_dict)\n",
    "embedding = load_embedding(data_title_sample,\"word2vec.model\")\n",
    "\n",
    "\n",
    "def test(batch_size,model,data):\n",
    "    scores = model(input_xt)\n",
    "    predict = C.argmax(scores,axis = 0)\n",
    "    confuse = np.zeros((num_labels,num_labels))\n",
    "\n",
    "    test_data_title,test_data_label = data\n",
    "    batches = batch_iter(list(zip(test_data_title,test_data_label)), batch_size, 1)\n",
    "    \n",
    "    for batch in batches:\n",
    "        batch_data_title,batch_data_label = zip(*batch) \n",
    "        batch_data_title = [[[x] for x in y] for y in list(batch_data_title)] \n",
    "        batch_data_label = [x.tolist() for x in batch_data_label]\n",
    "        output = np.array(predict.eval({input_xt: batch_data_title}),dtype=np.int)\n",
    "        gt = np.array(batch_data_label,dtype=np.int)\n",
    "        confuse+=fast_hist(output,gt,num_labels)\n",
    "        \n",
    "    precision=np.diag(confuse)/np.sum(confuse,axis=0)\n",
    "    recall = np.diag(confuse)/np.sum(confuse,axis=1)\n",
    "    accuarcy = np.diag(confuse).sum() / confuse.sum()\n",
    "    aver_precision=np.nanmean(precision)\n",
    "    aver_recall = np.nanmean(recall)\n",
    "   \n",
    "    print(\"Precision:{} Recall:{} Acc:{}\".format(aver_precision,aver_recall,accuarcy))\n",
    "    return accuarcy\n",
    "\n",
    "\n",
    "\n",
    "def train(train_data,num_epochs,learning_rate,batch_size,tag=\"CNN\",l2_weight=0):\n",
    "    global model\n",
    "    #learning_rate *= batch_size\n",
    "    model = create_model_cnn_dynamic()\n",
    "    print(C.logging.get_node_outputs(model))\n",
    "    scores = model(input_xt)\n",
    "    loss =C.reduce_mean(C.losses.cross_entropy_with_softmax(scores, input_y_one_hot))\n",
    "    \n",
    "    # Training\n",
    "    lr_schedule = C.learning_parameter_schedule(learning_rate)\n",
    "    #learner = C.adam(scores.parameters, lr=lr_schedule, momentum=0.9,l2_regularization_weight=0)\n",
    "    progress_printer = C.logging.ProgressPrinter(tag='Training', num_epochs=num_epochs)\n",
    "    momentums = C.momentum_schedule(0.99, minibatch_size=batch_size)\n",
    "    learner = C.adam(parameters=scores.parameters,#model.parameters,\n",
    "                     lr=lr_schedule,\n",
    "                     momentum=momentums,\n",
    "                     gradient_clipping_threshold_per_sample=15,\n",
    "                     gradient_clipping_with_truncation=True,\n",
    "                     l2_regularization_weight=l2_weight)\n",
    "    trainer = C.Trainer(scores, (loss), [learner], progress_printer)\n",
    "    \n",
    "    train_data_title,train_data_label = train_data\n",
    "    batches = batch_iter(list(zip(train_data_title,train_data_label)), batch_size, num_epochs)\n",
    "\n",
    "    # training loop\n",
    "    count = 0\n",
    "    t = time.time()\n",
    "    for batch in batches:\n",
    "        count += 1\n",
    "        batch_data_title,batch_data_label = zip(*batch)\n",
    "        batch_data_title = [[[x] for x in y] for y in list(batch_data_title)]\n",
    "        batch_data_label = [x.tolist() for x in batch_data_label]\n",
    "        #print(batch_data_label[0])\n",
    "        trainer.train_minibatch({input_xt: batch_data_title, input_y: batch_data_label})\n",
    "        if count%200== 0:\n",
    "            print(count,time.time()-t)\n",
    "            t=time.time()\n",
    "            acc1=test(batch_size,model,test_data)\n",
    "            #acc2=test(batch_size,model,test_data_editor)\n",
    "\n",
    "            model.save('./model/{}/{}_acc{:.3f}.dnn'.format(suffix,tag,acc1))\n",
    "            #model.save('./model/{}/{}_acc1{:.3f}_acc2{:.3f}.dnn'.format(suffix,tag,acc1,acc2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composite(Sequence[Tensor[1]]) -> Sequence[Tensor[300]]\n",
      "[Output('hidden', [#], [19]), Output('Block3465_Output_0', [#], [600 x 1]), Output('Splice3445_Output_0', [#], [600 x 1]), Output('Block3219_Output_0', [#], [200 x 1]), Output('Block3192_Output_0', [#, defaultDynamicAxis_times_1_minus_1], [200 x 1]), Output('PastValue3176_Output_0', [#, defaultDynamicAxis_times_1_minus_1], [200 x 1]), Output('Block2562_Output_0', [#, defaultDynamicAxis_times_1_minus_1], [200 x 1]), Output('Squeeze2328_Output_0', [#, *], [300]), Output('embed_2', [#, *], [1 x 300]), Output('OneHotOp2294_Output_0', [#, *], [1 x 83110]), Output('Block3125_Output_0', [#], [200 x 1]), Output('Block3098_Output_0', [#, *], [200 x 1]), Output('PastValue3082_Output_0', [#, *], [200 x 1]), Output('Block2381_Output_0', [#, *], [200 x 1]), Output('Block3345_Output_0', [#], [200 x 1]), Output('Block3318_Output_0', [#, defaultDynamicAxis_times_1_minus_2], [200 x 1]), Output('PastValue3302_Output_0', [#, defaultDynamicAxis_times_1_minus_2], [200 x 1]), Output('Block2851_Output_0', [#, defaultDynamicAxis_times_1_minus_2], [200 x 1])]\n",
      "data_size:  2076724 batch_size:  150 num_batches_per_epoch:  13845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t-haohu/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/core.py:411: UserWarning: you provided the minibatch data as a list, but your corresponding input variable (uid \"Input2293\") has only one dynamic axis (batch axis). To speed up graph execution, please convert the data beforehand into one NumPy array to speed up training.\n",
      "  'training.' % var.uid)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate per minibatch: 0.075\n",
      "200 61.502437114715576\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t-haohu/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/ipykernel_launcher.py:260: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/t-haohu/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/ipykernel_launcher.py:261: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:0.1361307187925286 Recall:0.5643236335532786 Acc:0.38771719895508555\n",
      "400 272.73400807380676\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.35812596154097986 Recall:0.65881970251418 Acc:0.5608381787028455\n",
      "600 248.49844074249268\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.4907402781370913 Recall:0.7019090642211341 Acc:0.6516828178983737\n",
      "800 239.59740138053894\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.5583798503807208 Recall:0.7273635547544806 Acc:0.6836100109547485\n",
      "1000 296.1381106376648\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.5944641200932337 Recall:0.7275689259357517 Acc:0.6988668857616359\n",
      "1200 241.86806511878967\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.6164044229189977 Recall:0.7562862659970515 Acc:0.7092194039493273\n",
      "1400 295.40894532203674\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.6266008611363484 Recall:0.7547845755652806 Acc:0.7150911491250246\n",
      "1600 225.40976691246033\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.6442691801504672 Recall:0.7500245432067981 Acc:0.7210583972360327\n",
      "1800 286.2026026248932\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.661933790402998 Recall:0.7450692010144059 Acc:0.7254436673127159\n",
      "2000 247.96090722084045\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.672051645642459 Recall:0.7511590272920878 Acc:0.7265750962051628\n",
      "2200 258.368079662323\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.6849303121190687 Recall:0.7453015388584745 Acc:0.7339243279683155\n",
      "2400 284.7379262447357\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.687693700340718 Recall:0.7482248473645539 Acc:0.7358130389595798\n",
      "2600 237.79705667495728\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.6882894994175067 Recall:0.752046202011529 Acc:0.7362950478919131\n",
      "2800 308.0127820968628\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.690541314409778 Recall:0.756930169294131 Acc:0.7389916013595124\n",
      "3000 238.6064531803131\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.6975322721578876 Recall:0.7552225070189431 Acc:0.7414982725204348\n",
      "3200 293.1542127132416\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7033208381448559 Recall:0.7517512618788011 Acc:0.7435723715625966\n",
      "3400 230.09335160255432\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7003233719056171 Recall:0.7535086542989411 Acc:0.743125193112553\n",
      "3600 294.3801305294037\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.6976404012328814 Recall:0.7569572462317214 Acc:0.7421431982247689\n",
      "3800 258.341201543808\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.707156263263968 Recall:0.7545889280984884 Acc:0.7449667144181343\n",
      "4000 274.84271597862244\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7060075290971507 Recall:0.7596649066848544 Acc:0.7460981433105812\n",
      "4200 253.61475467681885\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7127443406407834 Recall:0.7524862278832742 Acc:0.74587005982978\n",
      "4400 144.98603129386902\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7154778700922156 Recall:0.7543311808161688 Acc:0.7494261397151766\n",
      "4600 247.98569250106812\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7101741420820809 Recall:0.7615106330992802 Acc:0.7498620825257718\n",
      "4800 241.7624294757843\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7142907709663282 Recall:0.7600997838566376 Acc:0.749523889778377\n",
      "5000 243.45755100250244\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7169064087096177 Recall:0.7604288102753513 Acc:0.7520462908345271\n",
      "5200 276.7936406135559\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7147467405476254 Recall:0.7610338917963579 Acc:0.7498721946012753\n",
      "5400 273.27027893066406\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7165904139987935 Recall:0.76148455280055 Acc:0.7536226510491278\n",
      "5600 290.2161889076233\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7134516468250726 Recall:0.7643366732577688 Acc:0.751683379680346\n",
      "5800 289.5356023311615\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.718731741021619 Recall:0.7630051193613638 Acc:0.753621527485183\n",
      "6000 254.6599247455597\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7178434567488154 Recall:0.7626068173479238 Acc:0.7532372686160501\n",
      "6200 248.93864846229553\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7127928636418609 Recall:0.7692672788627091 Acc:0.7525238055110811\n",
      "6400 272.4324014186859\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7112867177294345 Recall:0.7712056454374657 Acc:0.7524518974186118\n",
      "6600 271.0592484474182\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7213943759431207 Recall:0.7619227605399929 Acc:0.75471812589534\n",
      "6800 275.08868885040283\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7179202972374124 Recall:0.7648193563658924 Acc:0.7538260161231426\n",
      "7000 252.7080307006836\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7178713150379152 Recall:0.7640061750152982 Acc:0.7546990253082778\n",
      "7200 240.1276617050171\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7203477804304882 Recall:0.7650524353549321 Acc:0.7554956321451645\n",
      "7400 264.5718128681183\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7224098366312975 Recall:0.7619090375453572 Acc:0.754375438892166\n",
      "7600 264.4406313896179\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7181095073243452 Recall:0.7657147757761829 Acc:0.7551484508862111\n",
      "7800 253.50152969360352\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.725189881354014 Recall:0.7600453109063235 Acc:0.756285497598382\n",
      "8000 245.81965851783752\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7274337516718895 Recall:0.7607003949161361 Acc:0.7576124266172298\n",
      "8200 252.58795380592346\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7244707047254116 Recall:0.7622451253205597 Acc:0.7569079520238196\n",
      "8400 277.20568561553955\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7250903504469025 Recall:0.7627623796725176 Acc:0.7566270610376113\n",
      "8600 286.46044301986694\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7228016019445241 Recall:0.7666016984305203 Acc:0.7573461419623044\n",
      "8800 270.64033794403076\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7231631849872773 Recall:0.7653080268719864 Acc:0.7554518131513159\n",
      "9000 247.15388798713684\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7229326682601858 Recall:0.7663379098265837 Acc:0.7574045672874358\n",
      "9200 242.86351919174194\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7251370137934017 Recall:0.7639166281083888 Acc:0.7582618465773433\n",
      "9400 283.27470302581787\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7262692613241458 Recall:0.7653506156374824 Acc:0.7591314850706441\n",
      "9600 267.95144152641296\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7221451868856588 Recall:0.767428238561818 Acc:0.7573719839330356\n",
      "9800 248.4033980369568\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7234552878585863 Recall:0.7687248306435522 Acc:0.7597078733743434\n",
      "10000 238.9083776473999\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7231661013199409 Recall:0.7698125122502069 Acc:0.7576337743321817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10200 252.5757839679718\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7309455854856599 Recall:0.7671872454851915 Acc:0.7609134574871492\n",
      "10400 270.07917761802673\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7278627556655896 Recall:0.7641673689165552 Acc:0.7583202719024746\n",
      "10600 250.7258596420288\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7276227353263903 Recall:0.7670061374417999 Acc:0.7584258869132889\n",
      "10800 249.7187876701355\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7295713928723506 Recall:0.7670106246670241 Acc:0.7609100867953147\n",
      "11000 237.6287899017334\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7297974492781508 Recall:0.7655966735101581 Acc:0.759475295637763\n",
      "11200 277.68195104599\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7284239324396786 Recall:0.7663271521522507 Acc:0.7612909749726131\n",
      "11400 280.4107050895691\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7282631745521413 Recall:0.7690258656921076 Acc:0.7611404174040055\n",
      "11600 254.79433751106262\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7298589685144892 Recall:0.7655562845675767 Acc:0.7594809134574871\n",
      "11800 255.92048692703247\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7252194911224151 Recall:0.7724778205099794 Acc:0.7607707648661555\n",
      "12000 245.98081159591675\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7322442924335498 Recall:0.7643627672949371 Acc:0.7603629111541811\n",
      "12200 269.5143930912018\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7325094396906399 Recall:0.765730204757997 Acc:0.7613010870481166\n",
      "12400 276.99800181388855\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7335614050930598 Recall:0.7660484241011627 Acc:0.7624662228589084\n",
      "12600 292.1164207458496\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7342205508690826 Recall:0.7669487869690581 Acc:0.7633965338052302\n",
      "12800 289.56029987335205\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7261131093201972 Recall:0.772103096547504 Acc:0.761911182270161\n",
      "13000 247.95355463027954\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7302046555202963 Recall:0.7707982653317013 Acc:0.7622965647032387\n",
      "13200 244.55073857307434\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7280084552154562 Recall:0.7741653023869995 Acc:0.7613426589140755\n",
      "13400 274.6384289264679\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7267917265439595 Recall:0.7761526507887095 Acc:0.761029184573467\n",
      "13600 271.19079780578613\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7294368994379532 Recall:0.7726480328002532 Acc:0.7619359006769473\n",
      "13800 284.6631736755371\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.729798126109832 Recall:0.7745067731137558 Acc:0.7612550209263784\n",
      "14000 293.0343761444092\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7316685259581226 Recall:0.7707900042382252 Acc:0.7621696019774725\n",
      "14200 244.2293131351471\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7343115321847934 Recall:0.7670454237355111 Acc:0.7632605825679054\n",
      "14400 252.38700532913208\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7333940146149911 Recall:0.7667480716135078 Acc:0.762483076318081\n",
      "14600 267.24211287498474\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7273542112164924 Recall:0.7710063309497769 Acc:0.7599280919075306\n",
      "14800 255.4260711669922\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7278233323011474 Recall:0.7715709179667296 Acc:0.7610505322884189\n",
      "15000 254.59187841415405\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7292308378254871 Recall:0.7712014812177213 Acc:0.7610864863346535\n",
      "15200 251.86898565292358\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7359794024403068 Recall:0.7647783084390749 Acc:0.7617527597539395\n",
      "15400 271.7163369655609\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7328434256664744 Recall:0.7650415116285108 Acc:0.7596606836886605\n",
      "15600 269.13908863067627\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7305115484994896 Recall:0.7661967109101024 Acc:0.7606640262913963\n",
      "15800 268.243421792984\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.724033466954804 Recall:0.7699367157911267 Acc:0.7572854695092834\n",
      "16000 265.0872983932495\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7259295315773036 Recall:0.7702287626685165 Acc:0.758546108255386\n",
      "16200 238.6609275341034\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7267576130452634 Recall:0.7698612977439002 Acc:0.7592101345467824\n",
      "16400 262.2644474506378\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7306692094760966 Recall:0.7677863260517991 Acc:0.7607437993314794\n",
      "16600 258.6729910373688\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7311864464267666 Recall:0.7683365645165309 Acc:0.7603471812589534\n",
      "16800 257.33339500427246\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.729475141612693 Recall:0.7674730593882719 Acc:0.7604820089323333\n",
      "17000 235.77661895751953\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7295183533298207 Recall:0.7670949589828807 Acc:0.7609505350973287\n",
      "17200 257.53838992118835\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7333180165679211 Recall:0.7636436437384633 Acc:0.7604887503160024\n",
      "17400 265.4842002391815\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7356056165975399 Recall:0.7635733590365729 Acc:0.7620314036122581\n",
      "17600 250.6069643497467\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7292406148334517 Recall:0.7669939464323231 Acc:0.7607999775287211\n",
      "17800 242.8139123916626\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7309863451905009 Recall:0.7677148838644527 Acc:0.7610100839864049\n",
      "18000 252.13052606582642\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7325457612921227 Recall:0.7679721977021927 Acc:0.7617741074688913\n",
      "18200 268.3519287109375\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7299143799913688 Recall:0.7701911327963624 Acc:0.7616066964411112\n",
      "18400 251.27944493293762\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7314152519033273 Recall:0.7685292001048145 Acc:0.760208982893739\n",
      "18600 249.92795944213867\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7250167504746203 Recall:0.7752314255972214 Acc:0.7610707564394259\n",
      "18800 238.25336623191833\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7285679934207462 Recall:0.7720364030887212 Acc:0.7609786241959495\n",
      "19000 265.4913902282715\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7336042371479691 Recall:0.7697571115883259 Acc:0.7621347714951827\n",
      "19200 259.32977747917175\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.731123190825439 Recall:0.7704918340604274 Acc:0.7618426448695261\n",
      "19400 264.72295212745667\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7328519225719755 Recall:0.7671247530556717 Acc:0.762650487345861\n",
      "19600 239.4403257369995\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7266061961640201 Recall:0.774778073880589 Acc:0.7611022162298812\n",
      "19800 254.64090037345886\n",
      "data_size:  890025 batch_size:  150 num_batches_per_epoch:  5934\n",
      "Precision:0.7353087051358937 Recall:0.7685426936221053 Acc:0.7639908991320469\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/logging/progress_print.py\u001b[0m in \u001b[0;36mon_training_update_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m___write_progress_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregate_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregate_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mon_training_update_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0;31m# Override for ProgressWriter.on_training_update_end.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m___generate_progress_heartbeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "SWIG director method error.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b648726443b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5e-4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cnn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-4b8fc0ae6a16>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_data, num_epochs, learning_rate, batch_size, tag, l2_weight)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0mbatch_data_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_data_label\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;31m#print(batch_data_label[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minput_xt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_data_title\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_y\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_data_label\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/train/trainer.py\u001b[0m in \u001b[0;36mtrain_minibatch\u001b[0;34m(self, arguments, outputs, device, is_sweep_end)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 updated = super(Trainer, self).train_minibatch(arguments, is_sweep_end,\n\u001b[0;32m--> 184\u001b[0;31m                     device)\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/cntk_py.py\u001b[0m in \u001b[0;36mtrain_minibatch\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3027\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_cntk_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer_train_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3029\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: SWIG director method error."
     ]
    }
   ],
   "source": [
    "train(train_data,num_epochs=20,learning_rate=[5e-4*150]*2+[1e-4*150],batch_size = 150,tag = \"cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pro Russian Rebel Leader Assassinated in East Ukraine', 'EU to recommend abolishing daylight saving time', 'Trump cancels pay raises for federal employees', 'Contradictions Mount As Lawyer For Colorado Climate Suits Undermines Own Case', 'These Sassy Pyrex Storage Containers Are Perfect For Cooks With A Sense Of Humor']\n",
      "data_size:  361 batch_size:  20 num_batches_per_epoch:  19\n",
      "['Pro Russian Rebel Leader Assassinated in East Ukraine', 'EU to recommend abolishing daylight saving time', 'Trump cancels pay raises for federal employees', 'Contradictions Mount As Lawyer For Colorado Climate Suits Undermines Own Case', 'These Sassy Pyrex Storage Containers Are Perfect For Cooks With A Sense Of Humor']\n",
      "data_size:  361 batch_size:  20 num_batches_per_epoch:  19\n",
      "['Pro Russian Rebel Leader Assassinated in East Ukraine', 'EU to recommend abolishing daylight saving time', 'Trump cancels pay raises for federal employees', 'Contradictions Mount As Lawyer For Colorado Climate Suits Undermines Own Case', 'These Sassy Pyrex Storage Containers Are Perfect For Cooks With A Sense Of Humor']\n",
      "data_size:  361 batch_size:  20 num_batches_per_epoch:  19\n"
     ]
    }
   ],
   "source": [
    "from cntk import load_model\n",
    "from data_processor import *\n",
    "process_setting(low=False,old = True,stop = False)\n",
    "input_xt = C.input_variable(**Sequence[Tensor[1]])\n",
    "batch_size = 20\n",
    "def inference(model,val_doc_file,output_file,title_dict,industry_file):\n",
    "    \n",
    "    scores = model(input_xt)\n",
    "    predict = C.argmax(scores,axis = 0)\n",
    "    probability = C.reduce_max(C.softmax(scores),axis = 0)\n",
    "    \n",
    "    industry = [x.strip(\"\\n\") for x in open(industry_file,encoding =\"utf-8\").readlines()]\n",
    "    val_doc = open(val_doc_file,encoding = \"utf-8\")\n",
    "    output = open(output_file,\"w\",encoding = \"utf-8\")\n",
    "    val_doc = [tokenize(x.strip(\"\\n\").split(\"\\t\")[0]) for x in val_doc.readlines()]\n",
    "    print(val_doc[0:5])\n",
    "    data_title = [[] for x  in range(len(val_doc))]\n",
    "    \n",
    "    for index,title in enumerate(val_doc):       \n",
    "            \n",
    "        for jndex,token in enumerate(title.split(\" \")):\n",
    "            if jndex>=max_length_title:\n",
    "                break\n",
    "            data_title[index].append(title_dict.get(token,len(title_dict)-1))\n",
    "        while len(data_title[index])<5:\n",
    "            data_title[index].append(len(title_dict)-1)        \n",
    "    batches = batch_iter(data_title, batch_size, 1,shuffle =False)\n",
    "    for batch in batches:\n",
    "        batch_data_title = batch\n",
    "        pred = np.array(predict.eval({input_xt: batch_data_title}),dtype=np.int)\n",
    "        prob = np.array(probability.eval({input_xt: batch_data_title}),dtype=np.float32)\n",
    "        #gt = np.array(batch_data_label,dtype=np.int)\n",
    "        #confuse+=fast_hist(output,gt,num_labels)\n",
    "        for pre,pro in list(zip(pred,prob)):\n",
    "            output.write(\"\\t\".join([str(industry[int(pre)]),str(pro[0])])+\"\\n\")\n",
    "    output.close()\n",
    "    \n",
    "model_list =[\"model/180days_all_shuffled_with_linkedin/cnn_acc0.764.dnn\",\"model/180days_editor_shuffled/cnn_editor_acc0.617.dnn\",\"model/linkedin_only/linkedin_acc0.770.dnn\"]\n",
    "suffix_list = [\"180days_all_shuffled_with_linkedin\",\"180days_editor_shuffled\",\"linkedin_only\"]\n",
    "industry_list = [\"180days_all_shuffled\",\"180days_all_shuffled\",\"180days_all_shuffled\"]\n",
    "model = 0\n",
    "prefix = \"/home/t-haohu/IndustryClassifier/Data/\"\n",
    "for suffix,model_name,industry in list(zip(suffix_list,model_list,industry_list)):\n",
    "    #model_file = \"{}/{}\".format(prefix,industry)\n",
    "\n",
    "    #if model_name  ==\"model/180days_editor_shuffled/\":\n",
    "    #    continue\n",
    "    model = load_model(model_name)\n",
    "    data_industry_sample = \"{}/ready/industry_{}.wl\".format(prefix,industry)\n",
    "    data_title_sample    = \"{}/ready/title_{}.wl\".format(prefix,suffix)\n",
    "    data_body_sample     = \"{}/ready/body_{}.wl\".format(prefix,suffix)\n",
    "    title_dict =     { x:i for i,x in enumerate([x.strip(\"\\n\") for x in open(data_title_sample).readlines()])}\n",
    "    inference(model,\"Data/middle/1day_measure_sample_valid.txt\",\"val/cnn_1day_measure_{}.txt\".format(suffix),title_dict,data_industry_sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([1]), list([1, 2])], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
