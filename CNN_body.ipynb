{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function # Use a function definition from future version (say 3.x from 2.7 interpreter)\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import time \n",
    "\n",
    "import cntk as C\n",
    "import cntk.tests.test_utils\n",
    "import pickle\n",
    "import random\n",
    "from cntk import sequence\n",
    "from cntk import load_model\n",
    "from cntk.device import try_set_default_device, gpu,cpu\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "cntk.tests.test_utils.set_device_from_pytest_env() # (only needed for our build system)\n",
    "C.cntk_py.set_fixed_random_seed(1) # fix a random seed for CNTK components\n",
    "try_set_default_device(gpu(0))\n",
    "\n",
    "\n",
    "vocab_size = 80000\n",
    "num_labels = 19#\n",
    "title_size = 52000\n",
    "body_size  = 210000\n",
    "input_dim  = vocab_size\n",
    "label_dim  = num_labels\n",
    "emb_dim    = 300\n",
    "hidden_dim = 200\n",
    "\n",
    "max_length_title = 30\n",
    "max_length_body  = 100\n",
    "\n",
    "\n",
    "\n",
    "suffix = \"180days_all_body_shuffled\"\n",
    "prefix = \"/home/t-haohu/IndustryClassifier/Data/\"\n",
    "\n",
    "data_token_body        = \"{}/middle/{}_token_body.txt\".format(prefix,suffix)\n",
    "data_train_sample_body = \"{}/middle/train_{}.txt\".format(prefix,suffix)\n",
    "data_test_sample_body  = \"{}/middle/test_{}.txt\".format(prefix,suffix)\n",
    "data_test_sample_body_editor  = \"{}/middle/test_{}_editor.txt\".format(prefix,suffix)\n",
    "\n",
    "data_title_sample    = \"{}/ready/title_{}.wl\".format(prefix,suffix)\n",
    "#suffix = \"180days_editor_body\"\n",
    "data_body_sample     = \"{}/ready/body_{}.wl\".format(prefix,suffix)\n",
    "suffix = \"180days_all_body_shuffled\"\n",
    "data_industry_sample = \"{}/ready/industry_{}.wl\".format(prefix,suffix)\n",
    "suffix = \"180days_all_body_shuffled\"\n",
    "\n",
    "def load_data_body(input_file,title_dict,body_dict,industry_dict):\n",
    "    data = open(input_file, encoding = \"utf-8\").readlines()\n",
    "    \n",
    "    data_title = np.zeros((len(data),max_length_title),dtype = np.float32)\n",
    "    data_body  = np.zeros((len(data),max_length_body),dtype = np.float32)\n",
    "    data_label = np.zeros((len(data),1),dtype = np.float32)\n",
    "    \n",
    "    \n",
    "    for index,line in enumerate(data):\n",
    "        row = line.strip(\"\\n\").split(\"\\t\")\n",
    "        \n",
    "        title    =  row[0]\n",
    "        body     =  row[1]\n",
    "        industry =  row[2]\n",
    "        \n",
    "        for jndex,token in enumerate(title.split(\" \")):\n",
    "            if jndex>=max_length_title:\n",
    "                break\n",
    "            data_title[index,jndex]=title_dict.get(token,len(title_dict)-1)\n",
    "            \n",
    "        for jndex,token in enumerate(body.split(\" \")):\n",
    "            if jndex>=max_length_body:\n",
    "                break\n",
    "            data_body[index,jndex]=body_dict.get(token,len(title_dict)-1)\n",
    "            \n",
    "        data_label[index] = industry_dict.get(industry,len(industry_dict))\n",
    "    return data_title,data_body,data_label\n",
    "\n",
    "def create_model_cnn_with_body():\n",
    "    \n",
    "    h1t= C.layers.Embedding(300,name='embed')(input_xt_one_hot)#init=embedding,\n",
    "    h1b= C.layers.Embedding(300,name='embed')(input_xb_one_hot)#init=embedding,\n",
    "    \n",
    "    #bnb = C.layers.BatchNormalization(name='bn')(h1b)\n",
    "    #bnt = C.layers.BatchNormalization(name='bn')(h1t)\n",
    "\n",
    "\n",
    "\n",
    "    h2_1t=C.layers.Convolution((1,emb_dim),num_filters=50,reduction_rank=0,activation=C.relu)(h1t)\n",
    "    h2_2t=C.layers.Convolution((2,emb_dim),num_filters=50,reduction_rank=0,activation=C.relu)(h1t)\n",
    "    h2_3t=C.layers.Convolution((3,emb_dim),num_filters=50,reduction_rank=0,activation=C.relu)(h1t)\n",
    "\n",
    "    h2_1b=C.layers.Convolution((1,emb_dim),num_filters=50,reduction_rank=0,activation=C.relu)(h1b)\n",
    "    h2_2b=C.layers.Convolution((2,emb_dim),num_filters=50,reduction_rank=0,activation=C.relu)(h1b)\n",
    "    h2_3b=C.layers.Convolution((3,emb_dim),num_filters=50,reduction_rank=0,activation=C.relu)(h1b)\n",
    "\n",
    "    h3_2t=C.layers.MaxPooling((max_length_title-1,1),name='pooling_t_1')(h2_2t)\n",
    "    h3_1t=C.layers.MaxPooling((max_length_title-0,1),name='pooling_t_2')(h2_1t)\n",
    "    h3_3t=C.layers.MaxPooling((max_length_title-2,1),name='pooling_t_3')(h2_3t)\n",
    "\n",
    "    h3_2b=C.layers.MaxPooling((max_length_body-1,1),name='pooling_b_1')(h2_2b)\n",
    "    h3_1b=C.layers.MaxPooling((max_length_body-0,1),name='pooling_b_2')(h2_1b)\n",
    "    h3_3b=C.layers.MaxPooling((max_length_body-2,1),name='pooling_b_3')(h2_3b)\n",
    "\n",
    "    h3=C.splice(h3_2t,h3_1t,h3_3t,h3_2b,h3_1b,h3_3b,axis=0)\n",
    "\n",
    "    #h4=C.layers.Dense(hidden_dim, activation=C.relu,name='hidden')(h3)\n",
    "    #drop1 = C.layers.Dropout(0.5,name='drop1')(h3)\n",
    "\n",
    "    h4=C.layers.Dense(num_labels,name='classify')(h3)\n",
    "\n",
    "    return h4\n",
    "\n",
    "def batch_iter(data,batch_size, num_epochs, shuffle=True):\n",
    "    # Generates a batch iterator for a dataset.\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((data_size-1)/batch_size) + 1\n",
    "    print('data_size: ', data_size, 'batch_size: ', batch_size, 'num_batches_per_epoch: ', num_batches_per_epoch)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            random.shuffle(data)\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield data[start_index:end_index]\n",
    "            \n",
    "\n",
    "def fast_hist(a, b, n):\n",
    "    k = (a >= 0) & (a < n)\n",
    "    return np.bincount(n * a[k].astype(int) + b[k], minlength=n**2).reshape(n, n)\n",
    "\n",
    "title_dict =     { x:i for i,x in enumerate([x.strip(\"\\n\") for x in open(data_title_sample).readlines()])}\n",
    "body_dict  =     { x:i for i,x in enumerate([x.strip(\"\\n\") for x in open(data_body_sample ).readlines()])}\n",
    "industry_dict =  { x:i for i,x in enumerate([x.strip(\"\\n\") for x in open(data_industry_sample).readlines()])}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_xt = C.input_variable(shape=(max_length_title),  dtype=np.float32)\n",
    "input_xb = C.input_variable(shape=(max_length_body) ,  dtype=np.float32)\n",
    "input_y  = C.input_variable(shape=(1)               ,  dtype=np.int)\n",
    "\n",
    "input_xt_one_hot = C.one_hot(input_xt, num_classes=len(title_dict)   ,  sparse_output=True)\n",
    "input_xb_one_hot = C.one_hot(input_xb, num_classes=len(body_dict)    ,  sparse_output=True) \n",
    "input_y_one_hot = C.one_hot(input_y  , num_classes=len(industry_dict) ,  sparse_output=True)\n",
    "\n",
    "#test_data_title, test_data_body, test_data_label  = load_data_body(data_test_sample_body,title_dict,body_dict,industry_dict)\n",
    "#train_data_title,train_data_body,train_data_label = load_data_body(data_train_sample_body,title_dict,body_dict,industry_dict)\n",
    "\n",
    "test_data  = load_data_body(data_test_sample_body,title_dict,body_dict,industry_dict)\n",
    "train_data = load_data_body(data_train_sample_body,title_dict,body_dict,industry_dict)\n",
    "#test_data_editor  = load_data_body(data_test_sample_body_editor,title_dict,body_dict,industry_dict)\n",
    "\n",
    "\n",
    "def test_body(batch_size,model,data):\n",
    "    scores = model(input_xt,input_xb)\n",
    "    predict = C.argmax(scores,axis = 0)\n",
    "    confuse = np.zeros((num_labels,num_labels))\n",
    "    #C.element_add(input_y,C.element_times(predict,C.Constant([nums_labels])))\n",
    "    test_data_title,test_data_body,test_data_label = data\n",
    "    batches = batch_iter(list(zip(test_data_title,test_data_body,test_data_label)), batch_size, 1)\n",
    "    \n",
    "    for batch in batches:\n",
    "        batch_data_title,batch_data_body,batch_data_label = zip(*batch) \n",
    "        output = np.array(predict.eval({input_xb: np.array(batch_data_body),input_xt: np.array(batch_data_title)}),dtype=np.int)\n",
    "        gt = np.array(batch_data_label,dtype=np.int)\n",
    "        confuse+=fast_hist(output,gt,num_labels)\n",
    "    precision=np.diag(confuse)/np.sum(confuse,axis=0)\n",
    "    recall = np.diag(confuse)/np.sum(confuse,axis=1)\n",
    "    accuarcy = np.diag(confuse).sum() / confuse.sum()\n",
    "    aver_precision=np.nanmean(precision)\n",
    "    aver_recall = np.nanmean(recall)\n",
    "   \n",
    "    print(\"Precision:{} Recall:{} Acc:{}\".format(aver_precision,aver_recall,accuarcy))\n",
    "    return accuarcy\n",
    "\n",
    "\n",
    "def train_body(train_data,num_epochs,learning_rate,batch_size,l2_weight=0,tag = \"cnn\"):\n",
    "    \n",
    "    #learning_rate *= batch_size\n",
    "    model = create_model_cnn_with_body()\n",
    "    print(C.logging.get_node_outputs(model))\n",
    "    scores = model(input_xt,input_xb)\n",
    "\n",
    "    loss =C.reduce_mean(C.losses.cross_entropy_with_softmax(scores, input_y_one_hot))\n",
    "    \n",
    "    # Training\n",
    "    lr_schedule = C.learning_parameter_schedule(learning_rate)\n",
    "    #learner = C.adam(scores.parameters, lr=lr_schedule, momentum=0.9,l2_regularization_weight=0)\n",
    "    progress_printer = C.logging.ProgressPrinter(tag='Training', num_epochs=num_epochs)\n",
    "    momentums = C.momentum_schedule(0.99, minibatch_size=batch_size)\n",
    "    learner = C.adam(parameters=scores.parameters,#model.parameters,\n",
    "                     lr=lr_schedule,\n",
    "                     momentum=momentums,\n",
    "                     gradient_clipping_threshold_per_sample=15,\n",
    "                     gradient_clipping_with_truncation=True,\n",
    "                     l2_regularization_weight=l2_weight)\n",
    "    trainer = C.Trainer(scores, (loss), [learner], progress_printer)\n",
    "    \n",
    "    train_data_title,train_data_body,train_data_label = train_data\n",
    "    batches = batch_iter(list(zip(train_data_title,train_data_body,train_data_label)), batch_size, num_epochs)\n",
    "\n",
    "    # training loop\n",
    "    count = 0\n",
    "    t = time.time()\n",
    "    for batch in batches:\n",
    "        count += 1\n",
    "        batch_data_title,batch_data_body,batch_data_label = zip(*batch)\n",
    "        #print(np.array(batch_data_body).shape)\n",
    "        #batch_data_title,batch_data_body,batch_data_label = transfer(batch_data_title),transfer(batch_data_body),trainsfer(batch_data_label)\n",
    "        trainer.train_minibatch({input_xb: np.array(batch_data_body),input_xt: np.array(batch_data_title), input_y: np.array(batch_data_label)})\n",
    "        if count%1000== 0:\n",
    "            print(count,time.time()-t)\n",
    "            t=time.time()\n",
    "            acc1=test_body(batch_size,model,test_data)\n",
    "            #acc2=test_body(batch_size,model,test_data_editor)\n",
    "            #\n",
    "            # save model\n",
    "            model.save('/home/t-haohu/IndustryClassifier/model/{}/{}_acc{:.3f}.dnn'.format(suffix,tag,acc1))\n",
    "            #model.save('model/{}/{}_acc{:.3f}.dnn'.format(suffix,tag,acc1))\n",
    "            #print(a)\n",
    "            #model.save('./model/{}/{}_acc1{:.3f}_acc2{:.3f}.dnn'.format(suffix,tag,acc1,acc2))\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Output('classify', [#], [19]), Output('Splice24409_Output_0', [#], [300 x 1 x 1]), Output('pooling_t_1', [#], [50 x 1 x 1]), Output('Block23888_Output_0', [#], [50 x 29 x 1]), Output('embed', [#], [30 x 300]), Output('OneHotOp6_Output_0', [#], [30 x 56957]), Output('pooling_t_2', [#], [50 x 1 x 1]), Output('Block23825_Output_0', [#], [50 x 30 x 1]), Output('pooling_t_3', [#], [50 x 1 x 1]), Output('Block23951_Output_0', [#], [50 x 28 x 1]), Output('pooling_b_1', [#], [50 x 1 x 1]), Output('Block24077_Output_0', [#], [50 x 99 x 1]), Output('embed', [#], [100 x 300]), Output('OneHotOp9_Output_0', [#], [100 x 216866]), Output('pooling_b_2', [#], [50 x 1 x 1]), Output('Block24014_Output_0', [#], [50 x 100 x 1]), Output('pooling_b_3', [#], [50 x 1 x 1]), Output('Block24140_Output_0', [#], [50 x 98 x 1])]\n",
      "data_size:  794568 batch_size:  42 num_batches_per_epoch:  18919\n",
      "Learning rate per minibatch: 0.015\n",
      "1000 66.29022669792175\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t-haohu/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/ipykernel_launcher.py:170: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:0.4651162133469066 Recall:0.5985301594538266 Acc:0.6691892050626964\n",
      "2000 103.44389820098877\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n",
      "Precision:0.6234585165892195 Recall:0.7241720452288346 Acc:0.7620239039144863\n",
      "3000 104.94570660591125\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n",
      "Precision:0.6836173329492755 Recall:0.7602900654199338 Acc:0.7940240213784395\n",
      "4000 111.98880100250244\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n",
      "Precision:0.7150679523703309 Recall:0.7863165273573159 Acc:0.8120224356150706\n",
      "5000 95.66679191589355\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n",
      "Precision:0.7458572645120001 Recall:0.7860156230404663 Acc:0.8233077849235017\n",
      "6000 114.84154319763184\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n",
      "Precision:0.7569115025827928 Recall:0.7958914394082294 Acc:0.8296302822071476\n",
      "7000 106.75340270996094\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n",
      "Precision:0.7695136600087902 Recall:0.8014346732080715 Acc:0.8364549378909347\n",
      "8000 101.3631284236908\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n",
      "Precision:0.7706986235069337 Recall:0.8104588370116775 Acc:0.8396059084368485\n",
      "9000 113.03623270988464\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n",
      "Precision:0.779203087554207 Recall:0.8111740871210786 Acc:0.8441693830205855\n",
      "10000 98.86571931838989\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n",
      "Precision:0.7822284722178134 Recall:0.8161755038886637 Acc:0.8461280944410184\n",
      "11000 109.23811268806458\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n",
      "Precision:0.7892097374953736 Recall:0.8145842317514279 Acc:0.8494846269051185\n",
      "12000 105.13877654075623\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n",
      "Precision:0.7973065361873763 Recall:0.8159735127806218 Acc:0.8514257187325639\n",
      "13000 101.87098360061646\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n",
      "Precision:0.7963035568122816 Recall:0.8190104967083025 Acc:0.8532464100079288\n",
      "14000 113.56151294708252\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n",
      "Precision:0.798922640026737 Recall:0.823006777495463 Acc:0.8544827181158782\n",
      "15000 99.45783185958862\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n",
      "Precision:0.8001466378649936 Recall:0.8258267386962806 Acc:0.8565500836930667\n",
      "16000 110.51389479637146\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n",
      "Precision:0.8044094860559102 Recall:0.8247040640451524 Acc:0.8575514638945174\n",
      "17000 111.25332188606262\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n",
      "Precision:0.8042832270187956 Recall:0.8264091827220957 Acc:0.8584001409567439\n",
      "18000 97.67756199836731\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n",
      "Precision:0.8054974578872052 Recall:0.8308567082602291 Acc:0.8608433911843303\n",
      "19000 114.29289507865906\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n",
      "Precision:0.8109348158011556 Recall:0.8290512671466852 Acc:0.8614865063283704\n",
      "20000 103.15852570533752\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n",
      "Precision:0.8101402554042589 Recall:0.8344330374632108 Acc:0.8635127595219217\n",
      "21000 104.75390100479126\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n",
      "Precision:0.8146358941295747 Recall:0.8276020346576125 Acc:0.8626817020526826\n",
      "22000 113.51263546943665\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n",
      "Precision:0.8057695475008451 Recall:0.8296570277719733 Acc:0.8572460576160691\n",
      "23000 97.18480563163757\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n",
      "Precision:0.8069413583983897 Recall:0.8315246160285091 Acc:0.8603588523771768\n",
      "24000 114.02553987503052\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n",
      "Precision:0.8062460865329336 Recall:0.8321847070620015 Acc:0.860449886940945\n",
      "25000 108.62871670722961\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n",
      "Precision:0.8076040286918491 Recall:0.8343240522656857 Acc:0.8619093765600682\n",
      "26000 101.74319815635681\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n",
      "Precision:0.8072525349292636 Recall:0.8331094380704407 Acc:0.8598948697618418\n",
      "27000 114.6523048877716\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n",
      "Precision:0.8023738679155583 Recall:0.8391775294226561 Acc:0.8617361172290253\n",
      "28000 102.36828804016113\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n",
      "Precision:0.8151851358861288 Recall:0.8250814177072849 Acc:0.8613220567938213\n",
      "29000 110.17836046218872\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n",
      "Precision:0.812087176747056 Recall:0.8341567573582903 Acc:0.8644671541420726\n",
      "30000 113.69041681289673\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n",
      "Precision:0.8134302122493863 Recall:0.8313971173961197 Acc:0.8628285319942443\n",
      "31000 96.98633098602295\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n",
      "Precision:0.8126684029534269 Recall:0.8313612045735063 Acc:0.8615334919096702\n",
      "32000 113.85125613212585\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n",
      "Precision:0.81503273651739 Recall:0.8303370395347601 Acc:0.8635832378938714\n",
      "33000 103.23146390914917\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n",
      "Precision:0.8160307538695415 Recall:0.8331769921652123 Acc:0.8648812145772766\n",
      "34000 106.57873678207397\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n",
      "Precision:0.8157951294093053 Recall:0.8308007051765298 Acc:0.8620532699027986\n",
      "35000 110.69703602790833\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n",
      "Precision:0.8139544586214852 Recall:0.8339970025022163 Acc:0.86358030129504\n",
      "36000 96.36395740509033\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n",
      "Precision:0.8185548076202224 Recall:0.8354174592756748 Acc:0.866834052800047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method Value.<lambda> of Value([42 x 1], GPU)>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/t-haohu/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/cntk_py.py\", line 1777, in <lambda>\n",
      "    __del__ = lambda self: None\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37000 114.32347249984741\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n",
      "Precision:0.8149875499773598 Recall:0.8368324759387032 Acc:0.8652835286171556\n",
      "38000 102.15995144844055\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n",
      "Precision:0.8187253977631672 Recall:0.8356653058245088 Acc:0.8674918509382433\n",
      "39000 105.33269238471985\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n",
      "Precision:0.820519144114271 Recall:0.8327234873486874 Acc:0.8663906263765307\n",
      "40000 112.35302400588989\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n",
      "Precision:0.8157872936077338 Recall:0.8355590098062721 Acc:0.8661703814641882\n",
      "41000 95.79505705833435\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n",
      "Precision:0.8241130019469498 Recall:0.8247843544405228 Acc:0.8639473761489443\n",
      "42000 112.683265209198\n",
      "data_size:  340530 batch_size:  42 num_batches_per_epoch:  8108\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5f10ffe7ef51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5e-4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-fc41c2e0bd8f>\u001b[0m in \u001b[0;36mtrain_body\u001b[0;34m(train_data, num_epochs, learning_rate, batch_size, l2_weight, tag)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0macc1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;31m#acc2=test_body(batch_size,model,test_data_editor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-fc41c2e0bd8f>\u001b[0m in \u001b[0;36mtest_body\u001b[0;34m(batch_size, model, data)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mbatch_data_title\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_data_body\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_data_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minput_xb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_xt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data_title\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0mgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mconfuse\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mfast_hist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/ops/functions.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, arguments, outputs, device, as_numpy)\u001b[0m\n\u001b[1;32m    706\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_numpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msanitize_variable_value_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/internal/swig_helper.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mmap_if_possible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/ops/functions.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, arguments, outputs, keep_for_backward, device, as_numpy)\u001b[0m\n\u001b[1;32m    843\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mas_numpy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m                 \u001b[0moutput_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_value_as_sequence_or_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/internal/__init__.py\u001b[0m in \u001b[0;36m_value_as_sequence_or_array\u001b[0;34m(val, var)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mmap_if_possible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0m_serialization_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/tensor.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mndav\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/cntk_py.py\u001b[0m in \u001b[0;36mto_ndarray\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_cntk_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNDArrayView_to_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_newclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0mrandom_uniform_float\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_cntk_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNDArrayView_random_uniform_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_body(train_data,num_epochs=20,learning_rate=[5e-4*30]*2+[1e-4*30],batch_size = 42,tag = \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361 361\n",
      "30 100\n",
      "[56415. 55945. 42479. 54306. 56956. 56953. 56566. 49771.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.] [213428. 216865. 216865. 216024. 216861. 215294. 214429. 179513. 216861.\n",
      " 216865. 215749. 203252. 185580. 215449. 216860. 212890. 216851. 215706.\n",
      " 216856. 216639. 216859. 216862. 211545. 216852. 216862. 205199. 216859.\n",
      " 214417. 185580. 216854. 216140. 216465. 214594. 216861. 216865. 166641.\n",
      " 194662. 215723. 216865. 216860. 215909. 216467. 216855. 215559. 216864.\n",
      " 216852. 216516. 216823. 216790. 202565. 216601. 216140. 216859. 216837.\n",
      " 211050. 216853. 207806. 216862. 209269. 215765. 216854. 198014. 216865.\n",
      " 216317. 216859. 216862. 216637. 216762. 212872. 214913. 215294. 202910.\n",
      " 214764. 216859. 213495. 210014. 194425. 162844. 216596. 192315. 212409.\n",
      " 216677. 215952. 216863. 210014. 216803. 216865. 210977. 216861. 216862.\n",
      " 215899. 216857. 216842. 213208. 216850. 216862. 216411. 175155. 216859.\n",
      " 216865.]\n",
      "data_size:  361 batch_size:  32 num_batches_per_epoch:  12\n"
     ]
    }
   ],
   "source": [
    "from cntk import load_model\n",
    "from data_processor import *\n",
    "process_setting(low=False,old = True,stop = False)\n",
    "batch_size  = 32\n",
    "def inference_body(model,val_doc_file,output_file,title_dict,body_dict,industry_file):\n",
    "    \n",
    "    scores = model(input_xt,input_xb)\n",
    "    predict = C.argmax(scores,axis = 0)\n",
    "    probability = C.reduce_max(C.softmax(scores),axis = 0)\n",
    "    \n",
    "    industry = [x.strip(\"\\n\") for x in open(industry_file,encoding =\"utf-8\").readlines()]\n",
    "    val_doc = open(val_doc_file,encoding = \"utf-8\").readlines()\n",
    "    output = open(output_file,\"w\",encoding = \"utf-8\")\n",
    "    val_title = [tokenize(x.strip(\"\\n\").split(\"\\t\")[0]) for x in val_doc]\n",
    "    val_body = [tokenize(x.strip(\"\\n\").split(\"\\t\")[1]) for x in val_doc]\n",
    "    data_title = np.zeros((len(val_title),max_length_title),dtype = np.float32)\n",
    "    data_body= np.zeros((len(val_body),max_length_body),dtype = np.float32)\n",
    "    for index,title in enumerate(val_title):       \n",
    "        for jndex,token in enumerate(title.split(\" \")):\n",
    "            if jndex>=max_length_title:\n",
    "                break\n",
    "            data_title[index,jndex]=title_dict.get(token,len(title_dict)-1)\n",
    "    for index,body in enumerate(val_body):       \n",
    "        for jndex,token in enumerate(body.split(\" \")):\n",
    "            if jndex>=max_length_body:\n",
    "                break\n",
    "            data_body[index,jndex]=body_dict.get(token,len(body_dict)-1)\n",
    "    print(len(data_title),len(data_body))  \n",
    "    print(len(data_title[0]),len(data_body[0]))  \n",
    "    print(data_title[0],data_body[0])\n",
    "    batches = batch_iter(list(zip(data_title,data_body)), batch_size, 1,shuffle=False)\n",
    "    for batch in batches:\n",
    "       #print(batch)\n",
    "        batch_data_title,batch_data_body = zip(*batch)\n",
    "        pred = np.array(predict.eval({input_xt: np.array(batch_data_title),input_xb: np.array(batch_data_body)}))\n",
    "        prob = np.array(probability.eval({input_xt: np.array(batch_data_title),input_xb: np.array(batch_data_body)}),dtype=np.float32)\n",
    "        #gt = np.array(batch_data_label,dtype=np.int)\n",
    "        #confuse+=fast_hist(output,gt,num_labels)\n",
    "        for pre,pro in list(zip(pred,prob)):\n",
    "            \n",
    "            output.write(\"\\t\".join([str(industry[int(pre)]),str(pro[0])])+\"\\n\")\n",
    "    output.close()\n",
    "    \n",
    "model_list =[\"model/180days_all_body_shuffled/_acc0.867.dnn\"]#,\"180days_editor_shuffled\"]\n",
    "suffix_list = [\"180days_all_body_shuffled\"]\n",
    "industry_list = [\"180days_all_body_shuffled\"]#,\"180days_all_shuffled\"]\n",
    "prefix = \"/home/t-haohu/IndustryClassifier/Data/\"\n",
    "for suffix,model_name,industry in list(zip(suffix_list,model_list,industry_list)):\n",
    "    model = load_model(model_name)\n",
    "    data_industry_sample = \"{}/ready/industry_{}.wl\".format(prefix,industry)\n",
    "    data_title_sample    = \"{}/ready/title_{}.wl\".format(prefix,suffix)\n",
    "    data_body_sample     = \"{}/ready/body_{}.wl\".format(prefix,suffix)\n",
    "    title_dict =     { x:i for i,x in enumerate([x.strip(\"\\n\") for x in open(data_title_sample).readlines()])}\n",
    "    body_dict =     { x:i for i,x in enumerate([x.strip(\"\\n\") for x in open(data_body_sample).readlines()])}\n",
    "    inference_body(model,\"Data/middle/1day_measure_sample_valid_body.txt\",\"val/1day_measure_{}.txt\".format(suffix),title_dict,body_dict,data_industry_sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
