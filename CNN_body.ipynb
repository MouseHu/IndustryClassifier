{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function # Use a function definition from future version (say 3.x from 2.7 interpreter)\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import time \n",
    "\n",
    "import cntk as C\n",
    "import cntk.tests.test_utils\n",
    "import pickle\n",
    "import random\n",
    "from cntk import sequence\n",
    "from cntk import load_model\n",
    "from cntk.device import try_set_default_device, gpu,cpu\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "cntk.tests.test_utils.set_device_from_pytest_env() # (only needed for our build system)\n",
    "C.cntk_py.set_fixed_random_seed(1) # fix a random seed for CNTK components\n",
    "try_set_default_device(gpu(0))\n",
    "\n",
    "\n",
    "vocab_size = 80000\n",
    "num_labels = 19#\n",
    "title_size = 52000\n",
    "body_size  = 210000\n",
    "input_dim  = vocab_size\n",
    "label_dim  = num_labels\n",
    "emb_dim    = 300\n",
    "hidden_dim = 200\n",
    "\n",
    "max_length_title = 30\n",
    "max_length_body  = 100\n",
    "\n",
    "\n",
    "\n",
    "suffix = \"180days_all_body_shuffled\"\n",
    "prefix = \"/home/t-haohu/IndustryClassifier/Data/\"\n",
    "\n",
    "data_token_body        = \"{}/middle/{}_token_body.txt\".format(prefix,suffix)\n",
    "data_train_sample_body = \"{}/middle/train_{}.txt\".format(prefix,suffix)\n",
    "data_test_sample_body  = \"{}/middle/test_{}.txt\".format(prefix,suffix)\n",
    "data_test_sample_body_editor  = \"{}/middle/test_{}_editor.txt\".format(prefix,suffix)\n",
    "\n",
    "data_title_sample    = \"{}/ready/title_{}.wl\".format(prefix,suffix)\n",
    "#suffix = \"180days_editor_body\"\n",
    "data_body_sample     = \"{}/ready/body_{}.wl\".format(prefix,suffix)\n",
    "suffix = \"180days_all_body_shuffled\"\n",
    "data_industry_sample = \"{}/ready/industry_{}.wl\".format(prefix,suffix)\n",
    "suffix = \"180days_all_body_shuffled\"\n",
    "\n",
    "def load_data_body(input_file,title_dict,body_dict,industry_dict):\n",
    "    data = open(input_file, encoding = \"utf-8\").readlines()\n",
    "    \n",
    "    data_title = np.zeros((len(data),max_length_title),dtype = np.float32)\n",
    "    data_body  = np.zeros((len(data),max_length_body),dtype = np.float32)\n",
    "    data_label = np.zeros((len(data),1),dtype = np.float32)\n",
    "    \n",
    "    \n",
    "    for index,line in enumerate(data):\n",
    "        row = line.strip(\"\\n\").split(\"\\t\")\n",
    "        \n",
    "        title    =  row[0]\n",
    "        body     =  row[1]\n",
    "        industry =  row[2]\n",
    "        \n",
    "        for jndex,token in enumerate(title.split(\" \")):\n",
    "            if jndex>=max_length_title:\n",
    "                break\n",
    "            data_title[index,jndex]=title_dict.get(token,len(title_dict)-1)\n",
    "            \n",
    "        for jndex,token in enumerate(body.split(\" \")):\n",
    "            if jndex>=max_length_body:\n",
    "                break\n",
    "            data_body[index,jndex]=body_dict.get(token,len(title_dict)-1)\n",
    "            \n",
    "        data_label[index] = industry_dict.get(industry,len(industry_dict))\n",
    "    return data_title,data_body,data_label\n",
    "\n",
    "def create_model_cnn_with_body():\n",
    "    \n",
    "    h1t= C.layers.Embedding(300,name='embed')(input_xt_one_hot)#init=embedding,\n",
    "    h1b= C.layers.Embedding(300,name='embed')(input_xb_one_hot)#init=embedding,\n",
    "    \n",
    "    #bnb = C.layers.BatchNormalization(name='bn')(h1b)\n",
    "    #bnt = C.layers.BatchNormalization(name='bn')(h1t)\n",
    "\n",
    "\n",
    "\n",
    "    h2_1t=C.layers.Convolution((1,emb_dim),num_filters=50,reduction_rank=0,activation=C.relu)(h1t)\n",
    "    h2_2t=C.layers.Convolution((2,emb_dim),num_filters=50,reduction_rank=0,activation=C.relu)(h1t)\n",
    "    h2_3t=C.layers.Convolution((3,emb_dim),num_filters=50,reduction_rank=0,activation=C.relu)(h1t)\n",
    "\n",
    "    h2_1b=C.layers.Convolution((1,emb_dim),num_filters=50,reduction_rank=0,activation=C.relu)(h1b)\n",
    "    h2_2b=C.layers.Convolution((2,emb_dim),num_filters=50,reduction_rank=0,activation=C.relu)(h1b)\n",
    "    h2_3b=C.layers.Convolution((3,emb_dim),num_filters=50,reduction_rank=0,activation=C.relu)(h1b)\n",
    "\n",
    "    h3_2t=C.layers.MaxPooling((max_length_title-1,1),name='pooling_t_1')(h2_2t)\n",
    "    h3_1t=C.layers.MaxPooling((max_length_title-0,1),name='pooling_t_2')(h2_1t)\n",
    "    h3_3t=C.layers.MaxPooling((max_length_title-2,1),name='pooling_t_3')(h2_3t)\n",
    "\n",
    "    h3_2b=C.layers.MaxPooling((max_length_body-1,1),name='pooling_b_1')(h2_2b)\n",
    "    h3_1b=C.layers.MaxPooling((max_length_body-0,1),name='pooling_b_2')(h2_1b)\n",
    "    h3_3b=C.layers.MaxPooling((max_length_body-2,1),name='pooling_b_3')(h2_3b)\n",
    "\n",
    "    h3=C.splice(h3_2t,h3_1t,h3_3t,h3_2b,h3_1b,h3_3b,axis=0)\n",
    "\n",
    "    #h4=C.layers.Dense(hidden_dim, activation=C.relu,name='hidden')(h3)\n",
    "    #drop1 = C.layers.Dropout(0.5,name='drop1')(h3)\n",
    "\n",
    "    h4=C.layers.Dense(num_labels,name='classify')(h3)\n",
    "\n",
    "    return h4\n",
    "\n",
    "def batch_iter(data,batch_size, num_epochs, shuffle=True):\n",
    "    # Generates a batch iterator for a dataset.\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((data_size-1)/batch_size) + 1\n",
    "    print('data_size: ', data_size, 'batch_size: ', batch_size, 'num_batches_per_epoch: ', num_batches_per_epoch)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            random.shuffle(data)\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield data[start_index:end_index]\n",
    "            \n",
    "\n",
    "def fast_hist(a, b, n):\n",
    "    k = (a >= 0) & (a < n)\n",
    "    return np.bincount(n * a[k].astype(int) + b[k], minlength=n**2).reshape(n, n)\n",
    "\n",
    "title_dict =     { x:i for i,x in enumerate([x.strip(\"\\n\") for x in open(data_title_sample).readlines()])}\n",
    "body_dict  =     { x:i for i,x in enumerate([x.strip(\"\\n\") for x in open(data_body_sample ).readlines()])}\n",
    "industry_dict =  { x:i for i,x in enumerate([x.strip(\"\\n\") for x in open(data_industry_sample).readlines()])}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_xt = C.input_variable(shape=(max_length_title),  dtype=np.float32)\n",
    "input_xb = C.input_variable(shape=(max_length_body) ,  dtype=np.float32)\n",
    "input_y  = C.input_variable(shape=(1)               ,  dtype=np.int)\n",
    "\n",
    "input_xt_one_hot = C.one_hot(input_xt, num_classes=len(title_dict)   ,  sparse_output=True)\n",
    "input_xb_one_hot = C.one_hot(input_xb, num_classes=len(body_dict)    ,  sparse_output=True) \n",
    "input_y_one_hot = C.one_hot(input_y  , num_classes=len(industry_dict) ,  sparse_output=True)\n",
    "\n",
    "#test_data_title, test_data_body, test_data_label  = load_data_body(data_test_sample_body,title_dict,body_dict,industry_dict)\n",
    "#train_data_title,train_data_body,train_data_label = load_data_body(data_train_sample_body,title_dict,body_dict,industry_dict)\n",
    "\n",
    "test_data  = load_data_body(data_test_sample_body,title_dict,body_dict,industry_dict)\n",
    "train_data = load_data_body(data_train_sample_body,title_dict,body_dict,industry_dict)\n",
    "#test_data_editor  = load_data_body(data_test_sample_body_editor,title_dict,body_dict,industry_dict)\n",
    "\n",
    "\n",
    "def test_body(batch_size,model,data):\n",
    "    scores = model(input_xt,input_xb)\n",
    "    predict = C.argmax(scores,axis = 0)\n",
    "    confuse = np.zeros((num_labels,num_labels))\n",
    "    #C.element_add(input_y,C.element_times(predict,C.Constant([nums_labels])))\n",
    "    test_data_title,test_data_body,test_data_label = data\n",
    "    batches = batch_iter(list(zip(test_data_title,test_data_body,test_data_label)), batch_size, 1)\n",
    "    \n",
    "    for batch in batches:\n",
    "        batch_data_title,batch_data_body,batch_data_label = zip(*batch) \n",
    "        output = np.array(predict.eval({input_xb: np.array(batch_data_body),input_xt: np.array(batch_data_title)}),dtype=np.int)\n",
    "        gt = np.array(batch_data_label,dtype=np.int)\n",
    "        confuse+=fast_hist(output,gt,num_labels)\n",
    "    precision=np.diag(confuse)/np.sum(confuse,axis=0)\n",
    "    recall = np.diag(confuse)/np.sum(confuse,axis=1)\n",
    "    accuarcy = np.diag(confuse).sum() / confuse.sum()\n",
    "    aver_precision=np.nanmean(precision)\n",
    "    aver_recall = np.nanmean(recall)\n",
    "   \n",
    "    print(\"Precision:{} Recall:{} Acc:{}\".format(aver_precision,aver_recall,accuarcy))\n",
    "    return accuarcy\n",
    "\n",
    "\n",
    "def train_body(train_data,num_epochs,learning_rate,batch_size,l2_weight=0,tag = \"cnn\"):\n",
    "    \n",
    "    #learning_rate *= batch_size\n",
    "    model = create_model_cnn_with_body()\n",
    "    print(C.logging.get_node_outputs(model))\n",
    "    scores = model(input_xt,input_xb)\n",
    "\n",
    "    loss =C.reduce_mean(C.losses.cross_entropy_with_softmax(scores, input_y_one_hot))\n",
    "    \n",
    "    # Training\n",
    "    lr_schedule = C.learning_parameter_schedule(learning_rate)\n",
    "    #learner = C.adam(scores.parameters, lr=lr_schedule, momentum=0.9,l2_regularization_weight=0)\n",
    "    progress_printer = C.logging.ProgressPrinter(tag='Training', num_epochs=num_epochs)\n",
    "    momentums = C.momentum_schedule(0.99, minibatch_size=batch_size)\n",
    "    learner = C.adam(parameters=scores.parameters,#model.parameters,\n",
    "                     lr=lr_schedule,\n",
    "                     momentum=momentums,\n",
    "                     gradient_clipping_threshold_per_sample=15,\n",
    "                     gradient_clipping_with_truncation=True,\n",
    "                     l2_regularization_weight=l2_weight)\n",
    "    trainer = C.Trainer(scores, (loss), [learner], progress_printer)\n",
    "    \n",
    "    train_data_title,train_data_body,train_data_label = train_data\n",
    "    batches = batch_iter(list(zip(train_data_title,train_data_body,train_data_label)), batch_size, num_epochs)\n",
    "\n",
    "    # training loop\n",
    "    count = 0\n",
    "    t = time.time()\n",
    "    for batch in batches:\n",
    "        count += 1\n",
    "        batch_data_title,batch_data_body,batch_data_label = zip(*batch)\n",
    "        #print(np.array(batch_data_body).shape)\n",
    "        #batch_data_title,batch_data_body,batch_data_label = transfer(batch_data_title),transfer(batch_data_body),trainsfer(batch_data_label)\n",
    "        trainer.train_minibatch({input_xb: np.array(batch_data_body),input_xt: np.array(batch_data_title), input_y: np.array(batch_data_label)})\n",
    "        if count%1000== 0:\n",
    "            print(count,time.time()-t)\n",
    "            t=time.time()\n",
    "            acc1=test_body(batch_size,model,test_data)\n",
    "            #acc2=test_body(batch_size,model,test_data_editor)\n",
    "            #\n",
    "            # save model\n",
    "            model.save('/home/t-haohu/IndustryClassifier/model/{}/{}_acc{:.3f}.dnn'.format(suffix,tag,acc1))\n",
    "            #model.save('model/{}/{}_acc{:.3f}.dnn'.format(suffix,tag,acc1))\n",
    "            #print(a)\n",
    "            #model.save('./model/{}/{}_acc1{:.3f}_acc2{:.3f}.dnn'.format(suffix,tag,acc1,acc2))\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Output('classify', [#], [19]), Output('Splice32457_Output_0', [#], [300 x 1 x 1]), Output('pooling_t_1', [#], [50 x 1 x 1]), Output('Block31936_Output_0', [#], [50 x 29 x 1]), Output('embed', [#], [30 x 300]), Output('OneHotOp31768_Output_0', [#], [30 x 14387]), Output('pooling_t_2', [#], [50 x 1 x 1]), Output('Block31873_Output_0', [#], [50 x 30 x 1]), Output('pooling_t_3', [#], [50 x 1 x 1]), Output('Block31999_Output_0', [#], [50 x 28 x 1]), Output('pooling_b_1', [#], [50 x 1 x 1]), Output('Block32125_Output_0', [#], [50 x 99 x 1]), Output('embed', [#], [100 x 300]), Output('OneHotOp31771_Output_0', [#], [100 x 69771]), Output('pooling_b_2', [#], [50 x 1 x 1]), Output('Block32062_Output_0', [#], [50 x 100 x 1]), Output('pooling_b_3', [#], [50 x 1 x 1]), Output('Block32188_Output_0', [#], [50 x 98 x 1])]\n",
      "data_size:  70829 batch_size:  42 num_batches_per_epoch:  1687\n",
      "Learning rate per minibatch: 0.015\n",
      "1000 205.2827491760254\n",
      "data_size:  30356 batch_size:  42 num_batches_per_epoch:  723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t-haohu/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/ipykernel_launcher.py:169: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/t-haohu/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/ipykernel_launcher.py:170: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:0.4158767990184214 Recall:0.6837110988520275 Acc:0.6863552510212149\n",
      "2000 108.58866882324219\n",
      "data_size:  30356 batch_size:  42 num_batches_per_epoch:  723\n",
      "Precision:0.560912106945622 Recall:0.7479616838311519 Acc:0.7515153511661615\n",
      "3000 45.51376509666443\n",
      "data_size:  30356 batch_size:  42 num_batches_per_epoch:  723\n",
      "Precision:0.6131053087061927 Recall:0.7592320939458043 Acc:0.7729279219923574\n",
      "4000 74.26017332077026\n",
      "data_size:  30356 batch_size:  42 num_batches_per_epoch:  723\n",
      "Precision:0.632344624266501 Recall:0.7486705364364733 Acc:0.7755633153248123\n",
      "5000 70.72360301017761\n",
      "data_size:  30356 batch_size:  42 num_batches_per_epoch:  723\n",
      "Precision:0.6411546492115059 Recall:0.7465667151395242 Acc:0.7782645934905784\n",
      "6000 45.43521165847778\n",
      "data_size:  30356 batch_size:  42 num_batches_per_epoch:  723\n",
      "Precision:0.6497191198886505 Recall:0.7335352852869191 Acc:0.7776057451574647\n",
      "7000 70.82598185539246\n",
      "data_size:  30356 batch_size:  42 num_batches_per_epoch:  723\n",
      "Precision:0.648007923871316 Recall:0.7371467900256016 Acc:0.7776716299907761\n",
      "8000 74.96896505355835\n",
      "data_size:  30356 batch_size:  42 num_batches_per_epoch:  723\n",
      "Precision:0.6403100258719667 Recall:0.7427837449099247 Acc:0.7769139544076954\n",
      "9000 45.70229458808899\n",
      "data_size:  30356 batch_size:  42 num_batches_per_epoch:  723\n",
      "Precision:0.6437039650871472 Recall:0.737925448028592 Acc:0.777473975490842\n",
      "10000 70.35177040100098\n",
      "data_size:  30356 batch_size:  42 num_batches_per_epoch:  723\n",
      "Precision:0.6450217422357963 Recall:0.7361871570006883 Acc:0.7768810119910396\n",
      "11000 74.7934877872467\n",
      "data_size:  30356 batch_size:  42 num_batches_per_epoch:  723\n",
      "Precision:0.6418049270441889 Recall:0.7395772714984334 Acc:0.7772433785742522\n",
      "12000 46.44505953788757\n",
      "data_size:  30356 batch_size:  42 num_batches_per_epoch:  723\n",
      "Precision:0.6444989791688102 Recall:0.7404972795827216 Acc:0.7771774937409408\n",
      "13000 68.63475799560547\n",
      "data_size:  30356 batch_size:  42 num_batches_per_epoch:  723\n",
      "Precision:0.6481439734748282 Recall:0.729689065144054 Acc:0.776320990907893\n",
      "14000 46.229326248168945\n",
      "data_size:  30356 batch_size:  42 num_batches_per_epoch:  723\n",
      "Precision:0.6446677930036949 Recall:0.739053249015474 Acc:0.776848069574384\n",
      "15000 50.11334705352783\n",
      "data_size:  30356 batch_size:  42 num_batches_per_epoch:  723\n",
      "Precision:0.6442849545522578 Recall:0.7353998779676046 Acc:0.7759586243246804\n",
      "16000 76.88296031951904\n",
      "data_size:  30356 batch_size:  42 num_batches_per_epoch:  723\n",
      "Precision:0.6427464585306975 Recall:0.7410025571747596 Acc:0.7768810119910396\n",
      "17000 57.222564935684204\n",
      "data_size:  30356 batch_size:  42 num_batches_per_epoch:  723\n",
      "Precision:0.6441511654330635 Recall:0.7381062750423515 Acc:0.7765186454078271\n",
      "18000 45.59273314476013\n",
      "data_size:  30356 batch_size:  42 num_batches_per_epoch:  723\n",
      "Precision:0.6463369922336344 Recall:0.7346641622252842 Acc:0.7760574515746476\n",
      "19000 44.399457693099976\n",
      "data_size:  30356 batch_size:  42 num_batches_per_epoch:  723\n",
      "Precision:0.6472312552672098 Recall:0.7320576146674191 Acc:0.7755303729081565\n",
      "20000 46.488396406173706\n",
      "data_size:  30356 batch_size:  42 num_batches_per_epoch:  723\n",
      "Precision:0.6486641054481236 Recall:0.7366124510722706 Acc:0.7762551060745816\n",
      "21000 65.77492785453796\n",
      "data_size:  30356 batch_size:  42 num_batches_per_epoch:  723\n",
      "Precision:0.6441797507561803 Recall:0.7416363770219363 Acc:0.7763868757412044\n",
      "22000 76.38007807731628\n",
      "data_size:  30356 batch_size:  42 num_batches_per_epoch:  723\n",
      "Precision:0.6480043225915421 Recall:0.7309114379204386 Acc:0.775069179074977\n",
      "23000 46.79612874984741\n",
      "data_size:  30356 batch_size:  42 num_batches_per_epoch:  723\n",
      "Precision:0.6436589317570439 Recall:0.7348412125082308 Acc:0.7756621425747793\n",
      "24000 46.69162654876709\n",
      "data_size:  30356 batch_size:  42 num_batches_per_epoch:  723\n",
      "Precision:0.6485308827703651 Recall:0.7341849913615913 Acc:0.775695084991435\n",
      "25000 45.333839416503906\n",
      "data_size:  30356 batch_size:  42 num_batches_per_epoch:  723\n",
      "Precision:0.6429144231872037 Recall:0.7379491014445764 Acc:0.7747068124917644\n",
      "26000 45.258583307266235\n",
      "data_size:  30356 batch_size:  42 num_batches_per_epoch:  723\n",
      "Precision:0.6461063855114905 Recall:0.7361314764213468 Acc:0.7756292001581236\n",
      "27000 70.33302044868469\n",
      "data_size:  30356 batch_size:  42 num_batches_per_epoch:  723\n",
      "Precision:0.6417665505898213 Recall:0.7326058895597056 Acc:0.774015021741995\n",
      "28000 73.28077983856201\n",
      "data_size:  30356 batch_size:  42 num_batches_per_epoch:  723\n",
      "Precision:0.6438821679605132 Recall:0.7395409756036531 Acc:0.7753986032415338\n",
      "29000 44.9328134059906\n",
      "data_size:  30356 batch_size:  42 num_batches_per_epoch:  723\n",
      "Precision:0.647542988925696 Recall:0.7351411534113688 Acc:0.7745750428251417\n",
      "30000 45.25770688056946\n",
      "data_size:  30356 batch_size:  42 num_batches_per_epoch:  723\n",
      "Precision:0.6490376801250092 Recall:0.7233364459958944 Acc:0.7739491369086836\n",
      "31000 45.1162211894989\n",
      "data_size:  30356 batch_size:  42 num_batches_per_epoch:  723\n",
      "Precision:0.6448180848041161 Recall:0.7338147432133377 Acc:0.7754315456581895\n",
      "32000 45.1476845741272\n",
      "data_size:  30356 batch_size:  42 num_batches_per_epoch:  723\n",
      "Precision:0.6451748730018558 Recall:0.7387519712121209 Acc:0.7734220582421927\n",
      "33000 65.52365756034851\n",
      "data_size:  30356 batch_size:  42 num_batches_per_epoch:  723\n",
      "Precision:0.6422945658039637 Recall:0.7405266895957252 Acc:0.7746738700751087\n"
     ]
    }
   ],
   "source": [
    "train_body(train_data,num_epochs=20,learning_rate=[5e-3*30]*2+[1e-4*30],batch_size = 42,tag = \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361 361\n",
      "data_size:  361 batch_size:  32 num_batches_per_epoch:  12\n"
     ]
    }
   ],
   "source": [
    "from cntk import load_model\n",
    "from data_processor import *\n",
    "process_setting(low=False,old = True,stop = False)\n",
    "batch_size  = 32\n",
    "def inference_body(model,val_doc_file,output_file,title_dict,body_dict,industry_file):\n",
    "    \n",
    "    scores = model(input_xt,input_xb)\n",
    "    predict = C.argmax(scores,axis = 0)\n",
    "    probability = C.reduce_max(C.softmax(scores),axis = 0)\n",
    "    \n",
    "    industry = [x.strip(\"\\n\") for x in open(industry_file,encoding =\"utf-8\").readlines()]\n",
    "    val_doc = open(val_doc_file,encoding = \"utf-8\").readlines()\n",
    "    output = open(output_file,\"w\",encoding = \"utf-8\")\n",
    "    val_title = [tokenize(x.strip(\"\\n\").split(\"\\t\")[0]) for x in val_doc]\n",
    "    val_body = [tokenize(x.strip(\"\\n\").split(\"\\t\")[1]) for x in val_doc]\n",
    "    data_title = np.zeros((len(val_title),max_length_title),dtype = np.float32)\n",
    "    data_body= np.zeros((len(val_body),max_length_body),dtype = np.float32)\n",
    "    for index,title in enumerate(val_title):       \n",
    "        for jndex,token in enumerate(title.split(\" \")):\n",
    "            if jndex>=max_length_title:\n",
    "                break\n",
    "            data_title[index,jndex]=title_dict.get(token,len(title_dict)-1)\n",
    "    for index,body in enumerate(val_body):       \n",
    "        for jndex,token in enumerate(body.split(\" \")):\n",
    "            if jndex>=max_length_body:\n",
    "                break\n",
    "            data_body[index,jndex]=body_dict.get(token,len(body_dict)-1)\n",
    "    print(len(data_title),len(data_body))       \n",
    "    batches = batch_iter(list(zip(data_title,data_body)), batch_size, 1)\n",
    "    for batch in batches:\n",
    "       #print(batch)\n",
    "        batch_data_title,batch_data_body = zip(*batch)\n",
    "        pred = np.array(predict.eval({input_xt: np.array(batch_data_title),input_xb: np.array(batch_data_body)}),dtype=np.int)\n",
    "        prob = np.array(probability.eval({input_xt: np.array(batch_data_title),input_xb: np.array(batch_data_body)}),dtype=np.float32)\n",
    "        #gt = np.array(batch_data_label,dtype=np.int)\n",
    "        #confuse+=fast_hist(output,gt,num_labels)\n",
    "        for pre,pro in list(zip(pred,prob)):\n",
    "            \n",
    "            output.write(\"\\t\".join([str(industry[int(pre)]),str(pro[0])])+\"\\n\")\n",
    "    output.close()\n",
    "    \n",
    "model_list =[\"model/180days_all_body_shuffled/CNN_180days_all_body_shuffled_acc10.864_acc20.517.dnn\"]#,\"180days_editor_shuffled\"]\n",
    "suffix_list = [\"180days_all_body_shuffled\"]\n",
    "industry_list = [\"180days_all_body_shuffled\"]#,\"180days_all_shuffled\"]\n",
    "prefix = \"/home/t-haohu/IndustryClassifier/Data/\"\n",
    "for suffix,model_name,industry in list(zip(suffix_list,model_list,industry_list)):\n",
    "    model = load_model(model_name)\n",
    "    data_industry_sample = \"{}/ready/industry_{}.wl\".format(prefix,industry)\n",
    "    data_title_sample    = \"{}/ready/title_{}.wl\".format(prefix,suffix)\n",
    "    data_body_sample     = \"{}/ready/body_{}.wl\".format(prefix,suffix)\n",
    "    title_dict =     { x:i for i,x in enumerate([x.strip(\"\\n\") for x in open(data_title_sample).readlines()])}\n",
    "    body_dict =     { x:i for i,x in enumerate([x.strip(\"\\n\") for x in open(data_body_sample).readlines()])}\n",
    "    inference_body(model,\"Data/middle/1day_measure_sample_valid_body.txt\",\"val/1day_measure_{}.txt\".format(suffix),title_dict,body_dict,data_industry_sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
