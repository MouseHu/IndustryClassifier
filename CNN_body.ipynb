{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function # Use a function definition from future version (say 3.x from 2.7 interpreter)\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import time \n",
    "\n",
    "import cntk as C\n",
    "import cntk.tests.test_utils\n",
    "import pickle\n",
    "import random\n",
    "from cntk import sequence\n",
    "from cntk import load_model\n",
    "from cntk.device import try_set_default_device, gpu,cpu\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "cntk.tests.test_utils.set_device_from_pytest_env() # (only needed for our build system)\n",
    "C.cntk_py.set_fixed_random_seed(1) # fix a random seed for CNTK components\n",
    "try_set_default_device(gpu(0))\n",
    "\n",
    "\n",
    "vocab_size = 80000\n",
    "num_labels = 19\n",
    "title_size = 52000\n",
    "body_size  = 210000\n",
    "input_dim  = vocab_size\n",
    "label_dim  = num_labels\n",
    "emb_dim    = 300\n",
    "hidden_dim = 200\n",
    "\n",
    "max_length_title = 30\n",
    "max_length_body  = 100\n",
    "\n",
    "\n",
    "\n",
    "suffix = \"180days_sample\"\n",
    "prefix = \"/home/t-haohu/IndustryClassifier/Data/\"\n",
    "\n",
    "data_token_body        = \"{}/middle/{}_token_body.txt\".format(prefix,suffix)\n",
    "data_train_sample_body = \"{}/middle/train_{}_body.txt\".format(prefix,suffix)\n",
    "data_test_sample_body  = \"{}/middle/test_{}_body.txt\".format(prefix,suffix)\n",
    "\n",
    "data_industry_sample = \"{}/ready/industry_{}.wl\".format(prefix,suffix)\n",
    "data_title_sample    = \"{}/ready/title_{}.wl\".format(prefix,suffix)\n",
    "data_body_sample     = \"{}/ready/body_{}.wl\".format(prefix,suffix)\n",
    "\n",
    " \n",
    "\n",
    "def load_data_body(input_file,title_dict,body_dict,industry_dict):\n",
    "    data = open(input_file, encoding = \"utf-8\").readlines()\n",
    "    \n",
    "    data_title = np.zeros((len(data),max_length_title),dtype = np.float32)\n",
    "    data_body  = np.zeros((len(data),max_length_body),dtype = np.float32)\n",
    "    data_label = np.zeros((len(data),1),dtype = np.float32)\n",
    "    \n",
    "    \n",
    "    for index,line in enumerate(data):\n",
    "        row = line.strip(\"\\n\").split(\"\\t\")\n",
    "        \n",
    "        title    =  row[0]\n",
    "        body     =  row[1]\n",
    "        industry =  row[2]\n",
    "        \n",
    "        for jndex,token in enumerate(title.split(\" \")):\n",
    "            if jndex>=max_length_title:\n",
    "                break\n",
    "            data_title[index,jndex]=title_dict.get(token,len(title_dict)-1)\n",
    "            \n",
    "        for jndex,token in enumerate(body.split(\" \")):\n",
    "            if jndex>=max_length_body:\n",
    "                break\n",
    "            data_body[index,jndex]=body_dict.get(token,len(title_dict)-1)\n",
    "            \n",
    "        data_label[index] = industry_dict.get(industry,len(industry_dict))\n",
    "    return data_title,data_body,data_label\n",
    "\n",
    "def create_model_cnn_with_body():\n",
    "    \n",
    "    h1t= C.layers.Embedding(300,name='embed')(input_xt_one_hot)#init=embedding,\n",
    "    h1b= C.layers.Embedding(300,name='embed')(input_xb_one_hot)#init=embedding,\n",
    "    \n",
    "    #bnb = C.layers.BatchNormalization(name='bn')(h1b)\n",
    "    #bnt = C.layers.BatchNormalization(name='bn')(h1t)\n",
    "\n",
    "\n",
    "\n",
    "    h2_1t=C.layers.Convolution((1,emb_dim),num_filters=50,reduction_rank=0,activation=C.relu)(h1t)\n",
    "    h2_2t=C.layers.Convolution((2,emb_dim),num_filters=50,reduction_rank=0,activation=C.relu)(h1t)\n",
    "    h2_3t=C.layers.Convolution((3,emb_dim),num_filters=50,reduction_rank=0,activation=C.relu)(h1t)\n",
    "\n",
    "    h2_1b=C.layers.Convolution((1,emb_dim),num_filters=50,reduction_rank=0,activation=C.relu)(h1b)\n",
    "    h2_2b=C.layers.Convolution((2,emb_dim),num_filters=50,reduction_rank=0,activation=C.relu)(h1b)\n",
    "    h2_3b=C.layers.Convolution((3,emb_dim),num_filters=50,reduction_rank=0,activation=C.relu)(h1b)\n",
    "\n",
    "    h3_2t=C.layers.MaxPooling((max_length_title-1,1),name='pooling_t_1')(h2_2t)\n",
    "    h3_1t=C.layers.MaxPooling((max_length_title-0,1),name='pooling_t_2')(h2_1t)\n",
    "    h3_3t=C.layers.MaxPooling((max_length_title-2,1),name='pooling_t_3')(h2_3t)\n",
    "\n",
    "    h3_2b=C.layers.MaxPooling((max_length_body-1,1),name='pooling_b_1')(h2_2b)\n",
    "    h3_1b=C.layers.MaxPooling((max_length_body-0,1),name='pooling_b_2')(h2_1b)\n",
    "    h3_3b=C.layers.MaxPooling((max_length_body-2,1),name='pooling_b_3')(h2_3b)\n",
    "\n",
    "    h3=C.splice(h3_2t,h3_1t,h3_3t,h3_2b,h3_1b,h3_3b,axis=0)\n",
    "\n",
    "    #h4=C.layers.Dense(hidden_dim, activation=C.relu,name='hidden')(h3)\n",
    "    #drop1 = C.layers.Dropout(0.5,name='drop1')(h3)\n",
    "\n",
    "    h4=C.layers.Dense(num_labels,name='classify')(h3)\n",
    "\n",
    "    return h4\n",
    "\n",
    "def batch_iter(data,batch_size, num_epochs, shuffle=True):\n",
    "    # Generates a batch iterator for a dataset.\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((data_size-1)/batch_size) + 1\n",
    "    print('data_size: ', data_size, 'batch_size: ', batch_size, 'num_batches_per_epoch: ', num_batches_per_epoch)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            random.shuffle(data)\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield data[start_index:end_index]\n",
    "            \n",
    "\n",
    "def fast_hist(a, b, n):\n",
    "    k = (a >= 0) & (a < n)\n",
    "    return np.bincount(n * a[k].astype(int) + b[k], minlength=n**2).reshape(n, n)\n",
    "\n",
    "title_dict =     { x:i for i,x in enumerate([x.strip(\"\\n\") for x in open(data_title_sample).readlines()])}\n",
    "body_dict  =     { x:i for i,x in enumerate([x.strip(\"\\n\") for x in open(data_body_sample ).readlines()])}\n",
    "industry_dict =  { x:i for i,x in enumerate([x.strip(\"\\n\") for x in open(data_industry_sample).readlines()])}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_xt = C.input_variable(shape=(max_length_title),  dtype=np.float32)\n",
    "input_xb = C.input_variable(shape=(max_length_body) ,  dtype=np.float32)\n",
    "input_y  = C.input_variable(shape=(1)               ,  dtype=np.int)\n",
    "\n",
    "input_xt_one_hot = C.one_hot(input_xt, num_classes=len(title_dict)   ,  sparse_output=True)\n",
    "input_xb_one_hot = C.one_hot(input_xb, num_classes=len(body_dict)    ,  sparse_output=True) \n",
    "input_y_one_hot = C.one_hot(input_y  , num_classes=len(industry_dict) ,  sparse_output=True)\n",
    "\n",
    "#test_data_title, test_data_body, test_data_label  = load_data_body(data_test_sample_body,title_dict,body_dict,industry_dict)\n",
    "#train_data_title,train_data_body,train_data_label = load_data_body(data_train_sample_body,title_dict,body_dict,industry_dict)\n",
    "\n",
    "test_data  = load_data_body(data_test_sample_body,title_dict,body_dict,industry_dict)\n",
    "train_data = load_data_body(data_train_sample_body,title_dict,body_dict,industry_dict)\n",
    "\n",
    "def test_body(batch_size,model,data):\n",
    "    scores = model(input_xt,input_xb)\n",
    "    predict = C.argmax(scores,axis = 0)\n",
    "    confuse = np.zeros((num_labels,num_labels))\n",
    "    #C.element_add(input_y,C.element_times(predict,C.Constant([nums_labels])))\n",
    "    test_data_title,test_data_body,test_data_label = data\n",
    "    batches = batch_iter(list(zip(test_data_title,test_data_body,test_data_label)), batch_size, 1)\n",
    "    \n",
    "    for batch in batches:\n",
    "        batch_data_title,batch_data_body,batch_data_label = zip(*batch) \n",
    "        output = np.array(predict.eval({input_xb: np.array(batch_data_body),input_xt: np.array(batch_data_title)}),dtype=np.int)\n",
    "        gt = np.array(batch_data_label,dtype=np.int)\n",
    "        confuse+=fast_hist(output,gt,num_labels)\n",
    "    precision=np.diag(confuse)/np.sum(confuse,axis=0)\n",
    "    recall = np.diag(confuse)/np.sum(confuse,axis=1)\n",
    "    accuarcy = np.diag(confuse).sum() / confuse.sum()\n",
    "    aver_precision=np.nanmean(precision)\n",
    "    aver_recall = np.nanmean(recall)\n",
    "   \n",
    "    print(\"Precision:{} Recall:{} Acc:{}\".format(aver_precision,aver_recall,accuarcy))\n",
    "def train_body(train_data,num_epochs,learning_rate,batch_size,l2_weight=0):\n",
    "    \n",
    "    #learning_rate *= batch_size\n",
    "    model = create_model_cnn_with_body()\n",
    "    print(C.logging.get_node_outputs(model))\n",
    "    scores = model(input_xt,input_xb)\n",
    "\n",
    "    loss =C.reduce_mean(C.losses.cross_entropy_with_softmax(scores, input_y_one_hot))\n",
    "    \n",
    "    # Training\n",
    "    lr_schedule = C.learning_parameter_schedule(learning_rate)\n",
    "    #learner = C.adam(scores.parameters, lr=lr_schedule, momentum=0.9,l2_regularization_weight=0)\n",
    "    progress_printer = C.logging.ProgressPrinter(tag='Training', num_epochs=num_epochs)\n",
    "    momentums = C.momentum_schedule(0.99, minibatch_size=batch_size)\n",
    "    learner = C.adam(parameters=scores.parameters,#model.parameters,\n",
    "                     lr=lr_schedule,\n",
    "                     momentum=momentums,\n",
    "                     gradient_clipping_threshold_per_sample=15,\n",
    "                     gradient_clipping_with_truncation=True,\n",
    "                     l2_regularization_weight=l2_weight)\n",
    "    trainer = C.Trainer(scores, (loss), [learner], progress_printer)\n",
    "    \n",
    "    train_data_title,train_data_body,train_data_label = train_data\n",
    "    batches = batch_iter(list(zip(train_data_title,train_data_body,train_data_label)), batch_size, num_epochs)\n",
    "\n",
    "    # training loop\n",
    "    count = 0\n",
    "    t = time.time()\n",
    "    for batch in batches:\n",
    "        count += 1\n",
    "        batch_data_title,batch_data_body,batch_data_label = zip(*batch)\n",
    "        #print(np.array(batch_data_body).shape)\n",
    "        #batch_data_title,batch_data_body,batch_data_label = transfer(batch_data_title),transfer(batch_data_body),trainsfer(batch_data_label)\n",
    "        trainer.train_minibatch({input_xb: np.array(batch_data_body),input_xt: np.array(batch_data_title), input_y: np.array(batch_data_label)})\n",
    "        if count%1000== 0:\n",
    "            print(count,time.time()-t)\n",
    "            t=time.time()\n",
    "            test(batch_size,model,(data_test_sample_body ,title_dict,body_dict,industry_dict))\n",
    "\n",
    "    # save model\n",
    "    model.save('./model/IndustryClassifyCNN_body.dnn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Output('classify', [#], [19]), Output('Splice62824_Output_0', [#], [300 x 1 x 1]), Output('pooling_t_1', [#], [50 x 1 x 1]), Output('Block62303_Output_0', [#], [50 x 29 x 1]), Output('embed', [#], [30 x 300]), Output('OneHotOp62135_Output_0', [#], [30 x 23184]), Output('pooling_t_2', [#], [50 x 1 x 1]), Output('Block62240_Output_0', [#], [50 x 30 x 1]), Output('pooling_t_3', [#], [50 x 1 x 1]), Output('Block62366_Output_0', [#], [50 x 28 x 1]), Output('pooling_b_1', [#], [50 x 1 x 1]), Output('Block62492_Output_0', [#], [50 x 99 x 1]), Output('embed', [#], [100 x 300]), Output('OneHotOp62138_Output_0', [#], [100 x 60491]), Output('pooling_b_2', [#], [50 x 1 x 1]), Output('Block62429_Output_0', [#], [50 x 100 x 1]), Output('pooling_b_3', [#], [50 x 1 x 1]), Output('Block62555_Output_0', [#], [50 x 98 x 1])]\n",
      "data_size:  41940 batch_size:  30 num_batches_per_epoch:  1398\n",
      "Learning rate per minibatch: 0.03\n",
      "1000 36.22497200965881\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t-haohu/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/ipykernel_launcher.py:159: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:0.5324430853511949 Recall:0.6902382816627641 Acc:0.704144645340751\n",
      "2000 40.1063814163208\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.6264493572648457 Recall:0.6826649727497918 Acc:0.7337969401947149\n",
      "3000 40.165118932724\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.6447254218870008 Recall:0.6904095389824263 Acc:0.7491515994436717\n",
      "4000 40.35217499732971\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.6434215014286137 Recall:0.6714896054994925 Acc:0.7363004172461752\n",
      "5000 40.23284864425659\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.6483243291678749 Recall:0.6697220329971958 Acc:0.7425312934631433\n",
      "6000 40.12236523628235\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.6447874827329285 Recall:0.6747400128822413 Acc:0.7453685674547983\n",
      "7000 39.97572350502014\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.646859200898094 Recall:0.6740695810891048 Acc:0.7448122392211405\n",
      "8000 40.46642231941223\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.6417166792843019 Recall:0.6875912945454057 Acc:0.7466481223922115\n",
      "9000 40.120808124542236\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.6449308322190186 Recall:0.6851533025198353 Acc:0.7468150208623088\n",
      "10000 40.18081259727478\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.648312635593316 Recall:0.6810893881299639 Acc:0.7484283727399166\n",
      "11000 40.13318228721619\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.6439068092335439 Recall:0.6835014504785112 Acc:0.7482614742698191\n",
      "12000 40.71727705001831\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.6463813658378704 Recall:0.6829096407253761 Acc:0.7458692628650904\n",
      "13000 40.32339692115784\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.646427622338058 Recall:0.685284062236983 Acc:0.7490403337969402\n",
      "14000 40.2943754196167\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.6457124799077888 Recall:0.6851549225651408 Acc:0.747260083449235\n",
      "15000 40.362828731536865\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.6465217955154797 Recall:0.6852434019465728 Acc:0.7472044506258693\n",
      "16000 40.52878284454346\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.648364490691312 Recall:0.6833543789577883 Acc:0.7501529902642559\n",
      "17000 40.14714503288269\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.6449222316598948 Recall:0.6848881364512346 Acc:0.7491515994436717\n",
      "18000 39.15268588066101\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.6478182134685735 Recall:0.6843877406600738 Acc:0.7465368567454799\n",
      "19000 40.18277835845947\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.6466937795716639 Recall:0.6845366170212999 Acc:0.7493741307371349\n",
      "20000 40.118337631225586\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.646134003567112 Recall:0.6839886049751303 Acc:0.7492628650904033\n",
      "21000 40.02832365036011\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.6472618642020077 Recall:0.6838849495337666 Acc:0.7493741307371349\n",
      "22000 40.13299918174744\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.6464629312841988 Recall:0.6838403297213008 Acc:0.7493741307371349\n",
      "23000 40.18820667266846\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.6469448638399271 Recall:0.6835697215314049 Acc:0.7499860917941585\n",
      "24000 39.74358415603638\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.6471250728029586 Recall:0.684087694214798 Acc:0.7497635605006954\n",
      "25000 39.82077670097351\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.6476382166322091 Recall:0.6836742133340251 Acc:0.7500417246175244\n",
      "26000 39.92252206802368\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.647080501927495 Recall:0.6837310920961306 Acc:0.7496522948539638\n",
      "27000 40.07861924171448\n",
      "data_size:  17975 batch_size:  30 num_batches_per_epoch:  600\n",
      "Precision:0.6474453989900646 Recall:0.6835763932881198 Acc:0.749874826147427\n"
     ]
    }
   ],
   "source": [
    "train(num_epochs=20,learning_rate=[1e-3*30]*2+[1e-4*30],batch_size = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] (1, 2, 3)\n",
      "[1, 2, 3] [4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "x = [1,2,3]\n",
    "y = [4,5,6]\n",
    "zipped = zip(x,y)\n",
    "unzipped_x, unzipped_y = zip(*zipped)\n",
    "print(list(zipped),unzipped_x)\n",
    "txy=(x,y)\n",
    "a,b =txy\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[6.]\n",
      "  [6.]]]\n",
      "[[[6.]\n",
      "  [6.]]]\n"
     ]
    }
   ],
   "source": [
    "C.cntk_py.set_fixed_random_seed(int(time.time()))\n",
    "W = C.Constant([[1,1],[1,1]],dtype=np.float64)\n",
    "x = C.input_variable(shape=(2,1),dtype=np.float64)\n",
    "h1 = C.times(W,x)\n",
    "h2=C.layers.Dropout(0.5)(h1)\n",
    "h3 = C.times(W,h1)\n",
    "h4 = C.times(W,h2)\n",
    "print(h3.eval({x:np.array([[2],[1]],dtype=np.float64)}))\n",
    "print(h4.eval({x:np.array([[2],[1]],dtype=np.float64)}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
