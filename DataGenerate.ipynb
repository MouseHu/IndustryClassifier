{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data_generator.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_generator.py\n",
    "#preprocess the data & generate vocabulary\n",
    "import numpy as np\n",
    "import csv\n",
    "import operator\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "import gensim\n",
    "import pickle\n",
    "from data_processor import *\n",
    "np.random.seed(1)\n",
    "\n",
    "\n",
    "def extract_data(input_name,output_name,cols= [],processor=[],criteria = (lambda row:True),delimiter = \"\\t\"):\n",
    "    '''\n",
    "    extract data from a Unstructured Stream File Generated By Cosmos.\n",
    "    default delimiter is \\t\n",
    "    Processor process each row before output\n",
    "    and criteria decide whether the row is in output\n",
    "    '''\n",
    "    data=open(input_name,\"r\",encoding='utf_8')\n",
    "    output=open(output_name,\"w\",encoding='utf_8')\n",
    "    \n",
    "    if len(cols) == 0:\n",
    "        print(\"Empty Output.\")\n",
    "        return\n",
    "    if (len(cols) != len(processor)) and (len(processor) != 0):\n",
    "        print(\"Wrong Number of input,or processor.\")\n",
    "        return\n",
    "    count=0\n",
    "    for line in data.readlines():\n",
    "        row = line.rstrip(\"\\n\").split(delimiter)\n",
    "        if not criteria(row):\n",
    "            continue\n",
    "          \n",
    "        for i,index in enumerate(cols):\n",
    "            if len(processor):\n",
    "                output.write(processor[i](row[index]))\n",
    "            else:\n",
    "                output.write(row[index])\n",
    "            output.write(\"\\n\") if i == len(cols)-1 else output.write('\\t')\n",
    "        count+=1\n",
    "        if count%50000==0:\n",
    "            print(count)\n",
    "\n",
    "\n",
    "    data.close()\n",
    "    output.close()\n",
    "    print(\"Finished Extraction, Rows:{}, Cols:{}.\".format(count,len(cols)))\n",
    "    \n",
    "def csv2csv(input_file,output_file,input_delimiter = \"\\t\",output_delimiter=\",\"):\n",
    "    output = open(output_file,\"w\",encoding = \"utf-8\")\n",
    "    \n",
    "    for line in open(input_file,encoding=\"utf-8\").readlines():\n",
    "        output.write(output_delimiter.join(line.split(input_delimiter)))\n",
    "        \n",
    "    output.close()\n",
    "    \n",
    "    print(\"Generate New CSV File Successfully.\")\n",
    "    \n",
    "\n",
    "def shuffle_data(data_file,out_data_file=None):\n",
    "    data  = open(data_file,encoding = \"utf-8\").readlines()\n",
    "    random.shuffle(data)\n",
    "    if out_data_file:\n",
    "        out_data = open(out_data_file,\"w\",encoding=\"utf-8\")\n",
    "    else:\n",
    "        out_data = open(data_file,\"w\",encoding=\"utf-8\")\n",
    "    for line in data:\n",
    "        out_data.write(line)\n",
    "    out_data.close()\n",
    "    print(\"Data Shuffle Succeed.\")\n",
    "\n",
    "def tokenize_data(input_file,output_file,no_token_list =[],filters ={}):\n",
    "    '''\n",
    "    need to import process\n",
    "    '''\n",
    "    print(\"Generating Tokenized Data ...\")\n",
    "    \n",
    "    data=open(input_file,\"r\",encoding='utf8')\n",
    "    output=open(output_file,\"w\",encoding='utf_8')\n",
    "    #dict_list = [{} for x in range(len(data.readline().split(\"\\t\")))] \n",
    "    count =0\n",
    "    for line in data.readlines():\n",
    "        row = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "        for col in range(len(row)):\n",
    "            if col not in no_token_list:\n",
    "                row[col] = tokenize(row[col])\n",
    "        for col in filters.keys():\n",
    "            if filters[col](row[col]):\n",
    "                continue\n",
    "        #for col in range(len(row)):\n",
    "        #    _make_dict(row[col],dict_list[col])\n",
    "                    \n",
    "        out = '\\t'.join(row)\n",
    "        output.write(out+\"\\n\")\n",
    "        count+=1\n",
    "        if count%50000 == 0:\n",
    "            print(count)\n",
    "    output.close()\n",
    "    data.close()\n",
    "    print(\"Finished Generating Tokenized Data. Total:{} .\".format(count))\n",
    "    #return dict_list\n",
    "def _make_dict(string,dic,tokenized=True):\n",
    "    if not tokenized:\n",
    "        tokens=[string]\n",
    "    else:\n",
    "        tokens=string.split(\" \")\n",
    "    for x in tokens:\n",
    "        if x in dic:\n",
    "            dic[x]+=1\n",
    "        else:\n",
    "            dic[x]=1\n",
    "def dict_data(input_file,no_token_list =[]):\n",
    "    \n",
    "    \n",
    "    data=open(input_file,\"r\",encoding='utf8')\n",
    "    dict_list = [{} for x in range(len(data.readline().split(\"\\t\")))] \n",
    "    for line in data.readlines():\n",
    "        row = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "        for col in range(len(row)):\n",
    "            if col in no_token_list:\n",
    "                _make_dict(row[col],dict_list[col],tokenized=False)\n",
    "            else:\n",
    "                _make_dict(row[col],dict_list[col],tokenized=True)\n",
    "    return dict_list\n",
    "\n",
    "def dict2file(dict_list,dict_file_list,criteria,with_count=False):\n",
    "    for i in range(len(dict_file_list)):\n",
    "        sorted_dict = sorted(dict_list[i].items(), key=operator.itemgetter(1))\n",
    "        file = open(dict_file_list[i],\"w\",encoding=\"utf-8\")\n",
    "        for x in sorted_dict:\n",
    "            \n",
    "            if with_count and criteria[i](x[1]):\n",
    "                file.write(x[0]+\"\\t\"+str(x[1])+\"\\n\")\n",
    "            elif criteria[i](x[1]):\n",
    "                file.write(x[0]+\"\\n\")\n",
    "        if len(sorted_dict)>200 and not with_count:\n",
    "            file.write(\"UNK\\n\")\n",
    "        file.close()\n",
    "        \n",
    "def sample_data(input_file,output_file,num_samples_per_class,per_class=1,classifier=(lambda x:0)):\n",
    "    data = open(input_file,encoding = \"utf-8\").readlines()\n",
    "    random.shuffle(data)\n",
    "    output = open(output_file,\"w\",encoding = \"utf-8\")\n",
    "    class_count = {x:0 for x in range(per_class)}\n",
    "    \n",
    "    for line in data:\n",
    "        if class_count[classifier(line)]<num_samples_per_class:\n",
    "            class_count[classifier(line)]+=1\n",
    "            output.write(line)\n",
    "    print(\"Sample Data Succeed. Total: {}\".format(num_samples_per_class*per_class))   \n",
    "        \n",
    "def split_data(input_file,train_file,test_file,split_count=0,industry_dict=None):\n",
    "    '''\n",
    "    split count can either be a integer or a fraction(percentage)\n",
    "    '''\n",
    "    data = open(input_file,\"r\",encoding='utf_8').readlines()\n",
    "    train = open(train_file,\"w\",encoding='utf_8')\n",
    "    test = open(test_file,\"w\",encoding='utf_8')\n",
    "    if industry_dict:\n",
    "        temp_indusrey_dict = copy.copy(industry_dict)\n",
    "        print(industry_dict.keys())\n",
    "        for x in industry_dict.keys():        \n",
    "            temp_industry_dict[x]=int(industry_dict[x]*0.7)\n",
    "    if split_count == 0:\n",
    "        split_count = int(len(data)*0.7)\n",
    "    elif 0 < split_count < 1 :\n",
    "        split_count = int(len(data)*split_count)\n",
    "    if industry_dict:\n",
    "        for line in data:\n",
    "            industry = line.rstrip(\"\\n\").split(\"\\t\")[1]\n",
    "            if temp_industry_dict[industry]==0:\n",
    "                test.write(line)\n",
    "            else:\n",
    "                temp_industry_dict[industry]-=1\n",
    "                train.write(line)\n",
    "    else:\n",
    "        train_data=data[:split_count]\n",
    "        test_data = data[split_count:]\n",
    "        \n",
    "        for line in train_data:\n",
    "            train.write(line)\n",
    "        \n",
    "        for line in test_data:\n",
    "            test.write(line)\n",
    "    train.close()\n",
    "    test.close()\n",
    "    print(\"Split Succeed.Train:{} Test:{}\".format(split_count,len(data)-split_count))\n",
    "    \n",
    "    \n",
    "def ctf_data(input_name,output_name,dict_name_list):\n",
    "    '''\n",
    "    generating ctf data the number of dict should be more than zero \n",
    "    IMPORTANT: \n",
    "        it should be customized on your computer/server if you want to run properly\n",
    "    '''\n",
    "    py_root = \"C:\\Local\\Anaconda3-4.1.1-Windows-x86_64\\envs\\cntk-py35\\python.exe\"\n",
    "    py_file = \"C:/Local/CNTK-2.5.1/cntk/Scripts/txt2ctf.py\"\n",
    "    if len(dict_name_list) == 0:\n",
    "        print(\"Empty dict list.\")\n",
    "        return\n",
    "    command = \" {} {} --map {} --annotated True --input {} --output {} --unk UNK\".format(py_root,py_file,\" \".join(dict_name_list),input_name,output_name)\n",
    "    #print(command)\n",
    "    value = os.system(command)\n",
    "    if value == 0:\n",
    "        print(\"Finished Generating CTF Data.\")\n",
    "    else:\n",
    "        print(\"Error in Executing Command.\")\n",
    "        print(command)\n",
    "        \n",
    "def union_data(input_files,output_file,shuffle=True):\n",
    "    output = open(output_file,\"w\",encoding = \"utf-8\")\n",
    "    for file in input_files:\n",
    "        for line in open(file,encoding = \"utf-8\").readlines():\n",
    "            output.write_line()\n",
    "    output.close()\n",
    "    if shuffle:\n",
    "        shuffle_data(output_file)\n",
    "        \n",
    "def dedup_data(input_file,output_file = None,selector = lambda row:row):\n",
    "    data = open(input_file,encoding = \"utf-8\").readlines()\n",
    "    row_dict = {}\n",
    "    \n",
    "    for line in data:\n",
    "        row_dict[selector(line)] = line\n",
    "    #data.close()\n",
    "    if output_file:\n",
    "        output = open(output_file,\"w\",encoding = \"utf-8\")\n",
    "    else:\n",
    "        output = open(input_file,\"w\",encoding = \"utf-8\")\n",
    "    for key in row_dict.keys():\n",
    "        output.write(row_dict[key])\n",
    "    output.close()\n",
    "    print(\"Dedup Successfully. Before:{} After:{}\".format(len(data),len(row_dict)))\n",
    "    \n",
    "def embed_dict(w2v_dict,embed_file,vocab_dict):\n",
    "\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(w2v_dict, binary=True)\n",
    "    sorted_dict = sorted(vocab_dict.items(), key=operator.itemgetter(1))\n",
    "    w2v_emb = []\n",
    "    count = 0\n",
    "    for pair in sorted_dict:\n",
    "                word = pair[0]\n",
    "                if word == \"UNK\":\n",
    "                    #print(\"o\")\n",
    "                    w2v_emb.append(np.random.normal(0,1/300,300))\n",
    "\n",
    "                else:\n",
    "                    try:\n",
    "                        w2v_emb.append(model[word])\n",
    "                        count+=1\n",
    "                    except:\n",
    "                        #print(word)\n",
    "                        w2v_emb.append(np.random.normal(0,1/300,300))\n",
    "                #else:\n",
    "                #    w2v_dict.append(np.zeros((300)))\n",
    "    with open(embed_file, 'wb') as handle:\n",
    "                    pickle.dump(np.array(w2v_emb), handle)\n",
    "    print(\"Embedding File Generate Successfully. Total: {} Embedded: {}\".format(len(w2v_emb),count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [cntk-py35]",
   "language": "python",
   "name": "Python [cntk-py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
