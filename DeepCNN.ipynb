{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function # Use a function definition from future version (say 3.x from 2.7 interpreter)\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import time \n",
    "\n",
    "import cntk as C\n",
    "import cntk.tests.test_utils\n",
    "from cntk.layers import *\n",
    "from cntk.layers.typing import *\n",
    "import pickle\n",
    "import random\n",
    "from cntk import sequence\n",
    "from cntk import load_model\n",
    "from cntk.device import try_set_default_device, gpu,cpu\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "cntk.tests.test_utils.set_device_from_pytest_env() # (only needed for our build system)\n",
    "C.cntk_py.set_fixed_random_seed(1) # fix a random seed for CNTK components\n",
    "try_set_default_device(gpu(0))\n",
    "\n",
    "\n",
    "vocab_size = 80000\n",
    "num_labels = 19#19\n",
    "title_size = 52000\n",
    "body_size  = 210000\n",
    "input_dim  = vocab_size\n",
    "label_dim  = num_labels\n",
    "emb_dim    = 300\n",
    "hidden_dim = 200\n",
    "\n",
    "max_length_title = 53\n",
    "max_length_body  = 200\n",
    "\n",
    "suffix = \"180days_all_shuffled\"\n",
    "#suffix = \"linkedin_only\"\n",
    "prefix = \"/home/t-haohu/IndustryClassifier/Data/\"\n",
    "\n",
    "#data_token_body        = \"{}/middle/{}_token_body.txt\".format(prefix,suffix)\n",
    "data_train_sample = \"{}/middle/train_{}.txt\".format(prefix,suffix)\n",
    "#data_train_sample = \"{}/middle/train_{}_with_linkedin_all.txt\".format(prefix,suffix)\n",
    "data_test_sample  = \"{}/middle/test_{}.txt\".format(prefix,suffix)\n",
    "#data_test_sample_editor  = \"{}/middle/test_{}_editor.txt\".format(prefix,suffix)\n",
    "\n",
    "data_title_sample    = \"{}/ready/title_{}.wl\".format(prefix,suffix)\n",
    "data_body_sample     = \"{}/ready/body_{}.wl\".format(prefix,suffix)\n",
    "suffix = \"180days_all_shuffled\"\n",
    "data_industry_sample = \"{}/ready/industry_{}.wl\".format(prefix,suffix)\n",
    "filter_num=200 \n",
    "dropout_rate = 0.5\n",
    "emb_dim =300\n",
    "\n",
    "def load_data(input_file,title_dict,industry_dict):\n",
    "    data = open(input_file, encoding = \"utf-8\").readlines()\n",
    "    \n",
    "    data_title =np.zeros((len(data),max_length_title),dtype = np.float32)\n",
    "    data_label = np.zeros((len(data),1),dtype = np.float32)\n",
    "       \n",
    "    for index,line in enumerate(data):\n",
    "        row = line.strip(\"\\n\").split(\"\\t\")       \n",
    "        title    =  row[0]\n",
    "        industry =  row[1]\n",
    "        \n",
    "        for jndex,token in enumerate(title.split(\" \")):\n",
    "            if jndex>=max_length_title:\n",
    "                break\n",
    "            data_title[index,jndex]=title_dict.get(token,len(title_dict)-1)    \n",
    "        data_label[index] = industry_dict.get(industry,len(industry_dict))\n",
    "    return data_title,data_label\n",
    "\n",
    "\n",
    "def load_embedding(title_file,embedding_model_file):\n",
    "    model = Word2Vec.load(embedding_model_file)\n",
    "    title_list = [x.strip(\"\\n\") for x in open(title_file,encoding = 'utf-8').readlines()]\n",
    "    embedding = np.zeros((len(title_list),emb_dim))\n",
    "    count = 0\n",
    "    for i,w in enumerate(title_list):\n",
    "        try:\n",
    "            vec = model.wv[w]\n",
    "        except:\n",
    "            vec=model.wv[\"UNK\"]\n",
    "            count+=1\n",
    "        embedding[i] =vec\n",
    "    print(count)\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def create_model_cnn(embed = False):\n",
    "    #version 2 : 1 dense layer version3: sigmoid activation in dense\n",
    "    if embed:\n",
    "        h1= C.layers.Embedding(weights=embedding,name='embed_1')(input_xt_one_hot)#\n",
    "    else:\n",
    "        h1= C.layers.Embedding(emb_dim,name='embed_2')(input_xt_one_hot)#init=embedding,\n",
    "    print(h1)\n",
    "    h2=C.layers.Convolution((3,emb_dim),num_filters=filter_num,reduction_rank=0,activation=C.relu)(h1)\n",
    "    h3=C.squeeze(C.layers.MaxPooling((3,1),strides =3,name='pooling_1')(h2))\n",
    "    print(h3)\n",
    "    #h2=BiRecurrence(C.layers.LSTM(hidden_dim), C.layers.LSTM(hidden_dim))(h1)\n",
    "    h4 = C.layers.Convolution((filter_num,3),num_filters=filter_num,reduction_rank=0,activation=C.relu)(h3)\n",
    "    print(h4)\n",
    "    h5 = C.squeeze(C.layers.MaxPooling((1,3),strides =3,name='pooling_2')(h4))\n",
    "    print(h5)\n",
    "    h6 = C.layers.Convolution((filter_num,3),num_filters=filter_num,reduction_rank=0,activation=C.relu)(h5)\n",
    "    print(h6)\n",
    "    h7 = C.squeeze(C.layers.MaxPooling((1,3),strides =3,name='pooling_3')(h6))\n",
    "    print(h7)\n",
    "    drop1 = C.layers.Dropout(dropout_rate)(h7)\n",
    "    #h8=C.layers.Dense(hidden_dim,name='hidden')(drop1)\n",
    "    logits = C.layers.Dense(num_labels,name='hidden')(drop1)\n",
    "    return logits\n",
    "\n",
    "def create_model_cnn_2fold():\n",
    "    #version 2 : 1 dense layer version3: sigmoid activation in dense\n",
    "    #\n",
    "    with C.layers.default_options(initial_state=0.1):\n",
    "\n",
    "\n",
    "        h1_1= C.layers.Embedding(weights=embedding,name='embed_1')(input_xt_one_hot)#\n",
    "        h1_2= C.layers.Embedding(300,name='embed_2')(input_xt_one_hot)#init=embedding,\n",
    "        \n",
    "        h1_1_expand = C.expand_dims(h1_1,-3)\n",
    "        h1_2_expand = C.expand_dims(h1_2,-3)\n",
    "        \n",
    "        h1 = C.splice(h1_1_expand,h1_2_expand,axis = -3)\n",
    "   #value,valid = to_static(h1)\n",
    "\n",
    "        filter_num=100\n",
    "\n",
    "        h2_1=C.layers.Convolution((3,emb_dim),num_filters=filter_num,reduction_rank=1,activation=C.relu)(h1)\n",
    "        h2_2=C.layers.Convolution((4,emb_dim),num_filters=filter_num,reduction_rank=1,activation=C.relu)(h1)\n",
    "        h2_3=C.layers.Convolution((5,emb_dim),num_filters=filter_num,reduction_rank=1,activation=C.relu)(h1)\n",
    "\n",
    "        h3_1=C.layers.MaxPooling((max_length_title-2,1),name='pooling_1')(h2_1)\n",
    "        h3_2=C.layers.MaxPooling((max_length_title-3,1),name='pooling_2')(h2_2)\n",
    "        h3_3=C.layers.MaxPooling((max_length_title-4,1),name='pooling_3')(h2_3)\n",
    "\n",
    "        h3=C.splice(h3_2,h3_1,h3_3,axis=0)\n",
    "        drop1 =C.layers.Dropout(0.5)(h3)\n",
    "        h4=C.layers.Dense(num_labels,name='hidden')(drop1)\n",
    "    return h4\n",
    "\n",
    "\n",
    "def batch_iter(data,batch_size, num_epochs, shuffle=True):\n",
    "    # Generates a batch iterator for a dataset.\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((data_size-1)/batch_size) + 1\n",
    "    print('data_size: ', data_size, 'batch_size: ', batch_size, 'num_batches_per_epoch: ', num_batches_per_epoch)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            random.shuffle(data)\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield data[start_index:end_index]\n",
    "            \n",
    "\n",
    "def fast_hist(a, b, n):\n",
    "    k = (a >= 0) & (a < n)\n",
    "    return np.bincount(n * a[k].astype(int) + b[k], minlength=n**2).reshape(n, n)\n",
    "\n",
    "title_dict =     { x:i for i,x in enumerate([x.strip(\"\\n\") for x in open(data_title_sample).readlines()])}\n",
    "industry_dict =  { x:i for i,x in enumerate([x.strip(\"\\n\") for x in open(data_industry_sample).readlines()])}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_xt = C.input_variable(shape=(max_length_title),  dtype=np.float32)\n",
    "#input_xt = C.input_variable(**Sequence[Tensor[1]])\n",
    "input_y  = C.input_variable(shape=(1)               ,  dtype=np.int)\n",
    "\n",
    "input_xt_one_hot = C.one_hot(input_xt, num_classes=len(title_dict)   ,  sparse_output=True)\n",
    "input_y_one_hot = C.one_hot(input_y  , num_classes=len(industry_dict) ,  sparse_output=True)\n",
    "\n",
    "\n",
    "test_data  = load_data(data_test_sample,title_dict,industry_dict)\n",
    "train_data = load_data(data_train_sample,title_dict,industry_dict)\n",
    "#test_data_editor  = load_data(data_test_sample_editor,title_dict,industry_dict)\n",
    "embedding = load_embedding(data_title_sample,\"word2vec.model\")\n",
    "def test(batch_size,model,data):\n",
    "    scores = model(input_xt)\n",
    "    predict = C.argmax(scores,axis = 0)\n",
    "    confuse = np.zeros((num_labels,num_labels))\n",
    "\n",
    "    test_data_title,test_data_label = data\n",
    "    batches = batch_iter(list(zip(test_data_title,test_data_label)), batch_size, 1)\n",
    "    \n",
    "    for batch in batches:\n",
    "        batch_data_title,batch_data_label = zip(*batch) \n",
    "        output = np.array(predict.eval({input_xt: np.array(batch_data_title)}),dtype=np.int)\n",
    "        gt = np.array(batch_data_label,dtype=np.int)\n",
    "        confuse+=fast_hist(output,gt,num_labels)\n",
    "        \n",
    "    precision=np.diag(confuse)/np.sum(confuse,axis=0)\n",
    "    recall = np.diag(confuse)/np.sum(confuse,axis=1)\n",
    "    accuarcy = np.diag(confuse).sum() / confuse.sum()\n",
    "    aver_precision=np.nanmean(precision)\n",
    "    aver_recall = np.nanmean(recall)\n",
    "   \n",
    "    print(\"Precision:{} Recall:{} Acc:{}\".format(aver_precision,aver_recall,accuarcy))\n",
    "    return accuarcy\n",
    "def train(train_data,num_epochs,learning_rate,batch_size,tag=\"CNN\",l2_weight=0):\n",
    "    global model\n",
    "    #learning_rate *= batch_size\n",
    "    model = create_model_cnn()\n",
    "    print(C.logging.get_node_outputs(model))\n",
    "    scores = model(input_xt)\n",
    "\n",
    "    loss =C.reduce_mean(C.losses.cross_entropy_with_softmax(scores, input_y_one_hot))\n",
    "    \n",
    "    # Training\n",
    "    lr_schedule = C.learning_parameter_schedule(learning_rate)\n",
    "    #learner = C.adam(scores.parameters, lr=lr_schedule, momentum=0.9,l2_regularization_weight=0)\n",
    "    progress_printer = C.logging.ProgressPrinter(tag='Training', num_epochs=num_epochs)\n",
    "    momentums = C.momentum_schedule(0.99, minibatch_size=batch_size)\n",
    "    learner = C.adam(parameters=scores.parameters,#model.parameters,\n",
    "                     lr=lr_schedule,\n",
    "                     momentum=momentums,\n",
    "                     gradient_clipping_threshold_per_sample=15,\n",
    "                     gradient_clipping_with_truncation=True,\n",
    "                     l2_regularization_weight=l2_weight)\n",
    "    trainer = C.Trainer(scores, (loss), [learner], progress_printer)\n",
    "    \n",
    "    train_data_title,train_data_label = train_data\n",
    "    batches = batch_iter(list(zip(train_data_title,train_data_label)), batch_size, num_epochs)\n",
    "\n",
    "    # training loop\n",
    "    count = 0\n",
    "    t = time.time()\n",
    "    for batch in batches:\n",
    "        count += 1\n",
    "        batch_data_title,batch_data_label = zip(*batch)\n",
    "        batch_data_title = list(batch_data_title)\n",
    "        #print(type(batch_data_title),type(batch_data_title[0]),batch_data_title[0])\n",
    "        trainer.train_minibatch({input_xt: np.array(batch_data_title), input_y: np.array(batch_data_label)})\n",
    "        if count%1000== 0:\n",
    "            print(count,time.time()-t)\n",
    "            t=time.time()\n",
    "            acc1=test(batch_size,model,test_data)\n",
    "            #acc2=test(batch_size,model,test_data_editor)\n",
    "            \n",
    "            # save model\n",
    "            #model.save('./model/{}/{}_acc{:.3f}.dnn'.format(suffix,tag,acc1))\n",
    "            #model.save('./model/{}/{}_acc1{:.3f}_acc2{:.3f}.dnn'.format(suffix,tag,acc1,acc2))\n",
    "    \n",
    "\n",
    "    # save model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_2: Composite(Tensor[53]) -> Tensor[53,300]\n",
      "Composite(Tensor[53]) -> Tensor[200,17]\n",
      "Composite(Tensor[53]) -> Tensor[200,1,15]\n",
      "Composite(Tensor[53]) -> Tensor[200,5]\n",
      "Composite(Tensor[53]) -> Tensor[200,1,3]\n",
      "Composite(Tensor[53]) -> Tensor[200]\n",
      "[Output('hidden', [#], [19]), Output('Block70987_Output_0', [#], [200]), Output('Squeeze70967_Output_0', [#], [200]), Output('pooling_3', [#], [200 x 1 x 1]), Output('Block70843_Output_0', [#], [200 x 1 x 3]), Output('Squeeze70800_Output_0', [#], [200 x 5]), Output('pooling_2', [#], [200 x 1 x 5]), Output('Block70706_Output_0', [#], [200 x 1 x 15]), Output('Squeeze70663_Output_0', [#], [200 x 17]), Output('pooling_1', [#], [200 x 17 x 1]), Output('Block70599_Output_0', [#], [200 x 51 x 1]), Output('embed_2', [#], [53 x 300]), Output('OneHotOp44323_Output_0', [#], [53 x 56178])]\n",
      "data_size:  768505 batch_size:  150 num_batches_per_epoch:  5124\n",
      "Learning rate per minibatch: 0.15\n",
      "1000 312.30674409866333\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t-haohu/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/ipykernel_launcher.py:197: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:0.27568854175095475 Recall:0.3522617569721096 Acc:0.514774107359728\n",
      "2000 335.62683153152466\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.5546113518234629 Recall:0.6768274626594606 Acc:0.729639300461501\n",
      "3000 316.52771401405334\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.643518098304333 Recall:0.7442812647265873 Acc:0.7787314792324508\n",
      "4000 303.17350792884827\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.6807864486675332 Recall:0.7459879850557136 Acc:0.8022710711683264\n",
      "5000 314.23911356925964\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7067684443036854 Recall:0.7822588960401098 Acc:0.8155361914015059\n",
      "6000 323.2549171447754\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7205712087316287 Recall:0.7630266954233402 Acc:0.8218453971338353\n",
      "7000 339.90049958229065\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7348917069523956 Recall:0.7783954966876985 Acc:0.8266000728685936\n",
      "8000 322.52984261512756\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7378344295203183 Recall:0.7917473295900461 Acc:0.8306442798153996\n",
      "9000 330.03780579566956\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7521885315935426 Recall:0.7944730849830846 Acc:0.8356236337138693\n",
      "10000 317.1898717880249\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7565543095814914 Recall:0.7988436195797124 Acc:0.837758076269128\n",
      "11000 305.76563692092896\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7585795021630778 Recall:0.7927445800032904 Acc:0.8348463687150838\n",
      "12000 323.1156644821167\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7585285703744807 Recall:0.7951457100529196 Acc:0.8364525139664805\n",
      "13000 317.12022280693054\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7658042144962427 Recall:0.7915049941132114 Acc:0.8373299732815157\n",
      "14000 323.4637076854706\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.767949087821177 Recall:0.7951643250996099 Acc:0.8403661646830216\n",
      "15000 313.907842874527\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7664445438168945 Recall:0.8030743378969287 Acc:0.841604930774836\n",
      "16000 328.781596660614\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7716321437762332 Recall:0.7892779170974594 Acc:0.8381770706825358\n",
      "17000 320.5315022468567\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7687165506963576 Recall:0.7948383765940455 Acc:0.8380799125576877\n",
      "18000 290.2338936328888\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7748497540213307 Recall:0.7909353014164903 Acc:0.8386628613067768\n",
      "19000 321.9287259578705\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7736414236504676 Recall:0.7924309493935126 Acc:0.8387296575176099\n",
      "20000 320.32913875579834\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7746872631413663 Recall:0.7933299623910659 Acc:0.840044328394462\n",
      "21000 336.8133373260498\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7716200472385735 Recall:0.7925068820501892 Acc:0.8379493563274228\n",
      "22000 263.3572494983673\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7695559280994241 Recall:0.7879248136430052 Acc:0.8356509594364828\n",
      "23000 257.0367271900177\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7668583374137248 Recall:0.7958170906142005 Acc:0.8370961865435997\n",
      "24000 344.8366687297821\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7679349901480023 Recall:0.7953220807849001 Acc:0.8362885596307992\n",
      "25000 414.35377192497253\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7725367061302707 Recall:0.7882194541142811 Acc:0.8367257711926159\n",
      "26000 432.488484621048\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7648730778256565 Recall:0.7956814359573027 Acc:0.8347340296332281\n",
      "27000 420.56949186325073\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7708798503231161 Recall:0.7875139218026954 Acc:0.8348676220548943\n",
      "28000 413.94418954849243\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7671033849172136 Recall:0.7878850965563295 Acc:0.8337806655331552\n",
      "29000 439.63245606422424\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7676483590084057 Recall:0.7870525539566968 Acc:0.8332948749089143\n",
      "30000 458.381742477417\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7714476943743555 Recall:0.782159556003701 Acc:0.8335590235608453\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/logging/progress_print.py\u001b[0m in \u001b[0;36mon_training_update_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m___write_progress_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregate_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregate_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mon_training_update_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0;31m# Override for ProgressWriter.on_training_update_end.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m___generate_progress_heartbeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "SWIG director method error.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-bf4318341f15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"DeepCNN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-40-426f99456e0f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_data, num_epochs, learning_rate, batch_size, tag, l2_weight)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mbatch_data_title\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data_title\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;31m#print(type(batch_data_title),type(batch_data_title[0]),batch_data_title[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minput_xt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data_title\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_y\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/train/trainer.py\u001b[0m in \u001b[0;36mtrain_minibatch\u001b[0;34m(self, arguments, outputs, device, is_sweep_end)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 updated = super(Trainer, self).train_minibatch(arguments, is_sweep_end,\n\u001b[0;32m--> 184\u001b[0;31m                     device)\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/cntk/cntk_py.py\u001b[0m in \u001b[0;36mtrain_minibatch\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3027\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_cntk_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer_train_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3029\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: SWIG director method error."
     ]
    }
   ],
   "source": [
    "train(train_data,num_epochs=20,learning_rate=[5e-3*150]*2+[1e-4*150],batch_size = 150,tag = \"DeepCNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
