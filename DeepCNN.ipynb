{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function # Use a function definition from future version (say 3.x from 2.7 interpreter)\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import time \n",
    "\n",
    "import cntk as C\n",
    "import cntk.tests.test_utils\n",
    "from cntk.layers import *\n",
    "from cntk.layers.typing import *\n",
    "import pickle\n",
    "import random\n",
    "from cntk import sequence\n",
    "from cntk import load_model\n",
    "from cntk.device import try_set_default_device, gpu,cpu\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "cntk.tests.test_utils.set_device_from_pytest_env() # (only needed for our build system)\n",
    "C.cntk_py.set_fixed_random_seed(1) # fix a random seed for CNTK components\n",
    "try_set_default_device(gpu(0))\n",
    "\n",
    "\n",
    "vocab_size = 80000\n",
    "num_labels = 19#19\n",
    "title_size = 52000\n",
    "body_size  = 210000\n",
    "input_dim  = vocab_size\n",
    "label_dim  = num_labels\n",
    "emb_dim    = 300\n",
    "hidden_dim = 200\n",
    "\n",
    "max_length_title = 53\n",
    "max_length_body  = 200\n",
    "\n",
    "suffix = \"180days_all_shuffled\"\n",
    "#suffix = \"linkedin_only\"\n",
    "prefix = \"/home/t-haohu/IndustryClassifier/Data/\"\n",
    "\n",
    "#data_token_body        = \"{}/middle/{}_token_body.txt\".format(prefix,suffix)\n",
    "data_train_sample = \"{}/middle/train_{}.txt\".format(prefix,suffix)\n",
    "#data_train_sample = \"{}/middle/train_{}_with_linkedin_all.txt\".format(prefix,suffix)\n",
    "data_test_sample  = \"{}/middle/test_{}.txt\".format(prefix,suffix)\n",
    "#data_test_sample_editor  = \"{}/middle/test_{}_editor.txt\".format(prefix,suffix)\n",
    "\n",
    "data_title_sample    = \"{}/ready/title_{}.wl\".format(prefix,suffix)\n",
    "data_body_sample     = \"{}/ready/body_{}.wl\".format(prefix,suffix)\n",
    "suffix = \"180days_all_shuffled\"\n",
    "data_industry_sample = \"{}/ready/industry_{}.wl\".format(prefix,suffix)\n",
    "filter_num=200 \n",
    "dropout_rate = 0.5\n",
    "emb_dim =300\n",
    "\n",
    "def load_data(input_file,title_dict,industry_dict):\n",
    "    data = open(input_file, encoding = \"utf-8\").readlines()\n",
    "    \n",
    "    data_title =np.zeros((len(data),max_length_title),dtype = np.float32)\n",
    "    data_label = np.zeros((len(data),1),dtype = np.float32)\n",
    "       \n",
    "    for index,line in enumerate(data):\n",
    "        row = line.strip(\"\\n\").split(\"\\t\")       \n",
    "        title    =  row[0]\n",
    "        industry =  row[1]\n",
    "        \n",
    "        for jndex,token in enumerate(title.split(\" \")):\n",
    "            if jndex>=max_length_title:\n",
    "                break\n",
    "            data_title[index,jndex]=title_dict.get(token,len(title_dict)-1)    \n",
    "        data_label[index] = industry_dict.get(industry,len(industry_dict))\n",
    "    return data_title,data_label\n",
    "\n",
    "\n",
    "def load_embedding(title_file,embedding_model_file):\n",
    "    model = Word2Vec.load(embedding_model_file)\n",
    "    title_list = [x.strip(\"\\n\") for x in open(title_file,encoding = 'utf-8').readlines()]\n",
    "    embedding = np.zeros((len(title_list),emb_dim))\n",
    "    count = 0\n",
    "    for i,w in enumerate(title_list):\n",
    "        try:\n",
    "            vec = model.wv[w]\n",
    "        except:\n",
    "            vec=model.wv[\"UNK\"]\n",
    "            count+=1\n",
    "        embedding[i] =vec\n",
    "    print(count)\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def create_model_cnn(embed = False):\n",
    "    #version 2 : 1 dense layer version3: sigmoid activation in dense\n",
    "    if embed:\n",
    "        h1= C.layers.Embedding(weights=embedding,name='embed_1')(input_xt_one_hot)#\n",
    "    else:\n",
    "        h1= C.layers.Embedding(emb_dim,name='embed_2')(input_xt_one_hot)#init=embedding,\n",
    "    print(h1)\n",
    "    h2=C.layers.Convolution((3,emb_dim),num_filters=filter_num,reduction_rank=0,activation=C.relu)(h1)\n",
    "    h3=C.squeeze(C.layers.MaxPooling((3,1),strides =3,name='pooling_1')(h2))\n",
    "    print(h3)\n",
    "    #h2=BiRecurrence(C.layers.LSTM(hidden_dim), C.layers.LSTM(hidden_dim))(h1)\n",
    "    h4 = C.layers.Convolution((filter_num,3),num_filters=filter_num,reduction_rank=0,activation=C.relu)(h3)\n",
    "    print(h4)\n",
    "    h5 = C.squeeze(C.layers.MaxPooling((1,3),strides =3,name='pooling_2')(h4))\n",
    "    print(h5)\n",
    "    h6 = C.layers.Convolution((filter_num,3),num_filters=filter_num,reduction_rank=0,activation=C.relu)(h5)\n",
    "    print(h6)\n",
    "    h7 = C.squeeze(C.layers.MaxPooling((1,3),strides =3,name='pooling_3')(h6))\n",
    "    print(h7)\n",
    "    drop1 = C.layers.Dropout(dropout_rate)(h7)\n",
    "    #h8=C.layers.Dense(hidden_dim,name='hidden')(drop1)\n",
    "    logits = C.layers.Dense(num_labels,name='hidden')(drop1)\n",
    "    return logits\n",
    "\n",
    "def create_model_cnn_2fold():\n",
    "    #version 2 : 1 dense layer version3: sigmoid activation in dense\n",
    "    #\n",
    "    with C.layers.default_options(initial_state=0.1):\n",
    "\n",
    "\n",
    "        h1_1= C.layers.Embedding(weights=embedding,name='embed_1')(input_xt_one_hot)#\n",
    "        h1_2= C.layers.Embedding(300,name='embed_2')(input_xt_one_hot)#init=embedding,\n",
    "        \n",
    "        h1_1_expand = C.expand_dims(h1_1,-3)\n",
    "        h1_2_expand = C.expand_dims(h1_2,-3)\n",
    "        \n",
    "        h1 = C.splice(h1_1_expand,h1_2_expand,axis = -3)\n",
    "   #value,valid = to_static(h1)\n",
    "\n",
    "        filter_num=100\n",
    "\n",
    "        h2_1=C.layers.Convolution((3,emb_dim),num_filters=filter_num,reduction_rank=1,activation=C.relu)(h1)\n",
    "        h2_2=C.layers.Convolution((4,emb_dim),num_filters=filter_num,reduction_rank=1,activation=C.relu)(h1)\n",
    "        h2_3=C.layers.Convolution((5,emb_dim),num_filters=filter_num,reduction_rank=1,activation=C.relu)(h1)\n",
    "\n",
    "        h3_1=C.layers.MaxPooling((max_length_title-2,1),name='pooling_1')(h2_1)\n",
    "        h3_2=C.layers.MaxPooling((max_length_title-3,1),name='pooling_2')(h2_2)\n",
    "        h3_3=C.layers.MaxPooling((max_length_title-4,1),name='pooling_3')(h2_3)\n",
    "\n",
    "        h3=C.splice(h3_2,h3_1,h3_3,axis=0)\n",
    "        drop1 =C.layers.Dropout(0.5)(h3)\n",
    "        h4=C.layers.Dense(num_labels,name='hidden')(drop1)\n",
    "    return h4\n",
    "\n",
    "\n",
    "def batch_iter(data,batch_size, num_epochs, shuffle=True):\n",
    "    # Generates a batch iterator for a dataset.\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((data_size-1)/batch_size) + 1\n",
    "    print('data_size: ', data_size, 'batch_size: ', batch_size, 'num_batches_per_epoch: ', num_batches_per_epoch)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            random.shuffle(data)\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield data[start_index:end_index]\n",
    "            \n",
    "\n",
    "def fast_hist(a, b, n):\n",
    "    k = (a >= 0) & (a < n)\n",
    "    return np.bincount(n * a[k].astype(int) + b[k], minlength=n**2).reshape(n, n)\n",
    "\n",
    "title_dict =     { x:i for i,x in enumerate([x.strip(\"\\n\") for x in open(data_title_sample).readlines()])}\n",
    "industry_dict =  { x:i for i,x in enumerate([x.strip(\"\\n\") for x in open(data_industry_sample).readlines()])}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_xt = C.input_variable(shape=(max_length_title),  dtype=np.float32)\n",
    "#input_xt = C.input_variable(**Sequence[Tensor[1]])\n",
    "input_y  = C.input_variable(shape=(1)               ,  dtype=np.int)\n",
    "\n",
    "input_xt_one_hot = C.one_hot(input_xt, num_classes=len(title_dict)   ,  sparse_output=True)\n",
    "input_y_one_hot = C.one_hot(input_y  , num_classes=len(industry_dict) ,  sparse_output=True)\n",
    "\n",
    "\n",
    "test_data  = load_data(data_test_sample,title_dict,industry_dict)\n",
    "train_data = load_data(data_train_sample,title_dict,industry_dict)\n",
    "#test_data_editor  = load_data(data_test_sample_editor,title_dict,industry_dict)\n",
    "embedding = load_embedding(data_title_sample,\"word2vec.model\")\n",
    "def test(batch_size,model,data):\n",
    "    scores = model(input_xt)\n",
    "    predict = C.argmax(scores,axis = 0)\n",
    "    confuse = np.zeros((num_labels,num_labels))\n",
    "\n",
    "    test_data_title,test_data_label = data\n",
    "    batches = batch_iter(list(zip(test_data_title,test_data_label)), batch_size, 1)\n",
    "    \n",
    "    for batch in batches:\n",
    "        batch_data_title,batch_data_label = zip(*batch) \n",
    "        output = np.array(predict.eval({input_xt: np.array(batch_data_title)}),dtype=np.int)\n",
    "        gt = np.array(batch_data_label,dtype=np.int)\n",
    "        confuse+=fast_hist(output,gt,num_labels)\n",
    "        \n",
    "    precision=np.diag(confuse)/np.sum(confuse,axis=0)\n",
    "    recall = np.diag(confuse)/np.sum(confuse,axis=1)\n",
    "    accuarcy = np.diag(confuse).sum() / confuse.sum()\n",
    "    aver_precision=np.nanmean(precision)\n",
    "    aver_recall = np.nanmean(recall)\n",
    "   \n",
    "    print(\"Precision:{} Recall:{} Acc:{}\".format(aver_precision,aver_recall,accuarcy))\n",
    "    return accuarcy\n",
    "def train(train_data,num_epochs,learning_rate,batch_size,tag=\"CNN\",l2_weight=0):\n",
    "    global model\n",
    "    #learning_rate *= batch_size\n",
    "    model = create_model_cnn()\n",
    "    print(C.logging.get_node_outputs(model))\n",
    "    scores = model(input_xt)\n",
    "\n",
    "    loss =C.reduce_mean(C.losses.cross_entropy_with_softmax(scores, input_y_one_hot))\n",
    "    \n",
    "    # Training\n",
    "    lr_schedule = C.learning_parameter_schedule(learning_rate)\n",
    "    #learner = C.adam(scores.parameters, lr=lr_schedule, momentum=0.9,l2_regularization_weight=0)\n",
    "    progress_printer = C.logging.ProgressPrinter(tag='Training', num_epochs=num_epochs)\n",
    "    momentums = C.momentum_schedule(0.99, minibatch_size=batch_size)\n",
    "    learner = C.adam(parameters=scores.parameters,#model.parameters,\n",
    "                     lr=lr_schedule,\n",
    "                     momentum=momentums,\n",
    "                     gradient_clipping_threshold_per_sample=15,\n",
    "                     gradient_clipping_with_truncation=True,\n",
    "                     l2_regularization_weight=l2_weight)\n",
    "    trainer = C.Trainer(scores, (loss), [learner], progress_printer)\n",
    "    \n",
    "    train_data_title,train_data_label = train_data\n",
    "    batches = batch_iter(list(zip(train_data_title,train_data_label)), batch_size, num_epochs)\n",
    "\n",
    "    # training loop\n",
    "    count = 0\n",
    "    t = time.time()\n",
    "    for batch in batches:\n",
    "        count += 1\n",
    "        batch_data_title,batch_data_label = zip(*batch)\n",
    "        batch_data_title = list(batch_data_title)\n",
    "        #print(type(batch_data_title),type(batch_data_title[0]),batch_data_title[0])\n",
    "        trainer.train_minibatch({input_xt: np.array(batch_data_title), input_y: np.array(batch_data_label)})\n",
    "        if count%1000== 0:\n",
    "            print(count,time.time()-t)\n",
    "            t=time.time()\n",
    "            acc1=test(batch_size,model,test_data)\n",
    "            #acc2=test(batch_size,model,test_data_editor)\n",
    "            \n",
    "            # save model\n",
    "            model.save('./model/{}/{}_acc{:.3f}.dnn'.format(suffix,tag,acc1))\n",
    "            #model.save('./model/{}/{}_acc1{:.3f}_acc2{:.3f}.dnn'.format(suffix,tag,acc1,acc2))\n",
    "    \n",
    "\n",
    "    # save model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_2: Composite(Tensor[53]) -> Tensor[53,300]\n",
      "Composite(Tensor[53]) -> Tensor[200,17]\n",
      "Composite(Tensor[53]) -> Tensor[200,1,15]\n",
      "Composite(Tensor[53]) -> Tensor[200,5]\n",
      "Composite(Tensor[53]) -> Tensor[200,1,3]\n",
      "Composite(Tensor[53]) -> Tensor[200]\n",
      "[Output('hidden', [#], [19]), Output('Block467_Output_0', [#], [200]), Output('Squeeze447_Output_0', [#], [200]), Output('pooling_3', [#], [200 x 1 x 1]), Output('Block323_Output_0', [#], [200 x 1 x 3]), Output('Squeeze280_Output_0', [#], [200 x 5]), Output('pooling_2', [#], [200 x 1 x 5]), Output('Block186_Output_0', [#], [200 x 1 x 15]), Output('Squeeze143_Output_0', [#], [200 x 17]), Output('pooling_1', [#], [200 x 17 x 1]), Output('Block79_Output_0', [#], [200 x 51 x 1]), Output('embed_2', [#], [53 x 300]), Output('OneHotOp5_Output_0', [#], [53 x 56178])]\n",
      "data_size:  768505 batch_size:  150 num_batches_per_epoch:  5124\n",
      "Learning rate per minibatch: 0.075\n",
      "1000 395.11466217041016\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t-haohu/anaconda3/envs/cntk-py35/lib/python3.5/site-packages/ipykernel_launcher.py:197: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:0.11568188416023845 Recall:0.31713140950901714 Acc:0.33024653874180226\n",
      "2000 464.9960551261902\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.32333796125933867 Recall:0.4414429827028993 Acc:0.5739768034976925\n",
      "3000 459.36325573921204\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.5446737076081135 Recall:0.6495704613583734 Acc:0.7347886810784552\n",
      "4000 469.9437847137451\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.6457276577045178 Recall:0.7437508973532624 Acc:0.7842755647316006\n",
      "5000 458.44051790237427\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.6993789015861225 Recall:0.7785658015061736 Acc:0.8093727228564489\n",
      "6000 375.83518743515015\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.714438190337679 Recall:0.7747835829005495 Acc:0.8141729414622297\n",
      "7000 462.87565302848816\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7285600888888999 Recall:0.7877447517274712 Acc:0.8236276414865193\n",
      "8000 466.74100375175476\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7365506623718497 Recall:0.7874713684144966 Acc:0.8267458100558659\n",
      "9000 449.4772000312805\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7404982679211517 Recall:0.7845409147249207 Acc:0.8324690308477046\n",
      "10000 414.5603733062744\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7457826377057412 Recall:0.8098479228622032 Acc:0.8374574933203789\n",
      "11000 451.7221713066101\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.747817749923344 Recall:0.7999854769175778 Acc:0.8353443041049308\n",
      "12000 473.8542401790619\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7504984380895909 Recall:0.7995374432862336 Acc:0.8359849404906485\n",
      "13000 427.983927488327\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.763075876982944 Recall:0.7880846324821631 Acc:0.8384928345882925\n",
      "14000 458.9204788208008\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7638957966833058 Recall:0.7962041714611828 Acc:0.8411312849162011\n",
      "15000 410.4748077392578\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7629552445318346 Recall:0.8048266217279427 Acc:0.8425613310663104\n",
      "16000 452.69899821281433\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7639833070835842 Recall:0.7972051067837268 Acc:0.8397498178285159\n",
      "17000 478.82267141342163\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.762561891098242 Recall:0.7961739932283034 Acc:0.8388966480446928\n",
      "18000 478.92170453071594\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7725000464438989 Recall:0.7926414553345423 Acc:0.8393976196259412\n",
      "19000 446.7314553260803\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7725533513322115 Recall:0.7900598603091733 Acc:0.8401293417537041\n",
      "20000 418.8714768886566\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7698992452132665 Recall:0.801394305219484 Acc:0.8427374301675977\n",
      "21000 426.693932056427\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.770234837863161 Recall:0.7968731883541134 Acc:0.8410037648773379\n",
      "22000 469.99599146842957\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7649821290024752 Recall:0.795700764991626 Acc:0.8369656303133349\n",
      "23000 473.8093237876892\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7664716438509211 Recall:0.7946428726861853 Acc:0.8373967694923488\n",
      "24000 465.2667484283447\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7721043150636158 Recall:0.7915957861669285 Acc:0.8382347583191644\n",
      "25000 455.8822958469391\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7734096463853334 Recall:0.7875491526846766 Acc:0.8381983240223464\n",
      "26000 411.605961561203\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7729350168657687 Recall:0.7890843595681917 Acc:0.8363675006072383\n",
      "27000 428.53445982933044\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7674116658238299 Recall:0.7914960024745437 Acc:0.835975831916444\n",
      "28000 471.44227409362793\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7673914045330893 Recall:0.7894649656353753 Acc:0.8345670391061453\n",
      "29000 469.1760365962982\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7671321948228152 Recall:0.7910042040501617 Acc:0.8356904299247024\n",
      "30000 452.367972612381\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7716902497579904 Recall:0.7898654019702859 Acc:0.8351530240466359\n",
      "31000 416.9327721595764\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7675490999338187 Recall:0.7918385014420796 Acc:0.8335499149866408\n",
      "32000 465.5400114059448\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7684612153250042 Recall:0.7877993291333628 Acc:0.8337776293417537\n",
      "33000 485.6374862194061\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7680564354497649 Recall:0.7865718670389115 Acc:0.8333009472917172\n",
      "34000 436.9835698604584\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7695391168019525 Recall:0.7861509423890845 Acc:0.8349981782851591\n",
      "35000 435.59504675865173\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7666872362515378 Recall:0.7900488907823352 Acc:0.833941583677435\n",
      "36000 418.2389042377472\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7696061886956221 Recall:0.7874676177862434 Acc:0.8325813699295603\n",
      "37000 455.3930265903473\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7640293819139294 Recall:0.7901006653864172 Acc:0.8326846004372116\n",
      "38000 487.74027943611145\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7701148670920874 Recall:0.7776668029034963 Acc:0.8298275443283945\n",
      "39000 459.7848060131073\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7705071429777108 Recall:0.7829811820709882 Acc:0.8307626912800583\n",
      "40000 446.3690376281738\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7691688034923407 Recall:0.7801129840489874 Acc:0.8320743259655089\n",
      "41000 421.5245850086212\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.765198159913474 Recall:0.7855615622239785 Acc:0.8317494534855477\n",
      "42000 424.8386478424072\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7619346583641664 Recall:0.7858211562725499 Acc:0.827292324508137\n",
      "43000 461.5121123790741\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7670133257372688 Recall:0.7807014147205841 Acc:0.8282973038620355\n",
      "44000 471.891241312027\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.765822494533423 Recall:0.7829381183071509 Acc:0.8296878795239252\n",
      "45000 434.17060470581055\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7680618728930391 Recall:0.7794097428552578 Acc:0.8310723828030119\n",
      "46000 417.3277838230133\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7714026016169374 Recall:0.7721493174227919 Acc:0.8301827787223707\n",
      "47000 467.3077702522278\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7662745257586031 Recall:0.7811082733419773 Acc:0.8290776050522225\n",
      "48000 466.4634566307068\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7633214083932298 Recall:0.7836049980465695 Acc:0.827820621811999\n",
      "49000 432.3454899787903\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7658384602792098 Recall:0.784783226637506 Acc:0.8307292931746417\n",
      "50000 402.14457964897156\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7592534213389455 Recall:0.7854754432777146 Acc:0.8278661646830217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51000 458.2557575702667\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.762288361148806 Recall:0.7794040497761026 Acc:0.8268581491377216\n",
      "52000 445.0088653564453\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7606745998669724 Recall:0.7860599951952074 Acc:0.8271374787466602\n",
      "53000 482.72023367881775\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7602552760038028 Recall:0.7819526123841675 Acc:0.826782244352684\n",
      "54000 474.8390634059906\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7622353934004938 Recall:0.7844927443874538 Acc:0.8296969880981298\n",
      "55000 412.5068564414978\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.76443925729589 Recall:0.7819506313879447 Acc:0.8267154481418508\n",
      "56000 424.58861541748047\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7621224687021954 Recall:0.7786533098083885 Acc:0.8247844304104931\n",
      "57000 454.8973104953766\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.758632420109693 Recall:0.784809567875973 Acc:0.8261476803497693\n",
      "58000 475.28238582611084\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7614998215605923 Recall:0.7815106751020998 Acc:0.8257651202331795\n",
      "59000 481.9409158229828\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.76089801694787 Recall:0.7848067499371867 Acc:0.8283458829244595\n",
      "60000 408.1175727844238\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7612911634935102 Recall:0.7776680446165253 Acc:0.8253582705853777\n",
      "61000 448.24986958503723\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7629844135180172 Recall:0.773870569368051 Acc:0.822689458343454\n",
      "62000 466.5013818740845\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7589180999791653 Recall:0.7798709695235694 Acc:0.8238067767792082\n",
      "63000 437.05304050445557\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7621427349313395 Recall:0.7746054208644381 Acc:0.8245597522467817\n",
      "64000 473.19665908813477\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7601821572884584 Recall:0.7765599442884037 Acc:0.8254068496478018\n",
      "65000 410.647741317749\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7571988864546072 Recall:0.7811709208808636 Acc:0.8236610395919359\n",
      "66000 459.8695070743561\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7583333487886591 Recall:0.7812715073003017 Acc:0.823685329123148\n",
      "67000 482.76634335517883\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7618310519040448 Recall:0.7786698432932662 Acc:0.8247115618168569\n",
      "68000 461.8400340080261\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7592925276849474 Recall:0.7834396195316663 Acc:0.826524168083556\n",
      "69000 462.4468026161194\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7547456551382669 Recall:0.7775470709536472 Acc:0.8191219334466845\n",
      "70000 403.2477810382843\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7563801911910263 Recall:0.7750716882726971 Acc:0.8233756376001943\n",
      "71000 431.6155982017517\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7584321127407753 Recall:0.7800089777512695 Acc:0.8218332523682292\n",
      "72000 452.83821177482605\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7567989483797459 Recall:0.7844398289955671 Acc:0.8252064610153024\n",
      "73000 469.0240099430084\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7618691407119929 Recall:0.7724416200725068 Acc:0.8227653631284916\n",
      "74000 478.4908404350281\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7629883390806069 Recall:0.7745661889619297 Acc:0.8252125333981054\n",
      "75000 428.84798765182495\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7547405831966718 Recall:0.7843790363506317 Acc:0.8243320378916686\n",
      "76000 457.6161103248596\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7633208557231604 Recall:0.772290582463751 Acc:0.8242288073840175\n",
      "77000 456.9863796234131\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.762320663146114 Recall:0.7731850864545813 Acc:0.8240344911343211\n",
      "78000 438.3092875480652\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7605396523151245 Recall:0.7779542785053998 Acc:0.8249787466601894\n",
      "79000 471.5551688671112\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7608904726704643 Recall:0.7772500358089454 Acc:0.8257013602137478\n",
      "80000 423.94507026672363\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7595396642912473 Recall:0.7739069208107441 Acc:0.8230082584406121\n",
      "81000 448.60952401161194\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7571027633644686 Recall:0.7799367303847402 Acc:0.8241377216419723\n",
      "82000 455.13334226608276\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7576683289405198 Recall:0.7728038011645179 Acc:0.8218271799854263\n",
      "83000 480.8815686702728\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7607618121546968 Recall:0.7743472378624804 Acc:0.8253036191401506\n",
      "84000 456.0115485191345\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.761674585347016 Recall:0.7685161564612928 Acc:0.8229262812727715\n",
      "85000 378.48479533195496\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7537652622503866 Recall:0.7765260641427748 Acc:0.8214931989312606\n",
      "86000 461.13134598731995\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7623943058797169 Recall:0.7691843927656214 Acc:0.8217846733058052\n",
      "87000 444.00718450546265\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7566638805205104 Recall:0.7712942501839174 Acc:0.8189883410250182\n",
      "88000 459.57934284210205\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7553757049576906 Recall:0.7750502611228974 Acc:0.8236762205489434\n",
      "89000 432.15387654304504\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7606587246579329 Recall:0.7690127706964048 Acc:0.8244747388875394\n",
      "90000 431.7916588783264\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7584718358073946 Recall:0.7711180326011176 Acc:0.8227653631284916\n",
      "91000 472.5080931186676\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7542911882102062 Recall:0.7799145685612822 Acc:0.8236519310177314\n",
      "92000 419.68675684928894\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7541693465451416 Recall:0.7739713906473147 Acc:0.8183719941705125\n",
      "93000 448.2603988647461\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7528795714413249 Recall:0.7799580100344183 Acc:0.8238128491620111\n",
      "94000 400.6327908039093\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7576444562510132 Recall:0.774459108007937 Acc:0.824754068496478\n",
      "95000 448.49142718315125\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.760012593043423 Recall:0.7722357362804724 Acc:0.8239069710954579\n",
      "96000 470.7534604072571\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7550403821627851 Recall:0.7756393158893364 Acc:0.8215326694194802\n",
      "97000 472.66056275367737\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7536419269539196 Recall:0.7814663123936618 Acc:0.8249666018945835\n",
      "98000 471.7507743835449\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7602949866935165 Recall:0.7716945053419075 Acc:0.8218484333252368\n",
      "99000 354.6777129173279\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7580903904379187 Recall:0.7725068811986607 Acc:0.822461743988341\n",
      "100000 454.84966135025024\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:0.7576715000502651 Recall:0.772451496320058 Acc:0.821657153266942\n",
      "101000 455.4182951450348\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.755478167334676 Recall:0.7775382514180663 Acc:0.8225801554529998\n",
      "102000 473.5091173648834\n",
      "data_size:  329360 batch_size:  150 num_batches_per_epoch:  2196\n",
      "Precision:0.7614014442276719 Recall:0.7697638715350259 Acc:0.8184630799125577\n"
     ]
    }
   ],
   "source": [
    "train(train_data,num_epochs=20,learning_rate=[5e-4*150]*2+[1e-4*150],batch_size = 150,tag = \"DeepCNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
