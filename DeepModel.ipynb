{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing deep_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile deep_model.py\n",
    "import pandas as pd\n",
    "# Import the relevant libraries\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "from __future__ import print_function # Use a function definition from future version (say 3.x from 2.7 interpreter)\n",
    "\n",
    "import cntk as C\n",
    "import cntk.tests.test_utils\n",
    "import pickle\n",
    "from cntk import sequence\n",
    "from cntk import load_model\n",
    "\n",
    "cntk.tests.test_utils.set_device_from_pytest_env() # (only needed for our build system)\n",
    "C.cntk_py.set_fixed_random_seed(1) # fix a random seed for CNTK components\n",
    "\n",
    "\n",
    "from cntk.device import try_set_default_device, gpu,cpu\n",
    "try_set_default_device(gpu(0))\n",
    "\n",
    "# number of words in vocab, slot labels, and intent labels\n",
    "vocab_size = 80000; num_labels = 19\n",
    "title_size = 52000\n",
    "body_size = 210000\n",
    "# model dimensions\n",
    "input_dim  = vocab_size\n",
    "label_dim  = num_labels\n",
    "emb_dim    = 300\n",
    "hidden_dim = 200\n",
    "max_length = 50\n",
    "model = 'cnn'\n",
    "embed_file = None\n",
    "industry=[i.rstrip(\"\\n\") for i in open(\"news/industry.wl\").readlines()]\n",
    "\n",
    "if embed_file:\n",
    "    with open(embedfile,'rb') as handle:\n",
    "        embedding=pickle.load(handle)\n",
    "    \n",
    "# Create the containers for input feature (x) and the label (y)\n",
    "i1_axis = C.Axis.new_unique_dynamic_axis('1')\n",
    "i2_axis = C.Axis.new_unique_dynamic_axis('2')\n",
    "xb = C.sequence.input(shape=body_size, is_sparse=True, sequence_axis=i1_axis, name='xb')\n",
    "xt = C.sequence.input(shape=title_size, is_sparse=True, sequence_axis=i2_axis, name='xt')\n",
    "x = C.sequence.input_variable(vocab_size)\n",
    "\n",
    "y = C.input_variable(num_labels)\n",
    "\n",
    "def BiRecurrence(fwd, bwd):\n",
    "    F = C.layers.Recurrence(fwd)\n",
    "    G = C.layers.Recurrence(bwd, go_backwards=True)\n",
    "    x = C.placeholder()\n",
    "    apply_x = C.splice(sequence.last(F(x)), sequence.first(G(x)),name='h2')\n",
    "    return apply_x\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    return {\n",
    "        'cnn':create_model_cnn(),\n",
    "        'gru':create_model_gru(),\n",
    "        'lstm':create_model_lstm(),\n",
    "        'cnn_body':create_model_cnn_body(),\n",
    "        'cnn_2fold':create_model_cnn_2fold()\n",
    "    }[model]\n",
    "def create_model_gru(embed=False):\n",
    "    with C.layers.default_options(initial_state=0.1):\n",
    "        if embed:\n",
    "            h1= C.layers.Sequential([\n",
    "            C.layers.Embedding(emb_dim,name='embed',init=embedding),\n",
    "            C.layers.BatchNormalization(),\n",
    "            C.layers.Stabilizer()])\n",
    "        else:\n",
    "            h1= C.layers.Sequential([\n",
    "            C.layers.Embedding(emb_dim,name='embed'),\n",
    "            C.layers.BatchNormalization(),\n",
    "            C.layers.Stabilizer()])\n",
    "\n",
    "        h2=BiRecurrence(C.layers.GRU(hidden_dim),C.layers.GRU(hidden_dim))(h1)\n",
    "        #h3=C.sequence.last(h2)\n",
    "        h4=C.layers.Dense(num_labels, name='classify')(h2)\n",
    "    return h4\n",
    "def create_model_lstm(embed = False):\n",
    "    with C.layers.default_options(initial_state=0.1):\n",
    "        if embed:\n",
    "            h1= C.layers.Sequential([\n",
    "            C.layers.Embedding(emb_dim,name='embed',init=embedding),\n",
    "            C.layers.BatchNormalization(),\n",
    "            C.layers.Stabilizer()])\n",
    "        else:\n",
    "            h1= C.layers.Sequential([\n",
    "            C.layers.Embedding(emb_dim,name='embed'),\n",
    "            C.layers.BatchNormalization(),\n",
    "            C.layers.Stabilizer()])       \n",
    "        h2=BiRecurrence(C.layers.LSTM(hidden_dim),C.layers.LSTM(hidden_dim))(h1)\n",
    "        h4=C.layers.Dense(num_labels, name='classify')(h2)\n",
    "    return h4\n",
    "def create_model_cnn_2fold():\n",
    "    #version 2 : 1 dense layer version3: sigmoid activation in dense\n",
    "    #\n",
    "    with C.layers.default_options(initial_state=0.1):\n",
    "\n",
    "\n",
    "        #h1_1= C.layers.Embedding(weights=embedding,name='embed')(x)#\n",
    "        h1_1= C.layers.Embedding(300,name='embed')(x)\n",
    "        h1_2= C.layers.Embedding(300,name='embed')(x)#init=embedding,\n",
    "        h1 = C.splice(h1_1,h1_2,axis = 0)\n",
    "        #bn = C.layers.BatchNormalization(name='bn')(h1)\n",
    "        to_static= C.layers.PastValueWindow(window_size=max_length, axis=-2)(h1)[0]\n",
    "\n",
    "        #value,valid = to_static(h1)\n",
    "\n",
    "        filter_num=100\n",
    "\n",
    "        h2_1=C.layers.Convolution((3,emb_dim),num_filters=filter_num,reduction_rank=0,activation=C.relu)(to_static)\n",
    "        h2_2=C.layers.Convolution((4,emb_dim),num_filters=filter_num,reduction_rank=0,activation=C.relu)(to_static)\n",
    "        h2_3=C.layers.Convolution((5,emb_dim),num_filters=filter_num,reduction_rank=0,activation=C.relu)(to_static)\n",
    "\n",
    "        h3_1=C.layers.MaxPooling((max_length-2,1),name='pooling')(h2_1)\n",
    "        h3_2=C.layers.MaxPooling((max_length-3,1),name='pooling')(h2_2)\n",
    "        h3_3=C.layers.MaxPooling((max_length-4,1),name='pooling')(h2_3)\n",
    "\n",
    "        h3=C.splice(h3_2,h3_1,h3_3,axis=0)\n",
    "        drop1 =C.layers.Dropout(0.5)(h3)\n",
    "        h4=C.layers.Dense(num_labels,name='hidden')(drop1)\n",
    "\n",
    "    return h4\n",
    "\n",
    "def create_model_cnn(embed = False):\n",
    "    #version 2 : 1 dense layer version3: sigmoid activation in dense\n",
    "    #\n",
    "    with C.layers.default_options(initial_state=0.1):\n",
    "\n",
    "       \n",
    "        if embed:\n",
    "            h1= C.layers.Embedding(300,init=embedding,name='embed')(x)#\n",
    "        else:\n",
    "            h1= C.layers.Embedding(300,name='embed')(x)#init=embedding,\n",
    "        \n",
    "        #bn = C.layers.BatchNormalization(name='bn')(h1)\n",
    "        to_static= C.layers.PastValueWindow(window_size=max_length, axis=-2)(h1)[0]\n",
    "\n",
    "        #value,valid = to_static(h1)\n",
    "\n",
    "        filter_num=100\n",
    "\n",
    "        h2_1=C.layers.Convolution((3,emb_dim),num_filters=filter_num,reduction_rank=0,activation=C.relu)(to_static)\n",
    "        h2_2=C.layers.Convolution((4,emb_dim),num_filters=filter_num,reduction_rank=0,activation=C.relu)(to_static)\n",
    "        h2_3=C.layers.Convolution((5,emb_dim),num_filters=filter_num,reduction_rank=0,activation=C.relu)(to_static)\n",
    "    \n",
    "        h3_1=C.layers.MaxPooling((max_length-2,1),name='pooling')(h2_1)\n",
    "        h3_2=C.layers.MaxPooling((max_length-3,1),name='pooling')(h2_2)\n",
    "        h3_3=C.layers.MaxPooling((max_length-4,1),name='pooling')(h2_3)\n",
    "        #h2=BiRecurrence(C.layers.LSTM(hidden_dim), C.layers.LSTM(hidden_dim))(h1)\n",
    "        h3=C.splice(h3_2,h3_1,h3_3,axis=0)\n",
    "        h4=C.layers.Dense(num_labels,name='hidden')(h3)\n",
    "\n",
    "    return h4\n",
    "\n",
    "def create_model_cnn_body():\n",
    "\n",
    "    with C.layers.default_options(initial_state=0.1):\n",
    "\n",
    "       \n",
    "\n",
    "        h1t= C.layers.Embedding(300,name='embed')(xb)#init=embedding,\n",
    "        h1b= C.layers.Embedding(300,name='embed')(xt)#init=embedding,\n",
    "        bnb = C.layers.BatchNormalization(name='bn')(h1b)\n",
    "        bnt = C.layers.BatchNormalization(name='bn')(h1t)\n",
    "        to_static_t= C.layers.PastValueWindow(window_size=max_length, axis=-2)(bnt)[0]\n",
    "        to_static_b= C.layers.PastValueWindow(window_size=max_length*4, axis=-2)(bnb)[0]\n",
    "\n",
    "\n",
    "        h2_1t=C.layers.Convolution((1,emb_dim),num_filters=100,reduction_rank=0,activation=C.relu)(to_static_t)\n",
    "        h2_2t=C.layers.Convolution((2,emb_dim),num_filters=100,reduction_rank=0,activation=C.relu)(to_static_t)\n",
    "        h2_3t=C.layers.Convolution((3,emb_dim),num_filters=100,reduction_rank=0,activation=C.relu)(to_static_t)\n",
    "        \n",
    "        h2_1b=C.layers.Convolution((1,emb_dim),num_filters=100,reduction_rank=0,activation=C.relu)(to_static_b)\n",
    "        h2_2b=C.layers.Convolution((2,emb_dim),num_filters=100,reduction_rank=0,activation=C.relu)(to_static_b)\n",
    "        h2_3b=C.layers.Convolution((3,emb_dim),num_filters=100,reduction_rank=0,activation=C.relu)(to_static_b)\n",
    "\n",
    "        h3_2t=C.layers.MaxPooling((max_length-1,1),name='pooling')(h2_2t)\n",
    "        h3_1t=C.layers.MaxPooling((max_length,1),name='pooling')(h2_1t)\n",
    "        h3_3t=C.layers.MaxPooling((max_length-2,1),name='pooling')(h2_3t)\n",
    "        \n",
    "        h3_2b=C.layers.MaxPooling((max_length-1,1),name='pooling')(h2_2b)\n",
    "        h3_1b=C.layers.MaxPooling((max_length,1),name='pooling')(h2_1b)\n",
    "        h3_3b=C.layers.MaxPooling((max_length-2,1),name='pooling')(h2_3b)\n",
    "\n",
    "        h3=C.splice(h3_2t,h3_1t,h3_3t,h3_2b,h3_1b,h3_3b,axis=0)\n",
    "        \n",
    "        h4=C.layers.Dense(hidden_dim, activation=C.relu,name='hidden')(h3)\n",
    "        drop2 = C.layers.Dropout(0.5)(h4)\n",
    "\n",
    "        h5=C.layers.Dense(num_labels,name='classify')(drop2)\n",
    "\n",
    "    return h5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_reader(path, is_training,is_body=False):\n",
    "    if is_body:\n",
    "        return C.io.MinibatchSource(C.io.CTFDeserializer(path, C.io.StreamDefs(\n",
    "         title         = C.io.StreamDef(field='S0', shape=title_size,  is_sparse=True),\n",
    "         industry        = C.io.StreamDef(field='S1', shape=num_labels, is_sparse=True),\n",
    "         body        = C.io.StreamDef(field='S2', shape=body_size, is_sparse=True),\n",
    "     )), randomize=is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)\n",
    "    else:\n",
    "        return C.io.MinibatchSource(C.io.CTFDeserializer(path, C.io.StreamDefs(\n",
    "         title         = C.io.StreamDef(field='S0', shape=vocab_size,  is_sparse=True),\n",
    "         industry        = C.io.StreamDef(field='S1', shape=num_labels, is_sparse=True),\n",
    "         \n",
    "     )), randomize=is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)\n",
    "\n",
    "\n",
    "\n",
    "def create_criterion_function(model):\n",
    "    labels = C.placeholder(name='labels')\n",
    "    ce   = C.cross_entropy_with_softmax(model, labels)\n",
    "    errs = C.classification_error      (model, labels)\n",
    "    return C.combine ([ce, errs]) # (features, labels) -> (loss, metric)\n",
    "\n",
    "def create_criterion_function_preferred(model, labels):\n",
    "    ce   = C.cross_entropy_with_softmax(model, labels)\n",
    "    errs = C.classification_error      (model, labels,topN=1)\n",
    "    return ce, errs # (model, labels) -> (loss, error metric)\n",
    "def fast_hist(a, b, n):\n",
    "    k = (a >= 0) & (a < n)\n",
    "    return np.bincount(n * a[k].astype(int) + b[k], minlength=n**2).reshape(n, n)\n",
    "\n",
    "def evaluate(reader,model_func,is_body=False):#cal precision and recall\n",
    "\n",
    "    if is_body:\n",
    "        test_xt = C.sequence.input_variable(title_size)\n",
    "        \n",
    "    else:\n",
    "        test_xt = C.sequence.input_variable(vocab_size)\n",
    "         \n",
    "    test_xb = C.sequence.input_variable(body_size)\n",
    "    test_y = C.input_variable(num_labels)\n",
    "    model=model_func(x)\n",
    "    # Create the loss and error functions\n",
    "    loss, label_error = create_criterion_function_preferred(model, y)\n",
    "    \n",
    "    # Assign the data fields to be read from the input\n",
    "    data_map={x: reader.streams.title, y: reader.streams.industry}\n",
    "    \n",
    "    confuse=np.zeros((num_labels,num_labels))\n",
    "    count=0\n",
    "    while True:\n",
    "        data = reader.next_minibatch(2048)  # fetch minibatch\n",
    "        if not data:\n",
    "            break\n",
    "            \n",
    "        for key in data.keys():\n",
    "            if(key.m_name==\"title\"):   \n",
    "                test_xt=data[key]            \n",
    "            if(key.m_name==\"industry\"):      \n",
    "                test_y=data[key]\n",
    "            if(key.m_name==\"body\"): \n",
    "                test_xb=data[key]\n",
    "        #print(data)   \n",
    "        if is_body:\n",
    "            output=z(x).eval({xt:test_xt,xb:test_xb}).argmax(axis=1)\n",
    "        else:\n",
    "            output=z(x).eval({x:test_xt}).argmax(axis=1)\n",
    "       \n",
    "        gt=C.squeeze(C.argmax(y)).eval({y:test_y}).astype(int)#.as_sequences(test_y)[0].indices[0]\n",
    "        confuse+=fast_hist(output,gt,num_labels)\n",
    "        count+=1\n",
    "\n",
    "    precision=np.diag(confuse)/np.sum(confuse,axis=0)\n",
    "    recall = np.diag(confuse)/np.sum(confuse,axis=1)\n",
    "    accuarcy = np.diag(confuse).sum() / confuse.sum()\n",
    "    aver_precision=np.nanmean(precision)\n",
    "    aver_recall = np.nanmean(recall)\n",
    "   \n",
    "    print(\"Precision:{} Recall:{} Acc:{}\".format(aver_precision,aver_recall,accuarcy))\n",
    "    return accuarcy\n",
    "\n",
    "\n",
    "def train(train_reader, test_reader, model_func,epoch_size, max_epochs=30,is_body=False,data_tag = \"180\"):\n",
    "    global model\n",
    "    criterion = create_criterion_function(create_model())\n",
    "    \n",
    "    if model in [\"lstm\",\"gru\"]:\n",
    "        criterion.replace_placeholders({criterion.placeholders[1]: C.input_variable(num_labels)})\n",
    "    else:\n",
    "        criterion.replace_placeholders({criterion.placeholders[0]: C.input_variable(num_labels)})\n",
    "\n",
    "    if is_body:\n",
    "        model = model_func(xb,xt)\n",
    "        data_map={xb: reader.streams.body,xt: reader.streams.title, y: reader.streams.industry}\n",
    "    else:\n",
    "        model = model_func(x)\n",
    "        data_map={x: reader.streams.title, y: reader.streams.industry}\n",
    "    \n",
    "    loss, label_error = create_criterion_function_preferred(model, y)\n",
    "\n",
    "    # training config\n",
    "\n",
    "    minibatch_size = 2048\n",
    "\n",
    "\n",
    "    lr_schedule = C.learning_parameter_schedule([3e-4*minibatch_size]*5+[1e-4*minibatch_size], epoch_size=epoch_size)\n",
    "\n",
    "    # Momentum schedule\n",
    "    momentums = C.momentum_schedule(0.99, minibatch_size=minibatch_size)\n",
    "\n",
    "    # We use a the Adam optimizer which is known to work well on this dataset\n",
    "    # Feel free to try other optimizers from\n",
    "    # https://www.cntk.ai/pythondocs/cntk.learner.html#module-cntk.learner\n",
    "    learner = C.adam(parameters=model.parameters,#tuple([x for  x in model.parameters][:-1]),\n",
    "                     lr=lr_schedule,\n",
    "                     momentum=momentums,\n",
    "                     gradient_clipping_threshold_per_sample=15,\n",
    "                     gradient_clipping_with_truncation=True,\n",
    "                     l2_regularization_weight=0.0001)\n",
    "\n",
    "    # Setup the progress updater\n",
    "    progress_printer = C.logging.ProgressPrinter(tag='Training', num_epochs=max_epochs)\n",
    "\n",
    "    # Uncomment below for more detailed logging\n",
    "    #progress_printer = ProgressPrinter(freq=100, first=10, tag='Training', num_epochs=max_epochs)\n",
    "\n",
    "    # Instantiate the trainer\n",
    "    trainer = C.Trainer(model, (loss, label_error), learner, progress_printer)\n",
    "\n",
    "    # process minibatches and perform model training\n",
    "    C.logging.log_number_of_parameters(model)\n",
    "\n",
    "    # Assign the data fields to be read from the input\n",
    "    \n",
    "    \n",
    "    t = 0\n",
    "    for epoch in range(max_epochs):         # loop over epochs\n",
    "        epoch_end = (epoch+1) * epoch_size\n",
    "        while t < epoch_end:                # loop over minibatches on the epoch\n",
    "            data = reader.next_minibatch(minibatch_size, input_map= data_map)  # fetch minibatch\n",
    "            trainer.train_minibatch(data)               # update model with it\n",
    "            t += data[y].num_samples                    # samples so far\n",
    "        trainer.summarize_training_progress()\n",
    "        if epoch%2==0:\n",
    "            acc = evaluate(test_reader,z,is_body)\n",
    "            z.save(\"news/Models/{}_Model_{}_epoch{}_acc{}.dnn\".format(m.upper(0:.2f),data_tag,int(epoch/2),acc))\n",
    "            \n",
    "def do_train(m,train_file,test_file,tag,epoch_size):\n",
    "    global z\n",
    "    global model\n",
    "    model = m\n",
    "    is_body = m.find('body')>=0\n",
    "    print(m)\n",
    "    z = create_model()\n",
    "    \n",
    "    #z= load_model(\"news/Models/LSTM_Model_180_Autosave_epoch3.0_acc0.7861713891009139.dnn\")\n",
    "    #do_test()\n",
    "    \n",
    "    print(z.parameters)                                                                               \n",
    "    train_reader = create_reader(train_file, is_training=True,is_body=is_body)\n",
    "    test_reader = create_reader(test_file, is_training=False,is_body=is_body)\n",
    "    train(train_reader,test_reader,z,epoch_size,is_body=is_body,data_tag=tag)\n",
    "    z.save(\"news/Models/{}Model_{}_final.dnn\".format(m.upper()))\n",
    "def init_param_setting(vocabulary= 100000,industry = 19,title=0,body = 0):\n",
    "    global vocab_size,title_size,body_size,num_labels\n",
    "    vocab_size,num_labels,title_size,body_size = vocabulary,indsutry,title,body\n",
    "    \n",
    "def super_param_setting(embed=300,hidden=200,sentence=50):\n",
    "    global emb_dim,hidden_dim,max_length\n",
    "    emb_dim,hidden_dim,max_length = embed,hidden,sentence\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cntk import load_model\n",
    "### training module\n",
    "\n",
    "\n",
    "\n",
    "models = ['cnn']\n",
    "for m in models:\n",
    "    do_train(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [cntk-py35]",
   "language": "python",
   "name": "Python [cntk-py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
