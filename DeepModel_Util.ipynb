{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting deepmodel_util.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile deepmodel_util.py\n",
    "from __future__ import print_function # Use a function definition from future version (say 3.x from 2.7 interpreter)\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import time \n",
    "\n",
    "import cntk as C\n",
    "import cntk.tests.test_utils\n",
    "import pickle\n",
    "import random\n",
    "from cntk import sequence\n",
    "from cntk import load_model\n",
    "from cntk.device import try_set_default_device, gpu,cpu\n",
    "from scipy.sparse import csr_matrix\n",
    "from data_processor import *\n",
    "from cntk.layers import *\n",
    "from cntk.layers.typing import *\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "cntk.tests.test_utils.set_device_from_pytest_env() # (only needed for our build system)\n",
    "C.cntk_py.set_fixed_random_seed(1) # fix a random seed for CNTK components\n",
    "try_set_default_device(gpu(0))\n",
    "\n",
    "def load_data_body(input_file,title_dict,body_dict,industry_dict,raw=False):\n",
    "    data = open(input_file, encoding = \"utf-8\").readlines()\n",
    "    \n",
    "    data_title = np.zeros((len(data),max_length_title),dtype = np.float32)\n",
    "    data_body  = np.zeros((len(data),max_length_body),dtype = np.float32)\n",
    "    data_label = np.zeros((len(data),1),dtype = np.float32)\n",
    "    \n",
    "    for index,line in enumerate(data):\n",
    "        row = line.strip(\"\\n\").split(\"\\t\")        \n",
    "        if not raw:\n",
    "            title    =  row[0]\n",
    "            body     =  row[1]     \n",
    "        else:\n",
    "            title    =  tokenize(row[0])\n",
    "            body     =  tokenize(row[1])\n",
    "            #industry =  tokenize(row[2])\n",
    "        industry =  row[2]\n",
    "        \n",
    "        for jndex,token in enumerate(title.split(\" \")):\n",
    "            if jndex>=max_length_title:\n",
    "                break\n",
    "            data_title[index,jndex]=title_dict.get(token,len(title_dict)-1)\n",
    "            \n",
    "        for jndex,token in enumerate(body.split(\" \")):\n",
    "            if jndex>=max_length_body:\n",
    "                break\n",
    "            data_body[index,jndex]=body_dict.get(token,len(body_dict)-1)\n",
    "            \n",
    "        data_label[index] = industry_dict.get(industry,len(industry_dict))\n",
    "    return data_title,data_body,data_label\n",
    "\n",
    "def load_data_dynamic(input_file,title_dict,industry_dict,raw=False):\n",
    "    data = open(input_file, encoding = \"utf-8\").readlines()\n",
    "    \n",
    "    data_title =[ [] for x in range(len(data))]#np.zeros((len(data),max_length_title),dtype = np.float32)\n",
    "    data_label = np.zeros((len(data),1),dtype = np.float32)\n",
    "    \n",
    "    \n",
    "    for index,line in enumerate(data):\n",
    "        row = line.strip(\"\\n\").split(\"\\t\")\n",
    "        if raw:\n",
    "            title    =  tokenize(row[0])\n",
    "        else:\n",
    "            title    =  row[0]\n",
    "        industry =  row[1]\n",
    "        \n",
    "        for jndex,token in enumerate(title.split(\" \")):\n",
    "            if jndex>=max_length_title:\n",
    "                break\n",
    "            data_title[index].append(title_dict.get(token,len(title_dict)-1))\n",
    "        while len(data_title[index])<5:\n",
    "            data_title[index].append(len(title_dict)-1)\n",
    "        data_label[index] = industry_dict.get(industry,len(industry_dict))\n",
    "    data_title = [ np.array(x) for x in data_title]\n",
    "    return data_title,data_label\n",
    "\n",
    "def load_data_static(input_file,title_dict,industry_dict,raw=False):\n",
    "    data = open(input_file, encoding = \"utf-8\").readlines()\n",
    "    \n",
    "    data_title =np.zeros((len(data),max_length_title),dtype = np.float32)\n",
    "    data_label = np.zeros((len(data),1),dtype = np.float32)\n",
    "    \n",
    "    \n",
    "    for index,line in enumerate(data):\n",
    "        row = line.strip(\"\\n\").split(\"\\t\")\n",
    "        if raw:\n",
    "            title    =  tokenize(row[0])\n",
    "        else:\n",
    "            title    =  row[0]\n",
    "        industry =  row[1]\n",
    "        \n",
    "        for jndex,token in enumerate(title.split(\" \")):\n",
    "            if jndex>=max_length_title:\n",
    "                break\n",
    "            data_title[index,jndex]=title_dict.get(token,len(title_dict)-1)\n",
    "        data_label[index] = industry_dict.get(industry,len(industry_dict))\n",
    "    \n",
    "    return data_title.tolist(),data_label.tolist()\n",
    "\n",
    "def load_embedding(dict_file,embedding_model_file):\n",
    "    model = Word2Vec.load(embedding_model_file)\n",
    "    dict_list = [x.strip(\"\\n\") for x in open(dict_file,encoding = 'utf-8').readlines()]\n",
    "    embedding = np.zeros((len(dict_list),emb_dim))\n",
    "    count = 0\n",
    "    for i,w in enumerate(dict_list):\n",
    "        try:\n",
    "            vec = model.wv[w]\n",
    "        except:\n",
    "            vec=model.wv[\"UNK\"]\n",
    "            count+=1\n",
    "        embedding[i] =vec\n",
    "    print(count)\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def batch_iter(data,batch_size, num_epochs, shuffle=True):\n",
    "    # Generates a batch iterator for a dataset.\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((data_size-1)/batch_size) + 1\n",
    "    print('data_size: ', data_size, 'batch_size: ', batch_size, 'num_batches_per_epoch: ', num_batches_per_epoch)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            random.shuffle(data)\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield data[start_index:end_index]\n",
    "            \n",
    "\n",
    "def fast_hist(a, b, n):\n",
    "    k = (a >= 0) & (a < n)\n",
    "    return np.bincount(n * a[k].astype(int) + b[k], minlength=n**2).reshape(n, n)\n",
    "\n",
    "def test(batch_size,model,data):\n",
    "    print(\"Testing...\")\n",
    "    scores = model(input_xt)\n",
    "    predict = C.argmax(scores,axis = 0)\n",
    "    confuse = np.zeros((num_labels,num_labels))\n",
    "\n",
    "    test_data_title,test_data_label = data\n",
    "    batches = batch_iter(list(zip(test_data_title,test_data_label)), batch_size, 1)\n",
    "    \n",
    "    for batch in batches:\n",
    "        batch_data_title,batch_data_label = zip(*batch) \n",
    "        \n",
    "        batch_data_title = list(batch_data_title)\n",
    "        #print(type(batch_data_title))\n",
    "        output = np.array(predict.eval({input_xt: batch_data_title}),dtype=np.int)\n",
    "        gt = np.array(batch_data_label,dtype=np.int)\n",
    "        confuse+=fast_hist(output,gt,num_labels)\n",
    "        \n",
    "    precision=np.diag(confuse)/np.sum(confuse,axis=0)\n",
    "    recall = np.diag(confuse)/np.sum(confuse,axis=1)\n",
    "    accuracy = np.diag(confuse).sum() / confuse.sum()\n",
    "    aver_precision=np.nanmean(precision)\n",
    "    aver_recall = np.nanmean(recall)\n",
    "   \n",
    "    print(\"Precision:{} Recall:{} Acc:{}\".format(aver_precision,aver_recall,accuracy))\n",
    "    return accuracy\n",
    "\n",
    "def test_body(batch_size,model,data):\n",
    "    print(\"Testing...\")\n",
    "    scores = model(input_xt,input_xb)\n",
    "    predict = C.argmax(scores,axis = 0)\n",
    "    confuse = np.zeros((num_labels,num_labels))\n",
    "    #C.element_add(input_y,C.element_times(predict,C.Constant([nums_labels])))\n",
    "    test_data_title,test_data_body,test_data_label = data\n",
    "    batches = batch_iter(list(zip(test_data_title,test_data_body,test_data_label)), batch_size, 1)\n",
    "    \n",
    "    for batch in batches:\n",
    "        batch_data_title,batch_data_body,batch_data_label = zip(*batch)\n",
    "        \n",
    "        batch_data_body = list(batch_data_body)\n",
    "        batch_data_title = list(batch_data_title)\n",
    "        output = np.array(predict.eval({input_xb: batch_data_body,input_xt: batch_data_title}),dtype=np.int)\n",
    "        gt = np.array(batch_data_label,dtype=np.int)\n",
    "        confuse+=fast_hist(output,gt,num_labels)\n",
    "    precision=np.diag(confuse)/np.sum(confuse,axis=0)\n",
    "    recall = np.diag(confuse)/np.sum(confuse,axis=1)\n",
    "    accuracy = np.diag(confuse).sum() / confuse.sum()\n",
    "    aver_precision=np.nanmean(precision)\n",
    "    aver_recall = np.nanmean(recall)\n",
    "   \n",
    "    print(\"Precision:{} Recall:{} Acc:{}\".format(aver_precision,aver_recall,accuracy))\n",
    "    return accuracy\n",
    "\n",
    "def train(model,train_data,num_epochs,learning_rate,batch_size,tag=\"CNN\",l2_weight=0,show_count =1000,do_test=True):\n",
    "    \n",
    "    #print(C.logging.get_node_outputs(model))\n",
    "    scores = model(input_xt)\n",
    "\n",
    "    loss =C.reduce_mean(C.losses.cross_entropy_with_softmax(scores, input_y_one_hot))\n",
    "    \n",
    "    # Training\n",
    "    lr_schedule = C.learning_parameter_schedule(learning_rate)\n",
    "    #learner = C.adam(scores.parameters, lr=lr_schedule, momentum=0.9,l2_regularization_weight=0)\n",
    "    progress_printer = C.logging.ProgressPrinter(tag='Training', num_epochs=num_epochs)\n",
    "    momentums = C.momentum_schedule(0.99, minibatch_size=batch_size)\n",
    "    learner = C.adam(parameters=scores.parameters,#model.parameters,\n",
    "                     lr=lr_schedule,\n",
    "                     momentum=momentums,\n",
    "                     gradient_clipping_threshold_per_sample=15,\n",
    "                     gradient_clipping_with_truncation=True,\n",
    "                     l2_regularization_weight=l2_weight)\n",
    "    trainer = C.Trainer(scores, (loss), [learner], progress_printer)\n",
    "    \n",
    "    train_data_title,train_data_label = train_data\n",
    "    batches = batch_iter(list(zip(train_data_title,train_data_label)), batch_size, num_epochs)\n",
    "\n",
    "    # training loop\n",
    "    count = 0\n",
    "    t = time.time()\n",
    "    for batch in batches:\n",
    "        count += 1\n",
    "        batch_data_title,batch_data_label = zip(*batch)\n",
    "        batch_data_title = list(batch_data_title)\n",
    "        batch_data_label = list(batch_data_label)\n",
    "        #print(type(batch_data_title),type(batch_data_title[0]),batch_data_title[0])\n",
    "        trainer.train_minibatch({input_xt: batch_data_title, input_y: batch_data_label})\n",
    "        if count%show_count== 0:\n",
    "            print(\"batch count:{} elapse time:{}\".format(count,time.time()-t))\n",
    "            t=time.time()\n",
    "            if do_test:\n",
    "                acc1=test(batch_size,model,test_data)\n",
    "                model.save('./model/{}_acc{:.3f}.dnn'.format(tag,acc1))\n",
    "            else:\n",
    "                model.save('./model/{}.dnn'.format(tag))\n",
    "            \n",
    "def train_body(model,train_data,num_epochs,learning_rate,batch_size,l2_weight=0,tag = \"cnn\",show_count =1000,do_test=True):\n",
    "    \n",
    "    #learning_rate *= batch_size\n",
    "    #model = model_func()\n",
    "    #print(C.logging.get_node_outputs(model))\n",
    "    scores = model(input_xt,input_xb)\n",
    "\n",
    "    loss =C.reduce_mean(C.losses.cross_entropy_with_softmax(scores, input_y_one_hot))\n",
    "    \n",
    "    # Training\n",
    "    lr_schedule = C.learning_parameter_schedule(learning_rate)\n",
    "    #learner = C.adam(scores.parameters, lr=lr_schedule, momentum=0.9,l2_regularization_weight=0)\n",
    "    progress_printer = C.logging.ProgressPrinter(tag='Training', num_epochs=num_epochs)\n",
    "    momentums = C.momentum_schedule(0.99, minibatch_size=batch_size)\n",
    "    learner = C.adam(parameters=scores.parameters,#model.parameters,\n",
    "                     lr=lr_schedule,\n",
    "                     momentum=momentums,\n",
    "                     gradient_clipping_threshold_per_sample=15,\n",
    "                     gradient_clipping_with_truncation=True,\n",
    "                     l2_regularization_weight=l2_weight)\n",
    "    trainer = C.Trainer(scores, (loss), [learner], progress_printer)\n",
    "    \n",
    "    train_data_title,train_data_body,train_data_label = train_data\n",
    "    batches = batch_iter(list(zip(train_data_title,train_data_body,train_data_label)), batch_size, num_epochs)\n",
    "\n",
    "    # training loop\n",
    "    count = 0\n",
    "    t = time.time()\n",
    "    for batch in batches:\n",
    "        count += 1\n",
    "        batch_data_title,batch_data_body,batch_data_label = zip(*batch)\n",
    "        batch_data_body = list(batch_data_body)\n",
    "        batch_data_title = list(batch_data_title)\n",
    "        batch_data_label = list(batch_data_label)\n",
    "        \n",
    "        trainer.train_minibatch({input_xb: batch_data_body,input_xt: batch_data_title, input_y: batch_data_label})\n",
    "        if count%show_count== 0:\n",
    "            print(\"batch count:{} elapse time:{}\".format(count,time.time()-t))\n",
    "            t=time.time()\n",
    "            if do_test:\n",
    "                acc1=test_body(batch_size,model,test_data)\n",
    "                model.save('./model/{}_acc{:.3f}.dnn'.format(tag,acc1))\n",
    "            else:\n",
    "                model.save('./model/{}.dnn'.format(tag))\n",
    "            \n",
    "def inference(model,val_doc_file,output_file,title_dict,industry_file,dynamic=False):\n",
    "    \n",
    "    scores = model(input_xt)\n",
    "    predict = C.argmax(scores,axis = 0)\n",
    "    probability = C.reduce_max(C.softmax(scores),axis = 0)\n",
    "    \n",
    "    industry = [x.strip(\"\\n\") for x in open(industry_file,encoding =\"utf-8\").readlines()]\n",
    "    val_doc = open(val_doc_file,encoding = \"utf-8\")\n",
    "    output = open(output_file,\"w\",encoding = \"utf-8\")\n",
    "    val_doc = [tokenize(x.strip(\"\\n\").split(\"\\t\")[0]) for x in val_doc.readlines()]\n",
    "    #print(val_doc[0:5])\n",
    "    if dynamic:\n",
    "        data_title = [[] for x  in range(len(val_doc))]\n",
    "    \n",
    "        for index,title in enumerate(val_doc):           \n",
    "            for jndex,token in enumerate(title.split(\" \")):\n",
    "                if jndex>=max_length_title:\n",
    "                    break\n",
    "                data_title[index].append(title_dict.get(token,len(title_dict)-1))\n",
    "            while len(data_title[index])<5:\n",
    "                data_title[index].append(len(title_dict)-1) \n",
    "    else:\n",
    "        data_title =np.zeros((len(val_doc),max_length_title),dtype = np.float32)\n",
    "        for index,title in enumerate(val_doc):\n",
    "            #title    =  row[0]     \n",
    "            for jndex,token in enumerate(title.split(\" \")):\n",
    "                if jndex>=max_length_title:\n",
    "                    break\n",
    "                data_title[index,jndex]=title_dict.get(token,len(title_dict)-1)\n",
    "    batches = batch_iter(data_title, batch_size, 1,shuffle =False)\n",
    "    for batch in batches:\n",
    "        batch_data_title = batch\n",
    "        pred = np.array(predict.eval({input_xt: batch_data_title}),dtype=np.int)\n",
    "        prob = np.array(probability.eval({input_xt: batch_data_title}),dtype=np.float32)\n",
    "        #gt = np.array(batch_data_label,dtype=np.int)\n",
    "        #confuse+=fast_hist(output,gt,num_labels)\n",
    "        for pre,pro in list(zip(pred,prob)):\n",
    "            output.write(\"\\t\".join([str(industry[int(pre)]),str(pro[0])])+\"\\n\")\n",
    "    output.close()\n",
    "\n",
    "def inference_body(model,val_doc_file,output_file,title_dict,body_dict,industry_file):\n",
    "    \n",
    "    scores = model(input_xt,input_xb)\n",
    "    predict = C.argmax(scores,axis = 0)\n",
    "    probability = C.reduce_max(C.softmax(scores),axis = 0)\n",
    "    \n",
    "    industry = [x.strip(\"\\n\") for x in open(industry_file,encoding =\"utf-8\").readlines()]\n",
    "    \n",
    "    val_doc = open(val_doc_file,encoding = \"utf-8\").readlines()\n",
    "    output = open(output_file,\"w\",encoding = \"utf-8\")\n",
    "    \n",
    "    val_title = [tokenize(x.strip(\"\\n\").split(\"\\t\")[0]) for x in val_doc]\n",
    "    val_body = [tokenize(x.strip(\"\\n\").split(\"\\t\")[1]) for x in val_doc]\n",
    "    \n",
    "    data_title = np.zeros((len(val_title),max_length_title),dtype = np.float32)\n",
    "    data_body= np.zeros((len(val_body),max_length_body),dtype = np.float32)\n",
    "    \n",
    "    for index,title in enumerate(val_title):       \n",
    "        for jndex,token in enumerate(title.split(\" \")):\n",
    "            if jndex>=max_length_title:\n",
    "                break\n",
    "            data_title[index,jndex]=title_dict.get(token,len(title_dict)-1)\n",
    "    for index,body in enumerate(val_body):       \n",
    "        for jndex,token in enumerate(body.split(\" \")):\n",
    "            if jndex>=max_length_body:\n",
    "                break\n",
    "            data_body[index,jndex]=body_dict.get(token,len(body_dict)-1)\n",
    "\n",
    "    batches = batch_iter(list(zip(data_title,data_body)), batch_size, 1,shuffle=False)\n",
    "    for batch in batches:\n",
    "\n",
    "        batch_data_title,batch_data_body = zip(*batch)\n",
    "        pred = np.array(predict.eval({input_xt: np.array(batch_data_title),input_xb: np.array(batch_data_body)}))\n",
    "        prob = np.array(probability.eval({input_xt: np.array(batch_data_title),input_xb: np.array(batch_data_body)}),dtype=np.float32)\n",
    "\n",
    "        for pre,pro in list(zip(pred,prob)):\n",
    "            \n",
    "            output.write(\"\\t\".join([str(industry[int(pre)]),str(pro[0])])+\"\\n\")\n",
    "    output.close()\n",
    "    print(\"predict finished.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
