{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn_2fold\n",
      "(Parameter('W', [], [300 x 1 x 301 x 19]), Parameter('b', [], [19]), Parameter('W', [], [100 x 1 x 4 x 300]), Parameter('b', [], [100 x 1 x 1]), Parameter('E', [], [69516 x 300]), Parameter('W', [], [100 x 1 x 3 x 300]), Parameter('b', [], [100 x 1 x 1]), Parameter('W', [], [100 x 1 x 5 x 300]), Parameter('b', [], [100 x 1 x 1]))\n",
      "Training 22930819 parameters in 9 parameter tensors.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA failure 4: unspecified launch failure ; GPU=0 ; hostname=MININT-8LA6JR2 ; expr=cudaDeviceSynchronize()\n\n[CALL STACK]\n    > Microsoft::MSR::CNTK::CudaTimer::  Stop\n    - Microsoft::MSR::CNTK::CudaTimer::  Stop (x2)\n    - RtlRunOnceExecuteOnce\n    - InitOnceExecuteOnce\n    - _crtInitOnceExecuteOnce\n    - Microsoft::MSR::CNTK::GPUMatrix<half>::  GetCublasHandle\n    - Microsoft::MSR::CNTK::GPUSparseMatrix<half>::  performElementWiseFunction (x2)\n    - Microsoft::MSR::CNTK::GPUMatrix<float>::  TensorOp\n    - Microsoft::MSR::CNTK::Matrix<float>::  TensorOp\n    - Microsoft::MSR::CNTK::TensorView<float>::  DoUnaryOpOf\n    - Microsoft::MSR::CNTK::TensorView<float>::  AssignCopyOf\n    - std::enable_shared_from_this<Microsoft::MSR::CNTK::MatrixBase>::  shared_from_this (x2)\n    - CNTK::Internal::  UseSparseGradientAggregationInDataParallelSGD\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5b79808e2e6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mmodels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'cnn_2fold'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'cnn'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mdo_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata_sample_ctf_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata_sample_ctf_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msuffix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m48000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[0mmodels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'cnn_2fold'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'cnn'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Python\\IndustryClassifier\\deep_model.py\u001b[0m in \u001b[0;36mdo_train\u001b[1;34m(m, train_file, test_file, tag, epoch_size)\u001b[0m\n\u001b[0;32m    360\u001b[0m     \u001b[0mtrain_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_reader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mis_body\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_body\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m     \u001b[0mtest_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_reader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mis_body\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_body\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_reader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_reader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mis_body\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_body\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata_tag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m     \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"news/Models/{}Model_{}_final.dnn\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minit_param_setting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m100000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindustry\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m19\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Python\\IndustryClassifier\\deep_model.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_reader, test_reader, model_func, epoch_size, max_epochs, is_body, data_tag)\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mepoch_end\u001b[0m\u001b[1;33m:\u001b[0m                \u001b[1;31m# loop over minibatches on the epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_map\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdata_map\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# fetch minibatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m             \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m               \u001b[1;31m# update model with it\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m             \u001b[0mt\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_samples\u001b[0m                    \u001b[1;31m# samples so far\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummarize_training_progress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\local\\Anaconda3-4.1.1-Windows-x86_64\\envs\\cntk-py35\\lib\\site-packages\\cntk\\train\\trainer.py\u001b[0m in \u001b[0;36mtrain_minibatch\u001b[1;34m(self, arguments, outputs, device, is_sweep_end)\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcontains_minibatch_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m                 updated = super(Trainer, self).train_minibatch_overload_for_minibatchdata(\n\u001b[1;32m--> 181\u001b[1;33m                     arguments, device)\n\u001b[0m\u001b[0;32m    182\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m                 updated = super(Trainer, self).train_minibatch(arguments, is_sweep_end,\n",
      "\u001b[1;32mC:\\local\\Anaconda3-4.1.1-Windows-x86_64\\envs\\cntk-py35\\lib\\site-packages\\cntk\\cntk_py.py\u001b[0m in \u001b[0;36mtrain_minibatch_overload_for_minibatchdata\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   3022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain_minibatch_overload_for_minibatchdata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3024\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_cntk_py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrainer_train_minibatch_overload_for_minibatchdata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA failure 4: unspecified launch failure ; GPU=0 ; hostname=MININT-8LA6JR2 ; expr=cudaDeviceSynchronize()\n\n[CALL STACK]\n    > Microsoft::MSR::CNTK::CudaTimer::  Stop\n    - Microsoft::MSR::CNTK::CudaTimer::  Stop (x2)\n    - RtlRunOnceExecuteOnce\n    - InitOnceExecuteOnce\n    - _crtInitOnceExecuteOnce\n    - Microsoft::MSR::CNTK::GPUMatrix<half>::  GetCublasHandle\n    - Microsoft::MSR::CNTK::GPUSparseMatrix<half>::  performElementWiseFunction (x2)\n    - Microsoft::MSR::CNTK::GPUMatrix<float>::  TensorOp\n    - Microsoft::MSR::CNTK::Matrix<float>::  TensorOp\n    - Microsoft::MSR::CNTK::TensorView<float>::  DoUnaryOpOf\n    - Microsoft::MSR::CNTK::TensorView<float>::  AssignCopyOf\n    - std::enable_shared_from_this<Microsoft::MSR::CNTK::MatrixBase>::  shared_from_this (x2)\n    - CNTK::Internal::  UseSparseGradientAggregationInDataParallelSGD\n\n"
     ]
    }
   ],
   "source": [
    "import cntk as C\n",
    "from deep_model import *\n",
    "import deep_model\n",
    "prefix = \"C:\\\\Users\\\\t-haohu\\\\Documents\\\\Python\\\\IndustryClassifier\\\\Data\"\n",
    "suffix = \"180days_sample\"\n",
    "\n",
    "data_sample_body_ctf_train = \"{}\\\\ready\\\\train_{}_body.ctf\".format(prefix,suffix)\n",
    "data_sample_body_ctf_test = \"{}bilibi\\ready\\\\train_{}_body.ctf\".format(prefix,suffix)\n",
    "data_sample_ctf_train = \"{}\\\\ready\\\\train_{}.ctf\".format(prefix,suffix)\n",
    "data_sample_ctf_test = \"{}\\\\ready\\\\test_{}.ctf\".format(prefix,suffix)\n",
    "\n",
    "data_title_dict = \"{}\\\\ready\\\\title_{}.pkl\".format(prefix,suffix)\n",
    "data_body_dict = \"{}\\\\ready\\\\body_{}.pkl\".format(prefix,suffix)\n",
    "data_industry_sample = \"{}\\\\ready\\\\industry_{}.wl\".format(prefix,suffix)\n",
    "\n",
    "init_param_setting(vocabulary= 69516,industry = 19,title=69516,body = 388082)\n",
    "#print(deep_model.vocab_size,vocab_size)\n",
    "super_param_setting(embed=300,hidden=200,sentence=50)\n",
    "file_param_setting(data_industry_sample,data_title_dict,data_body_dict)\n",
    "models = ['cnn_2fold','cnn']\n",
    "for m in models:\n",
    "    do_train(m,data_sample_ctf_train,data_sample_ctf_test,suffix,48000)\n",
    "models = ['cnn_2fold','cnn']\n",
    "for m in models:\n",
    "    do_train(m,data_sample_body_ctf_train,data_sample_body_ctf_test,suffix,48000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinibatchData(data=Value([1 x 22 x 23184], CPU), samples=22, seqs=1) (1, 22, 23184)\n",
      "(1, 22, 300)\n",
      "(1, 22, 300)\n",
      "Output('Block253189_Output_0', [#], [50 x 300])\n",
      "Output('Block270741_Output_0', [#], [50 x 300])\n",
      "(1, 1, 50, 300)\n",
      "(1, 1, 50, 300)\n",
      "(1, 2, 50, 300)\n",
      "(1, 100, 48, 1)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'h3_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-16457ec891cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;31m#print(np.array(h2_3.eval({x:test_x})).shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;31m#print(np.array(conv1_2.eval({x:test_x})).shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh3_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;31m#print(np.array(h3_3.eval({x:test_x})).shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"here\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'h3_2' is not defined"
     ]
    }
   ],
   "source": [
    "import cntk as C\n",
    "import numpy as np\n",
    "import cntk.tests.test_utils\n",
    "import pickle\n",
    "from cntk import sequence\n",
    "from cntk import load_model\n",
    "\n",
    "cntk.tests.test_utils.set_device_from_pytest_env() # (only needed for our build system)\n",
    "C.cntk_py.set_fixed_random_seed(1) # fix a random seed for CNTK components\n",
    "\n",
    "\n",
    "from cntk.device import try_set_default_device, gpu,cpu\n",
    "try_set_default_device(cpu())\n",
    "vocab_size = 23184\n",
    "num_labels = 19\n",
    "title_size = 23184\n",
    "body_size = 210000\n",
    "# model dimensions\n",
    "input_dim  = vocab_size\n",
    "label_dim  = num_labels\n",
    "emb_dim    = 300\n",
    "hidden_dim = 200\n",
    "max_length_title = 50\n",
    "max_length = 50\n",
    "max_length_body = 200\n",
    "model = 'cnn'\n",
    "minibatch_size = 8\n",
    "prefix = \"C:\\\\Users\\\\t-haohu\\\\Documents\\\\Python\\\\IndustryClassifier\\\\Data\"\n",
    "suffix = \"180days_sample\"\n",
    "\n",
    "def create_reader(path, is_training,is_body=False):\n",
    "    if is_body:\n",
    "        return C.io.MinibatchSource(C.io.CTFDeserializer(path, C.io.StreamDefs(\n",
    "         title         = C.io.StreamDef(field='S0', shape=title_size,  is_sparse=True),\n",
    "         industry        = C.io.StreamDef(field='S1', shape=num_labels, is_sparse=True),\n",
    "         body        = C.io.StreamDef(field='S2', shape=body_size, is_sparse=True),\n",
    "     )), randomize=is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)\n",
    "    else:\n",
    "        return C.io.MinibatchSource(C.io.CTFDeserializer(path, C.io.StreamDefs(\n",
    "         title         = C.io.StreamDef(field='S0', shape=vocab_size,  is_sparse=True),\n",
    "         industry        = C.io.StreamDef(field='S1', shape=num_labels, is_sparse=True),\n",
    "         \n",
    "     )), randomize=is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)\n",
    "\n",
    "\n",
    "\n",
    "def create_criterion_function(model):\n",
    "    labels = C.placeholder(name='labels')\n",
    "    ce   = C.cross_entropy_with_softmax(model, labels)\n",
    "    errs = C.classification_error      (model, labels)\n",
    "    return C.combine ([ce, errs]) # (features, labels) -> (loss, metric)\n",
    "\n",
    "def create_criterion_function_preferred(model, labels):\n",
    "    ce   = C.cross_entropy_with_softmax(model, labels)\n",
    "    errs = C.classification_error      (model, labels,topN=1)\n",
    "    return ce, errs # (model, labels) -> (loss, error metric)\n",
    "\n",
    "embed_title_file = \"{}\\\\ready\\\\title_{}.pkl\".format(prefix,suffix)\n",
    "embed_body_file = \"{}\\\\ready\\\\body_{}.pkl\".format(prefix,suffix)\n",
    "industry_file = \"{}\\\\ready\\\\industry_{}.wl\".format(prefix,suffix)\n",
    "industry=[i.rstrip(\"\\n\") for i in open(industry_file,encoding = \"utf-8\").readlines()]\n",
    "if embed_title_file:\n",
    "    with open(embed_title_file,'rb') as handle:\n",
    "        embedding_title=pickle.load(handle)\n",
    "if embed_body_file:\n",
    "    with open(embed_body_file,'rb') as handle:\n",
    "        embedding_body=pickle.load(handle)\n",
    "\n",
    "reader = create_reader(\"Data//ready//train_180days_sample.ctf\", is_training=True)\n",
    "x = C.sequence.input_variable(vocab_size)\n",
    "y = C.input_variable(num_labels) \n",
    "\n",
    "\n",
    "input_dim=hidden_dim\n",
    "output_dim = hidden_dim\n",
    "#data_map={x: reader.streams.title, y: reader.streams.industry}\n",
    "data = reader.next_minibatch(20)\n",
    "for key in data.keys():\n",
    "        if(key.m_name==\"title\"):\n",
    "\n",
    "            test_x=data[key]\n",
    "\n",
    "        if(key.m_name==\"industry\"):\n",
    "\n",
    "            test_y=data[key]\n",
    "print(test_x,test_x.shape)\n",
    "\n",
    "\n",
    "h1_1= C.layers.Embedding(weights=embedding_title,name='embed')(x)#\n",
    "#h1_1= C.layers.Embedding(300,name='embed')(x)\n",
    "h1_2= C.layers.Embedding(300,name='embed')(x)#init=embedding,\n",
    "\n",
    "\n",
    "#bn = C.layers.BatchNormalization(name='bn')(h1)\n",
    "to_static_1= C.layers.PastValueWindow(window_size=max_length_title, axis=-2)(h1_1)[0]\n",
    "to_static_2= C.layers.PastValueWindow(window_size=max_length_title, axis=-2)(h1_2)[0]\n",
    "#value,valid = to_static(h1)\n",
    "h1_1_expand = C.expand_dims(to_static_1, -3)\n",
    "h1_2_expand = C.expand_dims(to_static_2, -3)\n",
    "h1 = C.splice(h1_1_expand,h1_2_expand,axis = -3)\n",
    "filter_num=100\n",
    "emb_dim =300\n",
    "h2_1=C.layers.Convolution((3,emb_dim),num_filters=filter_num,reduction_rank=1,activation=C.relu)(h1)\n",
    "#h2_2=C.layers.Convolution((4,emb_dim),num_filters=filter_num,reduction_rank=1,activation=C.relu)(to_static)\n",
    "#h2_3=C.layers.Convolution((5,emb_dim),num_filters=filter_num,reduction_rank=1,activation=C.relu)(to_static)\n",
    "\n",
    "#h3_1=C.layers.MaxPooling((max_length-2,1),name='pooling')(h2_1)\n",
    "#h3_2=C.layers.MaxPooling((max_length-3,1),name='pooling')(h2_2)\n",
    "#h3_3=C.layers.MaxPooling((max_length-4,1),name='pooling')(h2_3)\n",
    "\n",
    "#h3=C.splice(h3_2,h3_1,h3_3,axis=0)\n",
    "#drop1 =C.layers.Dropout(0.5)(h3)\n",
    "#h4=C.layers.Dense(num_labels,name='hidden')(drop1)\n",
    "\n",
    "print(np.array(h1_1.eval({x:test_x})).shape)\n",
    "print(np.array(h1_2.eval({x:test_x})).shape)\n",
    "print(to_static_1)\n",
    "print(to_static_2)\n",
    "print(np.array(h1_1_expand.eval({x:test_x})).shape)\n",
    "print(np.array(h1_2_expand.eval({x:test_x})).shape)\n",
    "print(np.array(h1.eval({x:test_x})).shape)\n",
    "\n",
    "\n",
    "print(np.array(h2_1.eval({x:test_x})).shape)\n",
    "#print(np.array(h2_3.eval({x:test_x})).shape)\n",
    "#print(np.array(conv1_2.eval({x:test_x})).shape)\n",
    "print(np.array(h3_2.eval({x:test_x})).shape)\n",
    "#print(np.array(h3_3.eval({x:test_x})).shape)\n",
    "print(\"here\")\n",
    "#print(np.array(h3.eval({x:test_x})).shape)\n",
    "print(h4.parameters)\n",
    "print(np.array(h4.eval({x:test_x})).shape)\n",
    "#print(np.array(h5.eval({x:test_x})).shape)\n",
    "#print(np.array(h3.eval({x:test_x})).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [cntk-py35]",
   "language": "python",
   "name": "Python [cntk-py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
