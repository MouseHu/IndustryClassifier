{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\local\\Anaconda3-4.1.1-Windows-x86_64\\envs\\cntk-py35\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from data_generator import *\n",
    "from data_processor import *\n",
    "from svm import *\n",
    "\n",
    "valid_industry = [\n",
    "    \"Non-profit & civil organization\",\"Finance\",\"Video game\",\"Media\",\"Health care & biotechnology\",\n",
    "    \"Entertainment \", \"food\",\"Information technology\",\"Education\",\"Government\",\"Airline\",\"Retail\",\n",
    "    \"Arts\",\"Manufacture \",\"Transportation\",\"Construction \",\"Automotive industry\",\"evergreen\",\"sports\"\n",
    "]\n",
    "black_list= [\n",
    "    \"entertainment\", \"government\", \"arts\", \"education\", \"transportation\", \"evergreen\"\n",
    "]\n",
    "valid_industry=[x.lower().replace(\" \",\"\") for x in valid_industry]\n",
    "prefix = \"C:\\\\Users\\\\t-haohu\\\\Documents\\\\Python\\\\IndustryClassifier\\\\Data\"\n",
    "prefix_ori = \"C:\\\\Users\\\\t-haohu\\\\Documents\\\\Python\\\\news\\\\Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\local\\Anaconda3-4.1.1-Windows-x86_64\\envs\\cntk-py35\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dedup Successfully. Before:1148809 After:1120385\n",
      "Split Succeed.Train:1000000 Test:120385\n"
     ]
    }
   ],
   "source": [
    "#180days all\n",
    "\n",
    "\n",
    "#170days\n",
    "\n",
    "suffix = \"170days_again\"\n",
    "\n",
    "data_ori = \"{}\\\\raw\\\\ClassifierTrainingData-180days-180823.csv\".format(prefix_ori)\n",
    "data_raw = \"{}\\\\middle\\\\{}_raw.txt\".format(prefix,suffix)\n",
    "data_token = \"{}\\\\middle\\\\{}_tokenized.txt\".format(prefix,suffix)\n",
    "\n",
    "data_train = \"{}\\\\middle\\\\train_{}.txt\".format(prefix,suffix)\n",
    "data_test = \"{}\\\\middle\\\\test_{}.txt\".format(prefix,suffix)\n",
    "\n",
    "data_industry = \"{}\\\\ready\\\\industry_{}.wl\".format(prefix,suffix)\n",
    "data_vocabulary = \"{}\\\\ready\\\\title_{}.wl\".format(prefix,suffix)\n",
    "\n",
    "#extract_data(data_ori,data_raw,cols=[2,7],processor =[lambda x:x, lambda x:x.lower().replace(\" \",\"\")],\n",
    "#             criteria=(lambda row:row[7].lower().replace(\" \",\"\") in valid_industry))\n",
    "process_setting(low =False,old = False,stop = False)\n",
    "#tokenize_data(data_raw,data_token,[1],{0:lambda x:len(x.split(\" \"))<2})\n",
    "dict_list = dict_data(data_token,[1])\n",
    "dedup_data(data_token,data_token)\n",
    "dict2file(dict_list,[data_vocabulary,data_industry],criteria=[lambda x:x>5,lambda x:True])\n",
    "#shuffle_data(data_token)\n",
    "split_data(data_token,data_train,data_test,split_count  = 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "100000\n",
      "150000\n",
      "200000\n",
      "250000\n",
      "300000\n",
      "350000\n",
      "400000\n",
      "450000\n",
      "500000\n",
      "550000\n",
      "600000\n",
      "650000\n",
      "700000\n",
      "750000\n",
      "800000\n",
      "850000\n",
      "900000\n",
      "950000\n",
      "1000000\n",
      "1050000\n",
      "1100000\n",
      "Finished Extraction, Rows:1148810, Cols:3.\n",
      "Generating Tokenized Data ...\n",
      "50000\n",
      "Finished Generating Tokenized Data. Total:50000 .\n",
      "50000\n",
      "Finished Extraction, Rows:50000, Cols:2.\n",
      "Split Succeed.Train:35000 Test:15000\n",
      "Split Succeed.Train:35000 Test:15000\n"
     ]
    }
   ],
   "source": [
    "#180days sample & body\n",
    "\n",
    "\n",
    "suffix = \"180days_sample\"\n",
    "data_ori = \"{}\\\\raw\\\\ClassifierTrainingData-180days-180823.csv\".format(prefix_ori)\n",
    "data_raw = \"{}\\\\middle\\\\{}_raw_body.txt\".format(prefix,suffix)\n",
    "data_token = \"{}\\\\middle\\\\{}_token_body.txt\".format(prefix,suffix)\n",
    "\n",
    "data_sample = \"{}\\\\middle\\\\{}.txt\".format(prefix,suffix)\n",
    "data_train_sample = \"{}\\\\middle\\\\train_{}.txt\".format(prefix,suffix)\n",
    "data_test_sample = \"{}\\\\middle\\\\test_{}.txt\".format(prefix,suffix)\n",
    "\n",
    "data_sample_body = \"{}\\\\middle\\\\{}_body.txt\".format(prefix,suffix)\n",
    "data_train_sample_body = \"{}\\\\middle\\\\train_{}_body.txt\".format(prefix,suffix)\n",
    "data_test_sample_body = \"{}\\\\middle\\\\test_{}_body.txt\".format(prefix,suffix)\n",
    "\n",
    "data_industry_sample = \"{}\\\\ready\\\\industry_{}_sample.wl\".format(prefix,suffix)\n",
    "data_title_sample = \"{}\\\\ready\\\\title_{}_sample.wl\".format(prefix,suffix)\n",
    "data_body_sample = \"{}\\\\ready\\\\body_{}_sample.wl\".format(prefix,suffix)\n",
    "\n",
    "extract_data(data_ori,data_raw,cols=[2,3,7],processor =[lambda x:x,lambda x:x, lambda x:x.lower().replace(\" \",\"\")],\n",
    "             criteria=(lambda row:row[7].lower().replace(\" \",\"\") in valid_industry))\n",
    "dedup_data(data_raw,data_raw,selector = lambda row:row.split(\"\\t\")[0])\n",
    "sample_data(data_raw,data_sample_body,50000)\n",
    "tokenize_data(data_sample_body,data_token)\n",
    "dict_list = dict_data(data_sample_body,[2])\n",
    "\n",
    "extract_data(data_sample_body,data_sample,cols = [0,2],processor =[lambda x:x,lambda x:x])\n",
    "\n",
    "dict2file(dict_list,[data_title_sample,data_body_sample,data_industry_sample],criteria=[lambda x:x>2,lambda x:x>5,lambda x:True])\n",
    "\n",
    "split_data(data_sample,data_train_sample,data_test_sample)\n",
    "split_data(data_sample_body,data_train_sample_body,data_test_sample_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [cntk-py35]",
   "language": "python",
   "name": "Python [cntk-py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
