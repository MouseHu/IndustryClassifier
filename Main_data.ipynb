{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\local\\Anaconda3-4.1.1-Windows-x86_64\\envs\\cntk-py35\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\t-haohu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from data_generator import *\n",
    "from data_processor import *\n",
    "from svm import *\n",
    "import re\n",
    "valid_industry = [\n",
    "    \"Non-profit & civil organization\",\"Finance\",\"Video game\",\"Media\",\"Health care & biotechnology\",\n",
    "    \"Entertainment \", \"food\",\"Information technology\",\"Education\",\"Government\",\"Airline\",\"Retail\",\n",
    "    \"Arts\",\"Manufacture \",\"Transportation\",\"Construction \",\"Automotive industry\",\"evergreen\",\"sports\"\n",
    "]\n",
    "black_list= [\n",
    "    \"entertainment\", \"government\", \"arts\", \"education\", \"transportation\", \"evergreen\"\n",
    "]\n",
    "valid_industry=[x.lower().replace(\" \",\"\") for x in valid_industry]\n",
    "prefix = \"C:\\\\Users\\\\t-haohu\\\\Documents\\\\Python\\\\IndustryClassifier\\\\Data\"\n",
    "prefix_ori = \"C:\\\\Users\\\\t-haohu\\\\Documents\\\\Python\\\\news\\\\Data\"\n",
    "\n",
    "def valid_word(x):\n",
    "    if len(x[0])>1 and (re.search(r\"\\A'\",x[0]) or re.search(r\"t\\Z\",x[0])):\n",
    "        return False\n",
    "    if re.search(r\"[0-9]+\",x[0]):\n",
    "        return False\n",
    "    return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "100000\n",
      "150000\n",
      "200000\n",
      "250000\n",
      "300000\n",
      "350000\n",
      "400000\n",
      "450000\n",
      "500000\n",
      "550000\n",
      "600000\n",
      "650000\n",
      "700000\n",
      "750000\n",
      "800000\n",
      "850000\n",
      "900000\n",
      "950000\n",
      "1000000\n",
      "1050000\n",
      "1100000\n",
      "Finished Extraction, Rows:1148810, Cols:2.\n",
      "Generating Tokenized Data ...\n",
      "50000\n",
      "100000\n",
      "150000\n",
      "200000\n",
      "250000\n",
      "300000\n",
      "350000\n",
      "400000\n",
      "450000\n",
      "500000\n",
      "550000\n",
      "600000\n",
      "650000\n",
      "700000\n",
      "750000\n",
      "800000\n",
      "850000\n",
      "900000\n",
      "950000\n",
      "1000000\n",
      "1050000\n",
      "1100000\n",
      "Finished Generating Tokenized Data. Total:1148810 .\n",
      "Dedup Successfully. Before:1148810 After:1097865\n",
      "Split Succeed.Train:768505 Test:329360\n"
     ]
    }
   ],
   "source": [
    "#180days all\n",
    "suffix = \"180days_all\"\n",
    "process_setting(low =False,old = True,stop = False)\n",
    "data_ori = \"{}\\\\raw\\\\ClassifierTrainingData-180days-180823.csv\".format(prefix_ori)\n",
    "data_raw = \"{}\\\\middle\\\\{}_raw.txt\".format(prefix,suffix)\n",
    "data_token = \"{}\\\\middle\\\\{}_tokenized.txt\".format(prefix,suffix)\n",
    "\n",
    "data_train = \"{}\\\\middle\\\\train_{}.txt\".format(prefix,suffix)\n",
    "data_test = \"{}\\\\middle\\\\test_{}.txt\".format(prefix,suffix)\n",
    "\n",
    "\n",
    "data_industry = \"{}\\\\ready\\\\industry_{}.wl\".format(prefix,suffix)\n",
    "data_vocabulary = \"{}\\\\ready\\\\title_{}.wl\".format(prefix,suffix)\n",
    "\n",
    "extract_data(data_ori,data_raw,cols=[2,7],processor =[lambda x:x, lambda x:x.lower().replace(\" \",\"\")],\n",
    "             criteria=(lambda row:row[7].lower().replace(\" \",\"\") in valid_industry))\n",
    "\n",
    "tokenize_data(data_raw,data_token,[1],{0:lambda x:len(x.split(\" \"))<2})\n",
    "dict_list = dict_data(data_token,[1])\n",
    "dedup_data(data_token,data_token)\n",
    "dict2file(dict_list,[data_vocabulary,data_industry],criteria=[lambda x:x[1]>5,lambda x:True])\n",
    "#shuffle_data(data_token)\n",
    "split_data(data_token,data_train,data_test)\n",
    "\n",
    "data_token_shuffled = \"{}\\\\middle\\\\{}_tokenized_shuffled.txt\".format(prefix,suffix)\n",
    "data_train_shuffled = \"{}\\\\middle\\\\train_{}_shuffled.txt\".format(prefix,suffix)\n",
    "data_test_shuffled = \"{}\\\\middle\\\\test_{}_shuffled.txt\".format(prefix,suffix)\n",
    "\n",
    "shuffle_data(data_token,data_token_shuffled)\n",
    "split_data(data_token_shuffled,data_train_shuffled,data_test_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "# shuffle test editor\n",
    "suffix = \"180days_all\"\n",
    "data_token_shuffled = \"{}\\\\middle\\\\{}_tokenized_shuffled.txt\".format(prefix,suffix)\n",
    "#data_train_shuffled = \"{}\\\\middle\\\\train_{}_shuffled.txt\".format(prefix,suffix)\n",
    "data_test_shuffled = \"{}\\\\middle\\\\test_{}_shuffled.txt\".format(prefix,suffix)\n",
    "data_test_shuffled_editor = \"{}\\\\middle\\\\test_{}_shuffled_editor.txt\".format(prefix,suffix)\n",
    "suffix = \"180days_editor\"\n",
    "data_token = \"{}\\\\middle\\\\{}_tokenized.txt\".format(prefix,suffix)\n",
    "\n",
    "lookup = {x.split(\"\\t\")[0]:0 for x in open(data_token,encoding = \"utf-8\").readlines()}\n",
    "output = open(data_test_shuffled_editor,\"w\",encoding = \"utf-8\")\n",
    "for line in open(data_test_shuffled,encoding = \"utf-8\").readlines():\n",
    "    if line.split(\"\\t\")[0] in lookup:\n",
    "        output.write(line)\n",
    "output.close()\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "100000\n",
      "150000\n",
      "200000\n",
      "250000\n",
      "300000\n",
      "350000\n",
      "400000\n",
      "450000\n",
      "500000\n",
      "550000\n",
      "600000\n",
      "650000\n",
      "700000\n",
      "750000\n",
      "800000\n",
      "850000\n",
      "900000\n",
      "950000\n",
      "1000000\n",
      "1050000\n",
      "1100000\n",
      "Finished Extraction, Rows:1148810, Cols:3.\n",
      "Generating Tokenized Data ...\n",
      "50000\n",
      "100000\n",
      "150000\n",
      "200000\n",
      "250000\n",
      "300000\n",
      "350000\n",
      "400000\n",
      "450000\n",
      "500000\n",
      "550000\n",
      "600000\n",
      "650000\n",
      "700000\n",
      "750000\n",
      "800000\n",
      "850000\n",
      "900000\n",
      "950000\n",
      "1000000\n",
      "1050000\n",
      "1100000\n",
      "Finished Generating Tokenized Data. Total:1148810 .\n",
      "Dedup Successfully. Before:1148810 After:1135098\n",
      "Split Succeed.Train:794568 Test:340530\n",
      "Data Shuffle Succeed.\n",
      "Split Succeed.Train:794568 Test:340530\n"
     ]
    }
   ],
   "source": [
    "#180days all body\n",
    "suffix = \"180days_all_body\"\n",
    "process_setting(low =False,old = True,stop = False)\n",
    "data_ori = \"{}\\\\raw\\\\ClassifierTrainingData-180days-180823.csv\".format(prefix_ori)\n",
    "data_raw = \"{}\\\\middle\\\\{}_raw.txt\".format(prefix,suffix)\n",
    "data_token = \"{}\\\\middle\\\\{}_tokenized.txt\".format(prefix,suffix)\n",
    "\n",
    "data_train = \"{}\\\\middle\\\\train_{}.txt\".format(prefix,suffix)\n",
    "data_test = \"{}\\\\middle\\\\test_{}.txt\".format(prefix,suffix)\n",
    "\n",
    "\n",
    "data_industry = \"{}\\\\ready\\\\industry_{}.wl\".format(prefix,suffix)\n",
    "data_title = \"{}\\\\ready\\\\title_{}.wl\".format(prefix,suffix)\n",
    "data_body = \"{}\\\\ready\\\\body_{}.wl\".format(prefix,suffix)\n",
    "\n",
    "extract_data(data_ori,data_raw,cols=[2,3,7],processor =[lambda x:x,lambda x:x, lambda x:x.lower().replace(\" \",\"\")],\n",
    "             criteria=(lambda row:row[7].lower().replace(\" \",\"\") in valid_industry))\n",
    "\n",
    "tokenize_data(data_raw,data_token,[2],{0:lambda x:len(x.split(\" \"))<2})\n",
    "dict_list = dict_data(data_token,[2])\n",
    "dedup_data(data_token,data_token)\n",
    "dict2file(dict_list,[data_title,data_body,data_industry],criteria=[lambda x:x[1]>5,lambda x:x[1]>8,lambda x:True])\n",
    "#shuffle_data(data_token)\n",
    "split_data(data_token,data_train,data_test)\n",
    "\n",
    "data_token_shuffled = \"{}\\\\middle\\\\{}_tokenized_shuffled.txt\".format(prefix,suffix)\n",
    "data_train_shuffled = \"{}\\\\middle\\\\train_{}_shuffled.txt\".format(prefix,suffix)\n",
    "data_test_shuffled = \"{}\\\\middle\\\\test_{}_shuffled.txt\".format(prefix,suffix)\n",
    "\n",
    "shuffle_data(data_token,data_token_shuffled)\n",
    "split_data(data_token_shuffled,data_train_shuffled,data_test_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "100000\n",
      "Finished Extraction, Rows:101598, Cols:2.\n",
      "Generating Tokenized Data ...\n",
      "50000\n",
      "100000\n",
      "Finished Generating Tokenized Data. Total:101598 .\n",
      "Dedup Successfully. Before:101598 After:100062\n",
      "Split Succeed.Train:70043 Test:30019\n",
      "Data Shuffle Succeed.\n",
      "Split Succeed.Train:70043 Test:30019\n"
     ]
    }
   ],
   "source": [
    "#180days editor\n",
    "suffix = \"180days_editor\"\n",
    "process_setting(low =False,old = True,stop = False)\n",
    "data_ori = \"{}\\\\raw\\\\ClassifierTrainingData-180days-180823.csv\".format(prefix_ori)\n",
    "data_raw = \"{}\\\\middle\\\\{}_raw.txt\".format(prefix,suffix)\n",
    "data_token = \"{}\\\\middle\\\\{}_tokenized.txt\".format(prefix,suffix)\n",
    "\n",
    "data_train = \"{}\\\\middle\\\\train_{}.txt\".format(prefix,suffix)\n",
    "data_test = \"{}\\\\middle\\\\test_{}.txt\".format(prefix,suffix)\n",
    "\n",
    "\n",
    "data_industry = \"{}\\\\ready\\\\industry_{}.wl\".format(prefix,suffix)\n",
    "data_vocabulary = \"{}\\\\ready\\\\title_{}.wl\".format(prefix,suffix)\n",
    "\n",
    "extract_data(data_ori,data_raw,cols=[2,7],processor =[lambda x:x, lambda x:x.lower().replace(\" \",\"\")],\n",
    "             criteria=(lambda row:row[7].lower().replace(\" \",\"\") in valid_industry and (row[6] == \"95000\" or row[6] == \"94999\")))\n",
    "\n",
    "tokenize_data(data_raw,data_token,[2],{0:lambda x:len(x.split(\" \"))<2})\n",
    "dict_list = dict_data(data_token,[2])\n",
    "dedup_data(data_token,data_token)\n",
    "dict2file(dict_list,[data_vocabulary,data_industry],criteria=[lambda x:x[1]>2,lambda x:True])\n",
    "#shuffle_data(data_token)\n",
    "split_data(data_token,data_train,data_test)\n",
    "\n",
    "data_token_shuffled = \"{}\\\\middle\\\\{}_tokenized_shuffled.txt\".format(prefix,suffix)\n",
    "data_train_shuffled = \"{}\\\\middle\\\\train_{}_shuffled.txt\".format(prefix,suffix)\n",
    "data_test_shuffled = \"{}\\\\middle\\\\test_{}_shuffled.txt\".format(prefix,suffix)\n",
    "\n",
    "shuffle_data(data_token,data_token_shuffled)\n",
    "split_data(data_token_shuffled,data_train_shuffled,data_test_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "100000\n",
      "Finished Extraction, Rows:101598, Cols:3.\n",
      "Generating Tokenized Data ...\n",
      "50000\n",
      "100000\n",
      "Finished Generating Tokenized Data. Total:101598 .\n",
      "Dedup Successfully. Before:101598 After:101185\n",
      "Split Succeed.Train:70829 Test:30356\n",
      "Data Shuffle Succeed.\n",
      "Split Succeed.Train:70829 Test:30356\n"
     ]
    }
   ],
   "source": [
    "#180days editor body\n",
    "suffix = \"180days_editor_body\"\n",
    "process_setting(low =False,old = True,stop = False)\n",
    "data_ori = \"{}\\\\raw\\\\ClassifierTrainingData-180days-180823.csv\".format(prefix_ori)\n",
    "data_raw = \"{}\\\\middle\\\\{}_raw.txt\".format(prefix,suffix)\n",
    "data_token = \"{}\\\\middle\\\\{}_tokenized.txt\".format(prefix,suffix)\n",
    "\n",
    "data_train = \"{}\\\\middle\\\\train_{}.txt\".format(prefix,suffix)\n",
    "data_test = \"{}\\\\middle\\\\test_{}.txt\".format(prefix,suffix)\n",
    "\n",
    "\n",
    "data_industry = \"{}\\\\ready\\\\industry_{}.wl\".format(prefix,suffix)\n",
    "data_title = \"{}\\\\ready\\\\title_{}.wl\".format(prefix,suffix)\n",
    "data_body = \"{}\\\\ready\\\\body_{}.wl\".format(prefix,suffix)\n",
    "\n",
    "extract_data(data_ori,data_raw,cols=[2,3,7],processor =[lambda x:x,lambda x:x, lambda x:x.lower().replace(\" \",\"\")],\n",
    "             criteria=(lambda row:row[7].lower().replace(\" \",\"\") in valid_industry and (row[6] == \"95000\" or row[6] == \"94999\") ))\n",
    "\n",
    "tokenize_data(data_raw,data_token,[2],{0:lambda x:len(x.split(\" \"))<2})\n",
    "dict_list = dict_data(data_token,[2])\n",
    "dedup_data(data_token,data_token)\n",
    "dict2file(dict_list,[data_title,data_body,data_industry],criteria=[lambda x:x[1]>2,lambda x:x[1]>5,lambda x:True])\n",
    "#shuffle_data(data_token)\n",
    "split_data(data_token,data_train,data_test)\n",
    "\n",
    "data_token_shuffled = \"{}\\\\middle\\\\{}_tokenized_shuffled.txt\".format(prefix,suffix)\n",
    "data_train_shuffled = \"{}\\\\middle\\\\train_{}_shuffled.txt\".format(prefix,suffix)\n",
    "data_test_shuffled = \"{}\\\\middle\\\\test_{}_shuffled.txt\".format(prefix,suffix)\n",
    "\n",
    "shuffle_data(data_token,data_token_shuffled)\n",
    "split_data(data_token_shuffled,data_train_shuffled,data_test_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shuffle Succeed.\n",
      "Union Data Succeed.\n",
      "Data Shuffle Succeed.\n",
      "Union Data Succeed.\n",
      "Data Shuffle Succeed.\n",
      "Split Succeed.Train:2076724 Test:890025\n"
     ]
    }
   ],
   "source": [
    "#combine linkedin data\n",
    "process_setting(low =False,old = True,stop = False)\n",
    "suffix = \"180days_all_shuffled\"\n",
    "data_token = \"{}\\\\middle\\\\180days_all_tokenized_shuffled.txt\".format(prefix,suffix)\n",
    "data_train = \"{}\\\\middle\\\\train_{}.txt\".format(prefix,suffix)\n",
    "data_linkedin = \"{}\\\\middle\\\\{}.txt\".format(prefix_ori,\"LinkedInData_AfterMapping\")\n",
    "data_token_linkedin = \"{}\\\\middle\\\\{}_tokenized.txt\".format(prefix,\"LinkedInData_AfterMapping\")\n",
    "#extract_data(data_linkedin,data_token_linkedin,cols=[0,1],processor =[lambda x:tokenize(x), lambda x:x.lower().replace(\" \",\"\")],\n",
    " #            criteria=(lambda row:row[1].lower().replace(\" \",\"\") in valid_industry))\n",
    "#dedup_data(data_token_linkedin)\n",
    "data_train_with_linkedin_all = \"{}\\\\middle\\\\train_{}_with_linkedin_all.txt\".format(prefix,suffix)\n",
    "data_train_with_linkedin = \"{}\\\\middle\\\\train_{}_with_linkedin.txt\".format(prefix,suffix)\n",
    "data_test_with_linkedin = \"{}\\\\middle\\\\test_{}_with_linkedin.txt\".format(prefix,suffix)\n",
    "data_token_with_linkedin = \"{}\\\\middle\\\\{}_token_with_linkedin.txt\".format(prefix,suffix)\n",
    "\n",
    "\n",
    "union_data([data_train,data_token_linkedin],data_train_with_linkedin_all)\n",
    "union_data([data_token,data_token_linkedin],data_token_with_linkedin)\n",
    "shuffle_data(data_token_with_linkedin)\n",
    "split_data(data_token_linkedin,data_train_with_linkedin,data_test_with_linkedin)\n",
    "\n",
    "data_industry = \"{}\\\\ready\\\\industry_{}.wl\".format(prefix,suffix)\n",
    "data_title = \"{}\\\\ready\\\\title_{}.wl\".format(prefix,suffix)\n",
    "dict_list = dict_data(data_train_with_linkedin,[1])\n",
    "dict2file(dict_list,[data_title,data_industry],criteria=[lambda x:x[1]>5,lambda x:True])\n",
    "\n",
    "data_industry = \"{}\\\\ready\\\\industry_{}_all.wl\".format(prefix,suffix)\n",
    "data_title = \"{}\\\\ready\\\\title_{}_all.wl\".format(prefix,suffix)\n",
    "dict_list = dict_data(data_train_with_linkedin_all,[1])\n",
    "dict2file(dict_list,[data_title,data_industry],criteria=[lambda x:x[1]>5,lambda x:True])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#180days sample & body\n",
    "\n",
    "process_setting(low =True,old = True,stop = False)\n",
    "suffix = \"180days_sample\"\n",
    "data_ori = \"{}\\\\raw\\\\ClassifierTrainingData-180days-180823.csv\".format(prefix_ori)\n",
    "data_raw = \"{}\\\\middle\\\\{}_raw_body.txt\".format(prefix,suffix)\n",
    "\n",
    "\n",
    "data_sample = \"{}\\\\middle\\\\sample_{}.txt\".format(prefix,suffix)\n",
    "data_sample_truncated = \"{}\\\\middle\\\\sample_{}_truncated.txt\".format(prefix,suffix)\n",
    "\n",
    "data_token = \"{}\\\\middle\\\\{}_token.txt\".format(prefix,suffix)\n",
    "data_train_sample = \"{}\\\\middle\\\\train_{}.txt\".format(prefix,suffix)\n",
    "data_test_sample = \"{}\\\\middle\\\\test_{}.txt\".format(prefix,suffix)\n",
    "\n",
    "data_token_body = \"{}\\\\middle\\\\{}_token_body.txt\".format(prefix,suffix)\n",
    "data_train_sample_body = \"{}\\\\middle\\\\train_{}_body.txt\".format(prefix,suffix)\n",
    "data_test_sample_body = \"{}\\\\middle\\\\test_{}_body.txt\".format(prefix,suffix)\n",
    "\n",
    "data_industry_sample = \"{}\\\\ready\\\\industry_{}.wl\".format(prefix,suffix)\n",
    "data_title_sample = \"{}\\\\ready\\\\title_{}.wl\".format(prefix,suffix)\n",
    "data_body_sample = \"{}\\\\ready\\\\body_{}.wl\".format(prefix,suffix)\n",
    "\n",
    "#extract_data(data_ori,data_raw,cols=[2,3,7],processor =[lambda x:x,lambda x:x, lambda x:x.lower().replace(\" \",\"\")],\n",
    "#             criteria=(lambda row:row[7].lower().replace(\" \",\"\") in valid_industry))\n",
    "\n",
    "#sample_data(data_raw,data_sample,60000)\n",
    "\n",
    "#\n",
    "\n",
    "tokenize_data(data_sample,data_token_body,no_token_list=[2],filters={0:lambda x: len(x.split(\" \"))<2,1:lambda x:len(x.split(\" \"))<2})\n",
    "extract_data(data_token_body,data_sample_truncated,cols=[0,1,2],processor =[lambda x:x,lambda x:x.split(\"\\t\")[:200] if len(x.split(\"\\t\"))>200 else x,lambda x:x])\n",
    "extract_data(data_sample_truncated,data_token_body,cols=[0,1,2],processor =[lambda x:x,lambda x:x,lambda x:x])\n",
    "dedup_data(data_token_body,data_token_body,selector = lambda row:row.split(\"\\t\")[0])\n",
    "\n",
    "dict_list = dict_data(data_token_body,[2])\n",
    "dict2file(dict_list,[data_title_sample,data_body_sample,data_industry_sample],criteria=[lambda x:x[1]>1,lambda x:x[1]>3,lambda x:True])\n",
    "\n",
    "extract_data(data_token_body,data_token,cols = [0,2],processor =[lambda x:x,lambda x:x])\n",
    "\n",
    "\n",
    "split_data(data_token,data_train_sample,data_test_sample)\n",
    "split_data(data_token_body,data_train_sample_body,data_test_sample_body)\n",
    "\n",
    "data_sample_body_ctf_train = \"{}\\\\ready\\\\train_{}_body.ctf\".format(prefix,suffix)\n",
    "data_sample_body_ctf_test = \"{}\\\\ready\\\\train_{}_body.ctf\".format(prefix,suffix)\n",
    "data_sample_ctf_train = \"{}\\\\ready\\\\train_{}.ctf\".format(prefix,suffix)\n",
    "data_sample_ctf_test = \"{}\\\\ready\\\\test_{}.ctf\".format(prefix,suffix)\n",
    "\n",
    "ctf_data(data_train_sample, data_sample_ctf_train, [data_title_sample,data_industry_sample])\n",
    "ctf_data(data_test_sample , data_sample_ctf_test , [data_title_sample,data_industry_sample])\n",
    "ctf_data(data_train_sample_body, data_sample_body_ctf_train, [data_title_sample,data_body_sample,data_industry_sample])\n",
    "ctf_data(data_test_sample_body , data_sample_body_ctf_test , [data_title_sample,data_body_sample,data_industry_sample])\n",
    "\n",
    "data_title_dict = \"{}\\\\ready\\\\title_{}.pkl\".format(prefix,suffix)\n",
    "data_body_dict = \"{}\\\\ready\\\\body_{}.pkl\".format(prefix,suffix)\n",
    "data_w2v_dict = \"{}\\\\raw\\\\GoogleNews-vectors-negative300.bin\".format(prefix_ori)\n",
    "embed_dict(data_w2v_dict,data_title_dict,dict_list[0] ,criteria=lambda x:x[1]>1)\n",
    "embed_dict(data_w2v_dict,data_body_dict,dict_list[1] ,criteria=lambda x:x[1]>3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "100000\n",
      "150000\n",
      "200000\n",
      "250000\n",
      "300000\n",
      "350000\n",
      "400000\n",
      "450000\n",
      "500000\n",
      "550000\n",
      "600000\n",
      "650000\n",
      "700000\n",
      "750000\n",
      "Finished Extraction, Rows:764195, Cols:1.\n"
     ]
    }
   ],
   "source": [
    "suffix = \"1day_measure\"\n",
    "data_measure = \"{}\\\\middle\\\\{}.txt\".format(prefix,suffix)\n",
    "data_ori = \"{}\\\\raw\\\\2018-08-30-TestSample.txt\".format(prefix_ori)\n",
    "process_setting(low =True,old = True,stop = True)\n",
    "extract_data(data_ori,data_measure,cols=[2],processor =[lambda x:x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [cntk-py35]",
   "language": "python",
   "name": "Python [cntk-py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
